# Topic Modelling with LDA

## Load the data

We start by loading the data from a csv file, we then remove the publications without an abstract

```{r, eval = TRUE}
library(readr)
library(tidyverse)
library(tm)
library(ldatuning)
library(topicmodels)
library(tidytext)
library(LDAvis)
library(ggplot2)
entr_eco_publications <- read_csv("~/Desktop/Teachings/Topic_mod_workshop/entr_eco_publications.csv")
```

```{r, eval = TRUE}
entr_eco_publications = subset(entr_eco_publications, entr_eco_publications$Abstract != "[No abstract available]")
entr_eco_publications = entr_eco_publications$Abstract

```

## Text preprocessing

Before creating the model we need to prepare the text. This means removing punctuation, weird characters or even words that we don't want to find in our topics simply because they don't have any informational value.

```{r, eval = TRUE}
my_dico = c("publication", "research", "focus", "results", "result")
```

We build a function that prepares the text. It removed punctuation, removes stopwords and the words we don't want, stems the text etc. This function can be adapted and extended to your need by adding lemmatization for example.

```{r, eval=TRUE}
library(tm)
clean_text <- function(text){
  text = removePunctuation(text) # optional
  text <- tolower(text) # remove caps
  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with "".
  text <- gsub("\\.", "", text)
  text <- removeWords(text, my_dico) # Remove the terms from our own dictionary
  text <- stemDocument(text) # stem the terms
  text <- removeWords(text, stopwords(kind <- "en")) # remove stopwords (in english)
  text <- trimws(text) # remove any weird spaces in the text
}
```

We apply the function to our text

```{r, eval=TRUE}
entr_eco_publications = apply(as.matrix(entr_eco_publications), 1, clean_text)
```

## Prepare the data format

Now that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the **DocumentTermMatrix()** function from the **tm** package. This function has one argument which is a dataframe with the text of the corpus.

```{r, eval= TRUE}
# create the document-Term-matrix
dtm <- DocumentTermMatrix(entr_eco_publications)
```

## Find the number of topics

Based on this matrix we will now try to define how many topics we need to extract. For this we use the **FindTopicsNumber** and **FindTopicsNumber_plot** from the **ldatuning** package.

The **FindTopicsNumber** functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The **seq()** function creates a vector which starts at the *from* arguments, stops at the *to* argument. The size of the steps is set by the *by* argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, ...) we would write *seq(from = 2, to = 60, by = 5)*.

We then specify the metrics we want to compute, we have discussed these in the lecture.

::: callout-warning
For an unknown reason, the "Griffiths2004" function does not work on mac OSX. It should work for windows users.
:::

The mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algorithm is stuck, still running or finished.

```{r, eval=TRUE}
topic_num <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 60, by = 5),
  metrics = c("CaoJuan2009", "Arun2010", "Deveaud2014"),
  mc.cores = 8L,
  verbose = TRUE
)
FindTopicsNumber_plot(topic_num)
```

## Run the model

Now that we have an idea of how many topics we need, let's extract the topics. We will use the **LDA** function from the **topicmodels** package, and specify *K* as the number of topics. Here we can pick 12\~17 based on the previous plot.

```{r, eval = TRUE, warning=FALSE}
# perform topic modelling
k = 17
topic_model_results  <- LDA(dtm, k = k, method="Gibbs", control = list(seed = 42, alpha=50/k, nstart = 1, keep = 1, burnin = 1000, iter = 1000, verbose = FALSE)) 
```

## Visualize the results

topic_model_results contains the results of the topic modelling process, in other words it contains the probabilities that a word is part of a topic and a topic is part of a document. We can visualise this information by plotting the betas (probability that a word is part of a topic).

```{r, eval=TRUE}
# get the betas
betas <- tidy(topic_model_results, matrix = "beta")
# subset the betas for results
ap_top_terms <- betas %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

library(ggplot2)
ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

This plot shows the top 10 words for each topic. This means that it shows the 10 words with the highest probability to be part of the topic. The values under each barplot represent the betas.

## Create an interactive visualisation

```{r, eval = TRUE}
# Extract the necessary components for LDAvis
lda_topics <- posterior(topic_model_results)$topics
lda_terms <- posterior(topic_model_results)$terms
term_frequency <- colSums(as.matrix(dtm))
doc_length <- rowSums(as.matrix(dtm))

# Create the JSON object for visualization
json_lda <- createJSON(phi = lda_terms,
                       theta = lda_topics,
                       doc.length = doc_length,
                       vocab = colnames(dtm),
                       term.frequency = term_frequency)
```
```{r, eval=FALSE}
serVis(json_lda)
```

![](imgs/Topic_mod_workshop_images/LDA_vis.png)

In this visualisation, lambda controls the relevance of the shown terms:

-   lambda = 1: The relevance of terms is based entirely on their probability within the topic (p(term \| topic)). This means that the most probable terms for each topic will be shown.
-   lambda = 0: The relevance of terms is based entirely on their lift (p(term \| topic) / p(term)). This means that terms that are particularly unique to a topic (compared to their overall frequency) will be shown.
-   0 \< lambda \< 1: The relevance is a combination of probability and lift, allowing a balance between common terms within the topic and terms that are distinctive for the topic.

## Export the information

Often, we want to use the results of topic modelling to connect with other data (funding, authors, dates etc.). We can directly export the probability matrix. In this matrix each row is a document, and each column contains the probability that the document relates to this topic. The documents are in the same order as the initial data:

```{r, eval = TRUE}
posterior_probs <- posterior(topic_model_results)$topics
print(head(posterior_probs))
```

# Going further with text preparations:

## Lemmatization

It is common, even natural, to find inflections of the same term in a corpus of texts. The presence of a term and its plural (*desalinator*, *desalinators*), abbreviations (*pneu*, *pneumatics*), conjugations (*run*, *ran*, *running*) or terms with a close semantic meaning (*desalinator*, *desalination*) are common occurrences. These inflections, however, pose a problem in term frequency counts. In general, we consider that the terms *desalinator*, *desalination* and *desalinators* have the same informational value and are therefore synonymous. Retaining multiple inflections in the text results in a frequency calculation for each individual term resulting in a lower overall importance of each term. We would like to have only one count, for a term that we consider to be the reference term. There are two approaches to doing this, **stemming** and **lemmatization**. Stemming approaches this issue by reducing each word to its stem. The stem that results from this process is not always a word and can be difficult to understand out of context. Lemmatization has a different approach and used dictionary definitions to replace terms by a **main form**. @fig-lemmatization gives an example for different inflections which are reduced to a *main form*, in this case: *desalinate*).

::: columns
::: {.column width="45%"}
```{mermaid}
%%| label: fig-lemmatization
%%| fig-cap: Example of lemmatisation, where variations are replaced by "desalinate"
%%| fig-align: center
flowchart LR
  C(Desalinate)
  D[Desalinates] --> C
  E[Desalinating] --> C
  F[Desalinated] --> C
  G[Desalinator] --> C
  H[Desalination] --> C
```
:::

::: {.column width="10%"}
<!-- empty column to create gap -->
:::

::: {.column width="45%"}
```{mermaid}
%%| label: fig-stemming
%%| fig-cap: Example of Stemming, where variations are reduced to their stem "desalin"
%%| fig-align: center

flowchart LR
  O(Desalin)
  J[Desalinates] --> O
  K[Desalinating] --> O
  L[Desalinated] --> O
  M[Desalinator] --> O
  N[Desalination] --> O
```
:::
:::

There are practical advantages to using lemmatization since the main form remains readable, while with stemming this is more complicated. *In fine*, it's up to the analyst to decide which approach is best for both the question at hand and the data chosen. In the following table some advantages and disadvantages are shown:

| **Aspect**                   | **Lemmatization**                         | **Stemming**                                   |
|-------------------|---------------------------|---------------------------|
| **Accuracy**                 | Better accuracy, considers context        | Faster, computationally less expensive         |
| **Readability**              | Improved, real words                      | Simpler, heuristic rules                       |
| **Context Preservation**     | Considers word meaning, preserves context | May lead to over-stemming, loss of specificity |
| **Computational Complexity** | More computationally expensive            | Less computationally expensive                 |
| **Resource Intensive**       | Requires linguistic resources             | Minimal resources required                     |

#### Stemming and Lemmatization in R

For the implementation of lemmatization we will use the **textstem** package. Lemmatization is done in two steps, first a dictionnary is created based on the text. Basically this means that all terms in the provided text are identified and for these terms lemmas are identified. In a second step this dictionary is then applied to the text. The main reason for this two-step approach is to reduce computation time since we don't have to search through words that are not in the text.

For example, when we apply lemmatisation on the variation of the word desalinate:

```{r, eval = TRUE}
library(textstem)
# Some variations on a word as an example:
Example_text <-  c("Desalinates", "Desalinating", "Desalinated", "Desalinator", "Desalination")
Example_text <- tolower(Example_text) # remove the capital letters (required)
# we make a dictionary from the text
My_dico <-  make_lemma_dictionary(Example_text, engine = 'hunspell')
# now we apply the dictionnary to clean the text
lemmatized_text <- lemmatize_strings(Example_text, dictionary = My_dico)
lemmatized_text
```

We can include lemmatization in the cleaning function and then rerun the script from the beginning with this function.

```{r, eval=TRUE}
clean_text_lemma <- function(text){
  #text = removePunctuation(text) # optional
  text <- tolower(text) # remove caps
  text <- removeWords(text, my_dico) # Remove the terms from our own dictionary
  # here we apply lemmatization instead of stemming:
  lemma_dico <-  make_lemma_dictionary(text, engine = 'hunspell')
# now we apply the dictionnary to clean the text
  text <- lemmatize_strings(text, dictionary = lemma_dico)
  text <- removeWords(text, stopwords(kind <- "en")) # remove stopwords (in english)
  text <- trimws(text) # remove any weird spaces in the text
  text <- gsub("  ", " ", text)
}
```

[
  {
    "objectID": "NER.html",
    "href": "NER.html",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "We will used pre-trained models to perform named entity recognition. These models are quite heavy and require python/spark/torch/hadoop infrastructures. In case these do not work we will have a basic version (tagged but not trained) that will always work.\n\n\nTo access and run the different models we want to use, we need to install a couple of things.\n\n\nWe are going to install a light version of python to run from R. We will not use the python language but an R interface. It will however be possible to program in Python from Rstudio if you want to do this for another reason. The reticulate package allows this R-Python interface.\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n# for windows users, install GIT\n\nWe now install python from R in addition to a light version of anaconda. The combination of these installs allow you to create an environment from which to work. R will be able to find all that is required since it standardises all the information it needs in this environment. You only have to run the virtualenv_create(“DAFS”) once to create the environment. Later on you only have to use the use_virtualenv(“DAFS”) to load the environment when you want to use it. Note that DAFS is the name of the environment, you can change this to whatever you want.\n\ninstall_python()\n# Restart R manually\ninstall_miniconda()\n# Restart R manually\n\n# We now create a virtual environment for the course\nvirtualenv_create(\"DAFS\")\nuse_virtualenv(\"DAFS\")\n\n# note that when you start this script the virtualenv_create function is no longer required. You only run it once to create the environment.\n\n\n\n\nWe now install the interface to the huggingface website. We then need to install some python dependencies. It’s best to restart R when this is done.\n\ninstall.packages(\"devtools\")\ninstall.packages(\"usethis\")\ninstall.packages(\"cli\")\nlibrary(devtools)\ndevtools::install_github(\"farach/huggingfaceR\")\ninstall.packages(\"hfhub\")\nhuggingfaceR::hf_python_depends()\n# Restart R\n\nWe now install a couple of important Python packages that will allow us to load and use the models we want to use from huggingface:\n\npy_install(\"transformers\") # so we can use the models that are trained on tensorflow\npy_install(\"torch\") # so we can use the models that are trained on pytorch\n# Restart R\n\n\n\n\n\nNow we can start downloading the models. Let’s start by loading a bert model trained for NER. We add the argument aggregation_strategy= “simple to get an output that inclused readable tokens and not a list of syllable level tokens (cf. slides from the lecture). For NER, we will use the following model: bert-base-NER. If you have more patience and computation power bert-large-NER is also an option. The hf_load_pipeline will download the models directly and put them to use.\n\nNER_extract &lt;- huggingfaceR::hf_load_pipeline(model = \"dslim/bert-large-NER\", task = \"ner\", aggregation_strategy = \"simple\")\n\nLet’s get a sample text and try it out.\n\ntext &lt;- c(\"The 2024 edition of The European 5G Conference will take place on 30-31 January at the Hotel nhow Brussels Bloom. Now, in its 8th year, the conference has an established reputation as Brussels’ leading meeting place for discussion on 5G policy. Registration is now available – secure your place today. The event will, once again, provide the opportunity to hear from high-level policymakers and industry stakeholders on key themes such as investment, security, sustainability, emerging business models, and connectivity. It will provide an update on progress that has been made towards the 2030 ‘Path to the Digital Decade’ targets, as well as offering a first opportunity to examine the outcomes from WRC-23 and at what this may mean for the future connectivity environment around 5G and future technologies. By looking back at the lessons learnt to date and forward to the path towards 5G Advanced and 6G, the event will provide a comprehensive insight into all the key policy aspects that are shaping the 5G ecosystem in Europe.\")\nextracted_NE &lt;- NER_extract(text)\n#transform output into something readable:\nextracted_NE &lt;- plyr::ldply(extracted_NE, data.frame)\nextracted_NE\n\nWe can do the same with a different model that is capable of treating multiple languages. The basic structure is the same. We change the links to the models, and adjust what we download (tensor or pytorch):\n\nmultilanguage_NER = huggingfaceR::hf_load_pipeline(model = \"Babelscape/wikineural-multilingual-ner\", tokenizer = \"Babelscape/wikineural-multilingual-ner\", task = \"ner\", aggregation_strategy=\"simple\")\ntest_multi &lt;- multilanguage_NER(text)\ntest_multi &lt;- plyr::ldply(test_multi, data.frame)\ntest_multi\n\n\n\n\nThe workflow is the same, we just need to find the correct models:\n\nsentiment_classifier &lt;- huggingfaceR::hf_load_pipeline(model = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", tokenizer = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", top_k = 4)\nsent  &lt;- sentiment_classifier(text)\nsent &lt;- plyr::ldply(sent, data.frame)\nsent\n\nother type of model:\n\n# other type\nsubj_classifier = huggingfaceR::hf_load_pipeline(model = \"cffl/bert-base-styleclassification-subjective-neutral\", tokenizer = \"cffl/bert-base-styleclassification-subjective-neutral\", task=\"text-classification\")\nsubj &lt;- subj_classifier(text)\nsubj &lt;- plyr::ldply(subj, data.frame)\nsubj\n\n\n\n\nThe previous sections have shown how to use NER and sentiment analysis on a text. We now need to run this over a corpus. This means building a function that will allow us to automate the execution of the tasks.\nStart by importing the lexis uni data used in the previous tutorial. For this tutorial, this data is stored in the “LN_dataframe” object. If this takes too long to run, you can always change the NER system. For a faster system use the “dslim/bert-base-NER” model at the beginning.\n\nload(\"LN_dataframe.rdata\")\n# the text we want to analyse is in the \"Article\" column\nNER_function = function(data, score_thresh){\n  data = as.data.frame(data)\n  # perform NER on the chosen column\n  output &lt;- multilanguage_NER(as.character(data))\n  # organise the result in a df\n  output &lt;- plyr::ldply(output, data.frame)\n  # subset according to threshold\n  output &lt;- subset(output, output$score &gt;= score_thresh)\n  # this gives us a dataframe will all identified objects\n  # we need to regroup them by type (ORG, PER, MISC, LOC)\n  output_df &lt;- data.frame(\"LOC\" = paste(subset(output, output$entity_group == \"LOC\")$word, collapse = \";\"),\n                          \"ORG\" = paste(subset(output, output$entity_group == \"ORG\")$word, collapse = \";\"),\n                          \"PER\" = paste(subset(output, output$entity_group == \"PER\")$word, collapse = \";\"),\n                          \"MISC\" = paste(subset(output, output$entity_group == \"MISC\")$word, collapse = \";\")\n                        )\n  # return the output dataframe (technically not needed here)\n  return(output_df)\n} # closes function\n\nNER_results = apply(LN_dataframe, 1, NER_function, score_thresh = 0)\nNER_results &lt;- plyr::ldply(NER_results, data.frame)\n# this can take a while so we save the results\nsave(NER_results, file = \"NER_results.rdata\")\n\nIf your computer is a bit slower (or you want to run this a little bit at a time), you can also use a loop:\n\n# we start by adding the columns we want to add\nLN_dataframe_NER &lt;- LN_dataframe %&gt;% mutate(\"LOC\" = NA, \"ORG\" = NA, \"PER\" = NA, \"MISC\"= NA)\nfor(i in 1:dim(LN_dataframe)[1]){\n  if(i%%50 == 0){print(i)}\n  output &lt;- multilanguage_NER(as.character(LN_dataframe_NER[i,11]))\n  # organise the result in a df\n  output &lt;- plyr::ldply(output, data.frame)\n  # subset according to threshold\n  output &lt;- subset(output, output$score &gt;= 0)\n  # this gives us a dataframe will all identified objects\n  # we need to regroup them by type (ORG, PER, MISC, LOC)\n  output_df &lt;- data.frame(\"LOC\" = paste(subset(output, output$entity_group == \"LOC\")$word, collapse = \";\"),\n                          \"ORG\" = paste(subset(output, output$entity_group == \"ORG\")$word, collapse = \";\"),\n                          \"PER\" = paste(subset(output, output$entity_group == \"PER\")$word, collapse = \";\"),\n                          \"MISC\" = paste(subset(output, output$entity_group == \"MISC\")$word, collapse = \";\")\n                        )\n  LN_dataframe_NER[i,12:15] &lt;- output_df\n}\nsave(LN_dataframe_NER, file = \"LN_dataframe_NER.rdata\")\n\nOnce you have all the results in the dataframe, you can start the analysis with the tools you learned in previous tutorials.\n\n\n\nIn the previous tutorial we extracted topics, now let’s connect the topics to the dataframe so that we know which document connects to which topic.\n\nmost_likely_topics &lt;- topics(topics_model)# if you want THE most likely topic\nmost_likely_topics &lt;- topics(topics_model, k = 3) # if you want the 3 most likely\n\n# organise this data a bit\n# when requesting more than one topic, you need to transpose the df:\nmost_likely_topics &lt;- t(most_likely_topics)"
  },
  {
    "objectID": "NER.html#system-related-installation",
    "href": "NER.html#system-related-installation",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "To access and run the different models we want to use, we need to install a couple of things.\n\n\nWe are going to install a light version of python to run from R. We will not use the python language but an R interface. It will however be possible to program in Python from Rstudio if you want to do this for another reason. The reticulate package allows this R-Python interface.\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n# for windows users, install GIT\n\nWe now install python from R in addition to a light version of anaconda. The combination of these installs allow you to create an environment from which to work. R will be able to find all that is required since it standardises all the information it needs in this environment. You only have to run the virtualenv_create(“DAFS”) once to create the environment. Later on you only have to use the use_virtualenv(“DAFS”) to load the environment when you want to use it. Note that DAFS is the name of the environment, you can change this to whatever you want.\n\ninstall_python()\n# Restart R manually\ninstall_miniconda()\n# Restart R manually\n\n# We now create a virtual environment for the course\nvirtualenv_create(\"DAFS\")\nuse_virtualenv(\"DAFS\")\n\n# note that when you start this script the virtualenv_create function is no longer required. You only run it once to create the environment.\n\n\n\n\nWe now install the interface to the huggingface website. We then need to install some python dependencies. It’s best to restart R when this is done.\n\ninstall.packages(\"devtools\")\ninstall.packages(\"usethis\")\ninstall.packages(\"cli\")\nlibrary(devtools)\ndevtools::install_github(\"farach/huggingfaceR\")\ninstall.packages(\"hfhub\")\nhuggingfaceR::hf_python_depends()\n# Restart R\n\nWe now install a couple of important Python packages that will allow us to load and use the models we want to use from huggingface:\n\npy_install(\"transformers\") # so we can use the models that are trained on tensorflow\npy_install(\"torch\") # so we can use the models that are trained on pytorch\n# Restart R"
  },
  {
    "objectID": "NER.html#now-we-install-the-models-and-perform-ner",
    "href": "NER.html#now-we-install-the-models-and-perform-ner",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "Now we can start downloading the models. Let’s start by loading a bert model trained for NER. We add the argument aggregation_strategy= “simple to get an output that inclused readable tokens and not a list of syllable level tokens (cf. slides from the lecture). For NER, we will use the following model: bert-base-NER. If you have more patience and computation power bert-large-NER is also an option. The hf_load_pipeline will download the models directly and put them to use.\n\nNER_extract &lt;- huggingfaceR::hf_load_pipeline(model = \"dslim/bert-large-NER\", task = \"ner\", aggregation_strategy = \"simple\")\n\nLet’s get a sample text and try it out.\n\ntext &lt;- c(\"The 2024 edition of The European 5G Conference will take place on 30-31 January at the Hotel nhow Brussels Bloom. Now, in its 8th year, the conference has an established reputation as Brussels’ leading meeting place for discussion on 5G policy. Registration is now available – secure your place today. The event will, once again, provide the opportunity to hear from high-level policymakers and industry stakeholders on key themes such as investment, security, sustainability, emerging business models, and connectivity. It will provide an update on progress that has been made towards the 2030 ‘Path to the Digital Decade’ targets, as well as offering a first opportunity to examine the outcomes from WRC-23 and at what this may mean for the future connectivity environment around 5G and future technologies. By looking back at the lessons learnt to date and forward to the path towards 5G Advanced and 6G, the event will provide a comprehensive insight into all the key policy aspects that are shaping the 5G ecosystem in Europe.\")\nextracted_NE &lt;- NER_extract(text)\n#transform output into something readable:\nextracted_NE &lt;- plyr::ldply(extracted_NE, data.frame)\nextracted_NE\n\nWe can do the same with a different model that is capable of treating multiple languages. The basic structure is the same. We change the links to the models, and adjust what we download (tensor or pytorch):\n\nmultilanguage_NER = huggingfaceR::hf_load_pipeline(model = \"Babelscape/wikineural-multilingual-ner\", tokenizer = \"Babelscape/wikineural-multilingual-ner\", task = \"ner\", aggregation_strategy=\"simple\")\ntest_multi &lt;- multilanguage_NER(text)\ntest_multi &lt;- plyr::ldply(test_multi, data.frame)\ntest_multi"
  },
  {
    "objectID": "NER.html#and-now-sentiment-analysis",
    "href": "NER.html#and-now-sentiment-analysis",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "The workflow is the same, we just need to find the correct models:\n\nsentiment_classifier &lt;- huggingfaceR::hf_load_pipeline(model = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", tokenizer = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", top_k = 4)\nsent  &lt;- sentiment_classifier(text)\nsent &lt;- plyr::ldply(sent, data.frame)\nsent\n\nother type of model:\n\n# other type\nsubj_classifier = huggingfaceR::hf_load_pipeline(model = \"cffl/bert-base-styleclassification-subjective-neutral\", tokenizer = \"cffl/bert-base-styleclassification-subjective-neutral\", task=\"text-classification\")\nsubj &lt;- subj_classifier(text)\nsubj &lt;- plyr::ldply(subj, data.frame)\nsubj"
  },
  {
    "objectID": "NER.html#apply-this-to-our-data",
    "href": "NER.html#apply-this-to-our-data",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "The previous sections have shown how to use NER and sentiment analysis on a text. We now need to run this over a corpus. This means building a function that will allow us to automate the execution of the tasks.\nStart by importing the lexis uni data used in the previous tutorial. For this tutorial, this data is stored in the “LN_dataframe” object. If this takes too long to run, you can always change the NER system. For a faster system use the “dslim/bert-base-NER” model at the beginning.\n\nload(\"LN_dataframe.rdata\")\n# the text we want to analyse is in the \"Article\" column\nNER_function = function(data, score_thresh){\n  data = as.data.frame(data)\n  # perform NER on the chosen column\n  output &lt;- multilanguage_NER(as.character(data))\n  # organise the result in a df\n  output &lt;- plyr::ldply(output, data.frame)\n  # subset according to threshold\n  output &lt;- subset(output, output$score &gt;= score_thresh)\n  # this gives us a dataframe will all identified objects\n  # we need to regroup them by type (ORG, PER, MISC, LOC)\n  output_df &lt;- data.frame(\"LOC\" = paste(subset(output, output$entity_group == \"LOC\")$word, collapse = \";\"),\n                          \"ORG\" = paste(subset(output, output$entity_group == \"ORG\")$word, collapse = \";\"),\n                          \"PER\" = paste(subset(output, output$entity_group == \"PER\")$word, collapse = \";\"),\n                          \"MISC\" = paste(subset(output, output$entity_group == \"MISC\")$word, collapse = \";\")\n                        )\n  # return the output dataframe (technically not needed here)\n  return(output_df)\n} # closes function\n\nNER_results = apply(LN_dataframe, 1, NER_function, score_thresh = 0)\nNER_results &lt;- plyr::ldply(NER_results, data.frame)\n# this can take a while so we save the results\nsave(NER_results, file = \"NER_results.rdata\")\n\nIf your computer is a bit slower (or you want to run this a little bit at a time), you can also use a loop:\n\n# we start by adding the columns we want to add\nLN_dataframe_NER &lt;- LN_dataframe %&gt;% mutate(\"LOC\" = NA, \"ORG\" = NA, \"PER\" = NA, \"MISC\"= NA)\nfor(i in 1:dim(LN_dataframe)[1]){\n  if(i%%50 == 0){print(i)}\n  output &lt;- multilanguage_NER(as.character(LN_dataframe_NER[i,11]))\n  # organise the result in a df\n  output &lt;- plyr::ldply(output, data.frame)\n  # subset according to threshold\n  output &lt;- subset(output, output$score &gt;= 0)\n  # this gives us a dataframe will all identified objects\n  # we need to regroup them by type (ORG, PER, MISC, LOC)\n  output_df &lt;- data.frame(\"LOC\" = paste(subset(output, output$entity_group == \"LOC\")$word, collapse = \";\"),\n                          \"ORG\" = paste(subset(output, output$entity_group == \"ORG\")$word, collapse = \";\"),\n                          \"PER\" = paste(subset(output, output$entity_group == \"PER\")$word, collapse = \";\"),\n                          \"MISC\" = paste(subset(output, output$entity_group == \"MISC\")$word, collapse = \";\")\n                        )\n  LN_dataframe_NER[i,12:15] &lt;- output_df\n}\nsave(LN_dataframe_NER, file = \"LN_dataframe_NER.rdata\")\n\nOnce you have all the results in the dataframe, you can start the analysis with the tools you learned in previous tutorials."
  },
  {
    "objectID": "NER.html#connecting-topics-to-the-initial-dataframe",
    "href": "NER.html#connecting-topics-to-the-initial-dataframe",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "In the previous tutorial we extracted topics, now let’s connect the topics to the dataframe so that we know which document connects to which topic.\n\nmost_likely_topics &lt;- topics(topics_model)# if you want THE most likely topic\nmost_likely_topics &lt;- topics(topics_model, k = 3) # if you want the 3 most likely\n\n# organise this data a bit\n# when requesting more than one topic, you need to transpose the df:\nmost_likely_topics &lt;- t(most_likely_topics)"
  },
  {
    "objectID": "feature_extraction.html",
    "href": "feature_extraction.html",
    "title": "Feature Extraction",
    "section": "",
    "text": "Feature Extraction\n80% of the data at our disposal is textual and often unstructured (web, press, reports, internal notes). This data contains high value-added information for decision-making. Given the amount of information the text contains, it is important to know what is worth reading and what can be put aside [@fayyad1996knowledge]. Thus, the text requires particular techniques which aim to extract the salient elements of the text in order to be able to transform the text into intelligence. The objective of this training is to introduce the basic notions associated with textual and semantic analysis and present some use cases linked to sustainability.\nThe learning goals for this chapter are:\n\nKnow the vocabulary associated with textual and semantic analysis\nKnow how to differentiate between textual and semantic analysis\nUnderstand and implement the fundamental steps involved in preparing the text before analysis\nUnderstand the importance of preparing the text for analysis\nUnderstand the importance of an ontology\nKnow some applied cases for the study of sustainability\n\nThe difficulty associated with textual analysis is the identification of terms with high informational value contained in a corpus of documents. The raw text contains both terms which have value for the analysis we wish to make of them and terms which do not provide any particular information. The first step in textual analysis is the identification of terms that have value for the question the analyst seeks to address. In itself, extraction is a complex task, but there is software that more or less automate these steps (Gargantext, Cortext, Vantagepoint). However, the informational value of terms changes depending on a question asked. For example, identifying the population’s feeling towards a technology does not use the same terms as an analysis of the technological positioning of players in a field. It is therefore important to be able to intervene in the identification of terms to be able to guide the analysis for maximum impact on decision-making.\n\n\n\nFigure 1: 5 Short texts\n\n\nLet’s take the example of the three texts in Figure 1 . Three claims from two patents are represented. For an analysis that focuses on technology, we are mainly interested in the terms “reduced cross-section”, “edge geometry”, “thickness of the main body geometry” etc. We are not interested in elements such as “according to”, “claims”, “also includes”. We are therefore looking for an efficient method to identify the technical terms present in these patent documents.\nIn other cases, such as press data, we could be interested in events (conferences, congresses, symposiums) and the actors present, we then look for information of a different nature, requiring different extraction methods.\nRegardless of the question or the nature of the data sources, there are methods and indicators that allow us to zoom in on the information that we consider to have informational value. These algorithms and methods are based on hypotheses in information science in order to scientifically justify the indicators. These indicators are purely statistical in nature and highlight terms which have value due to their occurrence or positioning in the text. In this document we detail these indicators by exposing both the theoretical hypotheses and the calculations. After an initial automated extraction, comes the work of the analyst to rework the extraction through manual work (which is not necessarily very long) to better answer the question asked. This step is complementary to automatic statistical analysis in order to group and delete irrelevant terms. In this document we will present some use cases to illustrate this point.\nIn summary, textual analysis is initially based on an automatic extraction of candidate terms, followed by a cleaning step by the analyst which aims to direct towards the question:\nThe objective of textual analysis is to highlight salient elements in a textual corpus and to propose an analysis and contextualization of these elements.\nTextual analysis can be done in different ways. The one we use here is called the Bag-of-Words approach, literally, bag of words. The idea of this approach is to identify salient terms in the text and carry out a statistical analysis of these terms. Unlike semantic analysis, the meaning of terms is not taken into account. The approach focuses on terms and the association of these terms.\nThe approach is based on different sequential steps that we will follow in this tutorial.\n\nText cleanup\nIn text analysis, linguistic complexity is a major challenge. Whether it is plurals, conjugations, synonyms or even homographs and other variations, there are problems that we must address in order to maximize the informational value of the elements that we can extract from the text. In table X. different problems that can be encountered in a corpus of text are grouped together. Some are generic (like synonyms and homographs) others are specific to domains (the question of chemical elements in particular).\n\n\n\n\n\n\n\n\n\nCase\nExample\nProblem\nSolution\n\n\n\n\nSynonym - Abbreviation\nRhino ~ Rhinoceros\nUnderestimation of the term\nStemming, Lemmatisation\n\n\nSynonym\nDwelling ~ House\nUnderestimation of the terms\nStemming, Lemmatisation\n\n\nHomographe\nBass (Fish) ~ Bass (Instrument)\nOverestimation of the term\nLemmatisation\n\n\nCompound terms\nFuel Cell\nOverestimation of fuel and cell, Underestimation of fuel cell\nC-value\n\n\nNumbers - Latin\n1, 25gr, C02\nRemoving numbers can result in deleting valuable information.\nDictionary, RegEx\n\n\nNumbers - Roman\nUniv. Bordeaux IV\nCan be confused with acronyms (IV). Can create items with empty values.\nDictionary\n\n\nConjugation\nEvolved ~ Evolves ~ Evolving\nUnderestimation of the term evolve.\nLemmatisation\n\n\nPlurals\nMill ~ Mills\nUnderestimation of the term Mill\nStemming, Lemmatisation\n\n\nChemistry\nC02\nUnderestimation of the term C02 if improperly cleaned\nRegEx, Dictionary\n\n\nAcronyms\naminophenyltrimethoxysilane ~ APhTMS\nUnderestimation of the term APhTMS\nDictionary\n\n\n\nThe problems are mainly linked to counting problems which are the keystone of textual analysis. The elements which are given in solution will be explained one by one in the following paragraphs. For now the key point we want to make is that it is important to understand why each of these identified cases poses a problem in order to have correct counts. For the majority of cases, the underlying problem is that not processing the text creates more tokens than necessary resulting in an undervaluation of terms. For example, if we keep tire and tyres in the text, we have tokens with proper counts. Logic would, however, dictate that the two terms have the same informational value and are therefore grouped with a single frequency. The same thing is true for synonyms, what interests us basically is to know the importance of the concept of accommodation in a corpus. Suppose we are interested in the importance of housing among young people. In the stories that we can collect we are therefore indifferent between house, housing or even habitat the value is the same. Choosing a reference term and replacing all the synonyms with this value therefore allows you to better appreciate how many times the question of housing appears in the corpus.\nThe following subsections present the different stages of text preparation that we generally encounter in word processing software and in research papers that use open-source algorithms.\n\n\nStopwords\nThe BOW approach is based on the identification of terms that have strong informational value for a given question. One of the first steps then consists of removing terms with little or no informational value. These terms are called stop words. Typically words such a, the, or, and, to are considered tool words. These terms have high frequencies in any corpus and therefore create noise compared to the terms we seek to highlight. This list can, however, be amended if a term is essential for a given question. Conversely, it is possible to add terms which have no value in a particular context or for a particular question. For example, if we analyze a corpus of press data, it is possible to highlight events (trade fairs, congresses, etc.) which do not provide information if we seek to analyze technology. We can then choose to remove these terms for this particular question. A list of tool words starts from a list of basic terms and is then amended by the analyst depending on the case study and the question being addressed.\nIn R this can done with the tm package. The tm package has a removeWords function that takes two arguments, a first is the text from which we want to remove words, the second argument is an object that contains the words to remove. This can either be a simple list of words (made with the c() function), or stopword lists supplied by different packages. At this stage we are mostly interested in removing stopwords which implies that we need to also specify the language of the text. Using the kind argument allows for the specification of the language en for english, nl for dutch, fr for french and so on. In the code below we remove english stopwords from text_object which has the format of a character string.\nLet’s use as an example the abstract of a patent on water desalination:\n\ntext_object = \"Provided is a separation membrane for seawater desalination\nand a method for manufacturing the same, and more particularly, a separation\nmembrane for seawater desalination with excellent water permeability and salt\nrejection and a method for manufacturing the same. If the separation membrane\nfor seawater desalination and the method for manufacturing the same according\nto the present disclosure are applied, it is possible to provide a separation\nmembrane for seawater desalination with excellent water permeability and salt\nrejection. Therefore, it is possible to provide a separation membrane for\nseawater desalination with improved performance in comparison to an existing\nseparation membrane for seawater desalination. As a result, water resources\nmay be widely utilized.\"\n\nWe remove the stopwords from this text:\n\nlibrary(tm)\n  text_object_nostopwords = removeWords(text_object, stopwords(kind = \"en\")) \n\n\n\"Provided   separation membrane  seawater desalination   method \nmanufacturing  ,   particularly,  separation membrane  seawater\ndesalination  excellent water permeability  salt rejection   method \nmanufacturing  . If  separation membrane  seawater desalination \nmethod  manufacturing   according   present disclosure  applied,   possible  provide  separation membrane  seawater desalination  excellent\nwater permeability  salt rejection. Therefore,   possible  provide \nseparation membrane  seawater desalination  improved performance \ncomparison   existing separation membrane  seawater desalination. \nAs  result, water resources may  widely utilized.\"\n\n[1] \"Provided   separation membrane  seawater desalination   method \\nmanufacturing  ,   particularly,  separation membrane  seawater\\ndesalination  excellent water permeability  salt rejection   method \\nmanufacturing  . If  separation membrane  seawater desalination \\nmethod  manufacturing   according   present disclosure  applied,   possible  provide  separation membrane  seawater desalination  excellent\\nwater permeability  salt rejection. Therefore,   possible  provide \\nseparation membrane  seawater desalination  improved performance \\ncomparison   existing separation membrane  seawater desalination. \\nAs  result, water resources may  widely utilized.\"\n\n\n\n\nStemming and Lemmatization\nstart by describing the problem, also referring to the table with the problems and solutions.\nIt is common, even natural, to find inflections of the same term in a corpus of texts. The presence of a term and its plural (desalinator, desalinators), abbreviations (pneu, pneumatics), conjugations (run, ran, running) or terms with a close semantic meaning (desalinator, desalination) are common occurrences. These inflections, however, pose a problem in term frequency counts. In general, we consider that the terms desalinator, desalination and desalinators have the same informational value and are therefore synonymous. Retaining multiple inflections in the text results in a frequency calculation for each individual term resulting in a lower overall importance of each term. We would like to have only one count, for a term that we consider to be the reference term. There are two approaches to doing this, stemming and lemmatization. Stemming approaches this issue by reducing each word to its stem. The stem that results from this process is not always a word and can be difficult to understand out of context. Lemmatization has a different approach and used dictionary definitions to replace terms by a main form. Figure 2 gives an example for different inflections which are reduced to a main form, in this case: desalinate).\n\n\n\n\n\n\n\nflowchart LR\n  C(Desalinate)\n  D[Desalinates] --&gt; C\n  E[Desalinating] --&gt; C\n  F[Desalinated] --&gt; C\n  G[Desalinator] --&gt; C\n  H[Desalination] --&gt; C\n\n\nFigure 2: Example of lemmatisation, where variations are replaced by “desalinate”\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  O(Desalin)\n  J[Desalinates] --&gt; O\n  K[Desalinating] --&gt; O\n  L[Desalinated] --&gt; O\n  M[Desalinator] --&gt; O\n  N[Desalination] --&gt; O\n\n\nFigure 3: Example of Stemming, where variations are reduced to their stem “desalin”\n\n\n\n\n\n\nThere are practical advantages to using lemmatization since the main form remains readable, while with stemming this is more complicated. In fine, it’s up to the analyst to decide which approach is best for both the question at hand and the data chosen. In the following table some advantages and disadvantages are shown:\n\n\n\n\n\n\n\n\nAspect\nLemmatization\nStemming\n\n\n\n\nAccuracy\nBetter accuracy, considers context\nFaster, computationally less expensive\n\n\nReadability\nImproved, real words\nSimpler, heuristic rules\n\n\nContext Preservation\nConsiders word meaning, preserves context\nMay lead to over-stemming, loss of specificity\n\n\nComputational Complexity\nMore computationally expensive\nLess computationally expensive\n\n\nResource Intensive\nRequires linguistic resources\nMinimal resources required\n\n\n\n\nStemming and Lemmatization in R\nFor the implementation of lemmatization we will use the textstem package. Lemmatization is done in two steps, first a dictionnary is created based on the text. Basically this means that all terms in the provided text are identified and for these terms lemmas are identified. In a second step this dictionary is then applied to the text. The main reason for this two-step approach is to reduce computation time since we don’t have to search through words that are not in the text.\n\nlibrary(textstem)\n# Some variations on a word as an example:\nExample_text &lt;-  c(\"Desalinates\", \"Desalinating\", \"Desalinated\", \"Desalinator\", \"Desalination\")\nExample_text &lt;- tolower(Example_text) # remove the capital letters (required)\n# we make a dictionary from the text\nMy_dico &lt;-  make_lemma_dictionary(Example_text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\nlemmatized_text &lt;- lemmatize_strings(Example_text, dictionary = My_dico)\nlemmatized_text\n\n[1] \"desalinate\"  \"desalinate\"  \"desalinate\"  \"desalinator\" \"desalinate\" \n\n\nThe function has changed the words in the vector to their dictionary reference form. Not that “desalinator” has not been changed. This is because the word has no reference form in the underlying dictionary. We can address this issue by fist making our own dictionary and applying it before using the more generic ones. Actually, let’s go a step further. Suppose we are aiming for an analysis that aims at technologies. In this case all the conjugated forms of the word are actually relating to the “desalinator” as a technology. We could decide to replace all these words with “desalinator”. Lets’ see how we could do this:\n\nlibrary(quanteda)\n# first we create a dictionary that contains the variations of the word and the reference form:\nMy_dico = data.frame(term = c(\"Desalination\", \"Desalinates\", \"Desalinating\", \"Desalinated\"), lemma =c(\"Desalinator\", \"Desalinator\", \"Desalinator\", \"Desalinator\"), stringsAsFactors = FALSE)\n\n# we extract the tokens\nExtracted_tokens &lt;- tokens(text_object, remove_punct = TRUE)\n# then we use our new dictionnary to replace the tokens \nLemmatized_tokens &lt;- tokens_replace(Extracted_tokens,\n                             pattern = My_dico$term,\n                             replacement = My_dico$lemma,\n                             case_insensitive = TRUE, \n                             valuetype = \"fixed\")\n# the results is a list, we want our character string back so we unlist...\nLemmatized_tokens = unlist(Lemmatized_tokens)\n# ... and recombine the words\ncat(Lemmatized_tokens)\n\nIn certain cases we would want to keep the library from replacing specific terms, we can do this by explicitly excluding them from the set\n\nlibrary(lexicon)\n# we are looking to implement our own cleaning in this scenario, we have a particular vision of what we want to do, so we want to make sure changes is text follow our own definitions and not those of a dictionary which would not reflect our aim.\n# excluding specific words: \nmy_lex &lt;- lexicon::hash_lemmas[!token == \"Desalinator\", ]\ndf_lemmatized &lt;- lemmatize_strings(text_object, dictionary = my_lex)\ndf_lemmatized\n\nStemming is a more straightforward approach, it reduces the word to their stems with specific algorithms. Different algorithms have different approaches used to define the stem of a word.\n\n# we use the tm package and the stemDocument function\nExample_text &lt;-  c(\"Desalinates\", \"Desalinating\", \"Desalinated\", \"Desalinator\", \"Desalination\")\ntext_object_nostopwords &lt;- tm::stemDocument(Example_text)\ntext_object_nostopwords\n\n[1] \"Desalin\" \"Desalin\" \"Desalin\" \"Desalin\" \"Desalin\"\n\n\n\n\n\nThe dictionary\nIn many case there are word that we want to remove from the text because they appear in a high frequency but do not contain any value of the analysis. We can create a vector of words that we want to remove:\n\nMy_dictionnary &lt;- c(\"project\", \"invention\", \"aim\", \"goal\", \"paper\", \"water desalination\")\n\nWe can then remove these words using the removeWords function from the tm package.\n\ntext &lt;- tm::removeWords(tmp, My_dictionnary)\n\nThis approach breaks the structure of a sentence and should be used carefully. Later on we will search for specific sentence structures.\n\n\nThe Regex language\nIn many cases, text cleaning can require us to search and replace, remove or extract specific patterns in the text. For example in twitter data, we want to extract the # that are used in the tweets, when working with geo data we might want to extract postal codes which have specific patterns. For chemical solutions we might want to search specifically for µg, mg or numbers, in other cases the text contains errors or formatting that is creating noise in the text. All this and more can be done with a specific language called RegEx which stands for Regular Expressions. Using this language we can search for very specific patterns in the text, or indeed in any chain of characters. For example if we have the task of extracting postcodes from a character chain that contains a full address, we need to search for a chain that contains four numbers followed by two letters (at least in the Netherlands):\n\naddress &lt;- \"Straatweg 42, 9942AA, dorpstad\"\n\nWe can search for this pattern using the following regex:\n\n\"\\d[0-9]{3}[A-Z]{2}\"\n\n\nearches for digits, we specify that we search for any number between 0 and 9 by using the brackets.\n{3} indicates that we want to repeat this 3 times after the first, in other words we search for 4 numbers between 0 and 9\n[A-Z] searches for capital letters between A and Z. {2} indicates that we want 2\n\nWe can use this type of expression directly in most functions in R. However we often need to adjust some of the “\\” in the text. In the example below we need to add a “\\”:\n\nlibrary(stringr)\nstr_extract(\"Straatweg 42, 9942AA, dorpstad\", \"\\\\d[0-9]{3}[A-Z]{2}\")\n\n[1] \"9942AA\"\n\n\nThe combination of these elements will hence search for 4 numbers followed by 2 capital letters. Given that RegEx is a language in itself, it goes beyond the scope of this document to go into details. Know that it exists and can help with the identification of specific patterns in data. For more information see this website for testing and this website to get you started.\n\n\nPart-of-Speech Tagging\nOnce we have harmonized the text with the different cleaning steps, we are going to enter more into the details of the identification of terms with informational value. In textual data we will often see that one term alone has less informational value than when it’s combined with another: - Windmill energy - Fuel cell - Water desalination system\nWindmill is of less interest than Windmill energy. This means we need to find a way to identify these terms (also called n-grams) and extract them. The general understanding in the field of natural language processing and text mining is that n-grams (including bigrams, trigrams, etc.) can capture more complex patterns and relationships in text compared to monograms (single words). This is because n-grams consider sequences of words, providing more context and capturing dependencies between adjacent words.\nIf we take a look at the text we were initially working with we can identify mutiple multi-terms of interest:\n“Provided is a separation membrane for seawater desalination and a method for manufacturing the same, and more particularly, a separation membrane for seawater desalination with excellent water permeability and salt rejection and a method for manufacturing the same. If the separation membrane for seawater desalination and the method for manufacturing the same according to the present disclosure are applied, it is possible to provide a separation membrane for seawater desalination with excellent water permeability and salt rejection. Therefore, it is possible to provide a separation membrane for seawater desalination with improved performance in comparison to an existing separation membrane for seawater desalination. As a result, water resources may be widely utilized.”\nLet’s start with the analysis of this text. Start with downloading a language model that we will use for the tagging of the words in the text. The language can be changed according to the language of the text you are trying to analyse.\n\n#install.packages(\"udpipe\")\nlibrary(udpipe)\nud_model &lt;- udpipe::udpipe_download_model(language = \"english\")\nud_model &lt;- udpipe::udpipe_load_model(ud_model)\n\nx &lt;- udpipe_annotate(ud_model, x = text_object, doc_id = 1)\nx &lt;- as.data.frame(x)\nx\n\n\n\n\nFigure 4: Output of the Tagging process\n\n\nThe output of this script gives a dataframe in which each word (token) is identified, its lemma is supplied and then upos, xpos and feats are given:\n\nupos: “universal part-of-speech” -&gt; tags\nxpos: more detailed, language specific elements\n\n\n\n\nUPOS\nXPOS\nPOS\n\n\n\n\nNOUN\nNN\nNoun\n\n\nVERB\nVB\nVerb\n\n\nADJ\nJJ\nAdjective\n\n\nADV\nRB\nAdverb\n\n\nPRON\nPRP\nPersonal Pronoun\n\n\nADP\nIN\nAdposition\n\n\nCONJ\nCC\nConjunction\n\n\nNUM\nCD\nNumeral\n\n\n\n\nfeats: The feats field in the annotation output represents morphological features associated with each token in the processed text. Morphological features can include various linguistic information related to the morphology of a word, such as gender, number, case, tense, aspect, and more. These features are language-specific and can provide detailed information about the grammatical and morphological properties of words in a text.\n\nNow let’s do the same for a text that’s less about technologies but more about social issues with climate change:\n\ntext2 = \"Climate anxiety, an emotional response to the growing threat of climate change, \nhas become a prevalent concern in today's society. As global temperatures rise and extreme\nweather events escalate, individuals grapple with a sense of helplessness, fear, and \nimpending environmental doom. The constant barrage of alarming news, coupled with the \nrealization of the ecological crisis, fuels this anxiety. It manifests as eco-grief, \neco-anxiety, or climate despair, impacting mental health. Coping strategies \ninvolve activism, eco-conscious lifestyle changes, and fostering resilience. \nAddressing climate anxiety necessitates global cooperation, sustainable \npolicies, and a collective commitment to mitigating the impacts of climate change.\"\n\n\nx &lt;- udpipe_annotate(ud_model, x = text2, doc_id = 1)\nx &lt;- as.data.frame(x)\nx\n\n This example shows more clearly the effects of lemmatization on the text and all the different elements that are identified by the tagging system, punctuation included. Treating the text in this way allows us to identify specific phrases within the text that can be of value for an analysis. When we focus on the analysis of technologies, we are mainly interested in the nouns which would extract the technological terms from the text. If we want to identify social aspects, or emotions we can focus on nouns, verbs and adjectives. According to the question we are trying to answer, we can search for different elements within the text itself. The advantage of Part-Of-Speech tagging is that we can chose these patterns ourselves. These patterns are referred to as “word phrases”. Using the keywords_phrases function from the udpipe package we can search them directly.\n\n(A|N)*: This part represents a sequence of zero or more occurrences of either an adjective (A) or a noun (N).\nN: This represents a single occurrence of a noun.\n(P+D*(A|N)*N)*: This part represents a sequence that includes a preposition (P) followed by zero or more determiners (D) and then followed by the same pattern as described earlier for adjectives and nouns.\n\n\nx$phrase_tag &lt;- as_phrasemachine(x$upos, type = \"upos\")\nstats &lt;- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \n                            pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                            is_regex = TRUE, detailed = TRUE)\n\n\n\n\nResult of the phrases extraction\n\n\nThe results of the extraction show the different patterns containing adjectives, prepositions, determiners and nouns. The terms that are identified in this way are less random than extracting all possible combinations of sequential tokens. Of course the type of pattern can be adjusted in relation to a specific question or specific data.\nWe now have a long list of terms we can use which should have at least some informational value. But not all the terms have the same value. emotional response and response do not convey the same level of information. We hence need a method to determine which terms we would want to keep. This is especially the case for nested terms such as emotional response which contains two monograms. How can we automatically define that it’s the combination of both terms that has value rather than each term individually?\n\nC-value\nC value = \\(log_2|a|f(a) - \\frac{1}{P(T_a)}\\sum_{b \\in T_a}f(b)\\)\nIt’s this problem that the c-value indicator proposed by [@frantzi2000automatic] seeks to address. The logic is to reduce the value of a term according to the number of times it is nested within another term. The hypothesis is that if a term only appears as part of a larger term, it has no value. For example, in Table 1, we compute the c-values for the term “Fuel Cell Battery Stack”. This term has 3 nested terms (fuel cell battery, Fuel Cell, Fuel). The term \\(Fuel\\) appears 14 times in the text, of these 14 occurrences 12 are attributed to the broader term Fuel Cell. This reduces significantly the importance of the term \\(Fuel\\) in the text. Applying the formula from the beginning of this section we can compute the values for each term we get:\n\n\nTable 1: C Value computation illustration\n\n\n\n\n\n\n\n\n\n\nTerm\nFrequency\nn-grams\n\\(P_t\\)\n\\(\\sum f(b)\\)\nc-value\n\n\n\n\nFuel\n14\n1\n3\n12 + 10 + 7 = 29\n0\n\n\nFuel Cell\n12\n2\n2\n10 + 7 = 17\n3.5\n\n\nFuel Cell Battery\n10\n3\n1\n7\n4.75\n\n\nFuel Cell Battery Stack\n7\n4\n0\n0\n14\n\n\n\n\nThe results provide a high value for Fuel Cell Battery Stack, while Fuel Cell Battery and Fuel Cell have a positive but low value. Whether or not they will be kept in the final dataset depends on the values for other terms from other documents.\nThe c-value helps us select words based on a specific hypothesis, which related to the nestedness of terms. Another option is to count the frequencies of the terms and aim to assess when a frequency is high enough for us to consider a term important. For this we have multiple indicators which we will present now. Note however, that the c-values can be combined with the following indicators to identify terms more precisely.\n\n\n\nTerm classification\n\nOccurences\nThe first indicator of a term’s importance is simply the number of times it appears in a document. The assumption is that the frequency of occurrence of the term correlates with its importance in the corpus.\nThis frequency can be calculated in two ways: firstly, we can consider absolute frequency, i.e. the number of times the term is encountered in the corpus. This approach implies that if the term occurs three times in the same document, we increase the frequency by three. This method will therefore give the same score to a term that appears 10 times in a single document as to a term that appears once in 10 documents.\nThe second approach consists in calculating frequencies at document level, so that even if a term appears 10 times in a document, its frequency only increases by 1.\nBoth approaches have their advantages and disadvantages. Counting at the sentence level brings out terms that are important in a subset of documents, whereas the document-level approach will bring out mainly generic terms (terms present in all documents).\nTo conduct an in-depth analysis, these indicators have limitations. A problem of generality. We can consider a term to be generic when it appears in a significant number of documents [@jones1972statistical]. The use of frequency-based classification can therefore lead to the highlighting of elements of little interest. For example, when studying cars, it’s not necessarily useful to highlight , and . More precise terms might contain more information.\n\nTaking into account the absolute frequency of terms can help to highlight more specific terms. However, this approach is influenced by document size. In a corpus with heterogeneous text lengths, this largely influences the counts and can bias the identification work.\nThe limitations of raw counts in texts has led to the development of indicators that aim to avoid generic terms while highlighting more specific terms that represent at least part of the corpus. In the case of technological analyses, this means avoiding generic technological terms while retaining important terms for technological sub-segments.\n\n\n\n\\(\\chi^2\\) or Pearson residual\nA first idea to overcome these biases is to compare the frequency of a term with its expected frequency. In other words, given the size of the vocabulary and the number of documents, how often should a term be found in the corpus? The logic being that the longer the texts or the larger the corpus, the greater the probability of encountering a term. If the frequency exceeds the expected frequency, we could consider that the term is important for the corpus. This indicator is the \\(\\chi^2\\), commonly used in statistics to identify whether a deviation from an expected value is statistically significant. The formula is fairly self-explanatory:\n\\(\\chi^2(i,j) = \\frac{n(i,j) - e(i,j)}{\\sqrt{e(i,j)}}\\)\nHere, \\(n(i,j)\\) corresponds to the observed frequency of the term, while \\(e(i,j)\\) is the expected frequency, taking into account text length and number of documents. The nominator directly calculates the difference between the two values, and then normalizes. A value below what is expected will give rise to negative values, values above what is expected will give rise to positive values, close to 0 we are in the expected (generic) zone. In our corpus, the specificity score highlights as the most specific term. The higher value of certain terms is due to their presence in fewer documents (and therefore have lower co-occurrences). Using this indicator, it’s possible to remove generic terms that recur in a large majority of documents, to leave more room for more specific terms that may be of greater interest to us.\n\n\nTF-IDF / GF-IDF\nOne of the simplest indicators for classifying terms is known as or (). The aim of this indicator is to provide a measure of term specificity. The hypothesis on which the indicator is based is that a term should be considered more specific if it appears in only a few documents relative to the size of the corpus. Thus, a term with a frequency of 10 in the corpus, but present in only 2 documents, will be considered more specific than a term present in 10 documents.\nFormally, we express this idea as:\n\\[\\begin{equation}\n    \\frac{gf_t}{df_t}\n\\end{equation}\\]\nWith \\(gf_t\\) the overall frequency of the term \\(t\\), i.e. the number of times the term appears in the corpus. \\(df_t\\) the frequency in terms of documents, i.e. the number of documents containing the term \\(t\\). The element we need to decide on specificity is a comparison with the size of the corpus. 10 documents may not seem much, but if the corpus contains only 15 documents, 10 is a lot. So we’ll adjust the ratio in the previous equation by another ratio. What we’re interested in is a measure of the importance of the corpus containing the term in the overall corpus. We’ll proceed as follows:\n\\[\\begin{equation}\n    log(\\frac{N}{|d \\in D : t \\in d|})\n\\end{equation}\\]\n\\(N\\) is the total number of documents in the corpus. The numerator is the number of documents in the corpus containing the term \\(t\\). The ratio therefore measures the fraction of documents containing the term.\n\\[\\begin{equation}\n    gf.idf = \\frac{\\frac{gf_t}{df_t}}{log(\\frac{N}{|d \\in D : t \\in d|})}\n\\end{equation}\\]\nIn the nominator, \\(gf_t\\) corresponds to the frequency of appearance of the term \\(t\\) in the corpus. \\(df_t\\) corresponds to the number of documents in which \\(t\\) is present. The nominator therefore measures the average frequency of the term when present in a document.\nThis value is then adjusted in relation to the number of documents in the corpus. In the denominator, \\(N\\) corresponds to the number of documents in the corpus and \\(|d \\in D : t\\in d|\\) corresponds to the fraction of documents in the corpus containing the term \\(t\\). The measure therefore takes into account the importance of the term in the corpus as a whole, while integrating its importance in the documents themselves. The higher this value, the more important the term is in the corpus."
  },
  {
    "objectID": "Intro_programming_Databasemanip.html",
    "href": "Intro_programming_Databasemanip.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "In this chapter we look into database manipulation with R. We will use a combination of base-R functions and functions from the tidyverse. From this chapter you are expected to know how to:\n\nSubset a dataframe, matrix or dictionnary (specific to Python)\nFilter observations\ngroup observations\nMissing data treatment\nData transformations\nText data operations\n\nRegex\ngsub()\nsubstr()"
  },
  {
    "objectID": "Intro_programming_Databasemanip.html#database-manipulation",
    "href": "Intro_programming_Databasemanip.html#database-manipulation",
    "title": "NetworkIsLife",
    "section": "",
    "text": "In this chapter we look into database manipulation with R. We will use a combination of base-R functions and functions from the tidyverse. From this chapter you are expected to know how to:\n\nSubset a dataframe, matrix or dictionnary (specific to Python)\nFilter observations\ngroup observations\nMissing data treatment\nData transformations\nText data operations\n\nRegex\ngsub()\nsubstr()"
  },
  {
    "objectID": "Intro_programming_Databasemanip.html#data-manipulation-with-r",
    "href": "Intro_programming_Databasemanip.html#data-manipulation-with-r",
    "title": "NetworkIsLife",
    "section": "Data manipulation with R",
    "text": "Data manipulation with R\n\nData formats\nWhen working with data, we face different data object types. In practice we will mainly use the following formats:\n\nA scalar is just a numerical value.\n\nscalar_example &lt;- 4\n\nWhen we combine multiple of these values, we can create a vector. The c() function is used to combine different values into a vector:\n\nvector_example &lt;- as.vector(c(4,2,3,1,6))\nprint(vector_example)\n\n[1] 4 2 3 1 6\n\n\nA matrix, just as the mathematical object, is an aggregation of vectors.\n\nmatrix_example = matrix(c(1,2,3,4,5,6,7,8,9,10,11,12), nrow = 3, ncol = 4, byrow = T)\nprint(matrix_example)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\nA dataframe is the more versatile than a matrix since it can contain different types of data. It also has column names that we can refer to when manipulating the object, and can have row names.\n\ndataframe_example &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  Age = c(25, 30, 22, 28, 24),\n  Gender = c(\"Female\", \"Male\", \"Male\", \"Male\", \"Female\"),\n  Subscribed = c(TRUE, FALSE, TRUE, TRUE, FALSE)\n)\nprint(dataframe_example)\n\n     Name Age Gender Subscribed\n1   Alice  25 Female       TRUE\n2     Bob  30   Male      FALSE\n3 Charlie  22   Male       TRUE\n4   David  28   Male       TRUE\n5     Eve  24 Female      FALSE\n\n\n\n\n\nSelecting rows and columns\nThe selection of rows and columns can be achieved in two ways in base-R. Either by selecting the name of the column using the $ operator. Or by referencing the column number using [,].\nThe brackets have as first argument the row number(s) and as second argument the column umber(s). For example if you want to select the first row you would use [1,]. When no argument is supplied (as is the case here for the columns) then all columns are selected. For this reason [1,] gives the first row for all the columns. We can go further and search for the first row and only the first two columns, then we would write [1,c(1,2)].\n\n# select the column by its name: \"Name\"\ndataframe_example$Name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eve\"    \n\n# select the column by its index:\ndataframe_example[1]\n\n     Name\n1   Alice\n2     Bob\n3 Charlie\n4   David\n5     Eve\n\n# select the column by its index:\ndataframe_example[,1]\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eve\"    \n\n\nThe $ operator only works when the column has a header. Matrices do not have a header and therefor cannot be subsetted with the $ operator, only the [,] method works.\n\n# select the column by its index:\nmatrix_example[,1]\n\n[1] 1 5 9\n\n# select a row by its index:\nmatrix_example[1,]\n\n[1] 1 2 3 4\n\n# select multiple rows by their index:\nmatrix_example[c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    9   10   11   12\n\n# select multiple columns by their index:\nmatrix_example[,c(2,4)]\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    6    8\n[3,]   10   12\n\n\nUsing the tidyverse, we can accomplish similar tasks, often in a more efficient manner (in terms of computation time).\n\n\n\n\n\n\nPay Attention\n\n\n\nThe functions we expect you to know here are select() and slice().\n\n\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# Selecting one column\nselected_columns &lt;- data %&gt;% select(Pat_num)\nprint(selected_columns)\n\n       Pat_num\n1  WO200214562\n2 WO2023738962\n3 EP2023778962\n4 FR2019272698\n5  FR201922671\n\n# Selecting multiple columns\nselected_columns &lt;- data %&gt;% select(Pat_num, Year)\nprint(selected_columns)\n\n       Pat_num Year\n1  WO200214562 2002\n2 WO2023738962 2023\n3 EP2023778962 2023\n4 FR2019272698 2019\n5  FR201922671 2019\n\n# Selecting rows\nSelected_rows &lt;- data %&gt;% slice(1:3)\nprint(Selected_rows)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n\n\n\nFiltering your dataframe\nWe often need to subset datasets to suit specific needs. This means that we want to extract rows and columns from a dataset based on specific conditions. For example, we might want to extract all papers published in a specific year, from authors from a specific university. We need to filter according to specific conditions in the data. In base R we can do this by combining the logic operators (which we discussed in another chapter), and the function subset(). The subset() function requires two arguments, the first is the dataframe you want to subset, the second is the condition used to filter:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# Suppose we want to extract all patents filed in 2023:\npats_2023 &lt;- subset(data, data$Year == 2023)\nprint(pats_2023)\n\n       Pat_num Year Domain\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n# Suppose we want to extract all patent NOT filed in 2023:\nsubset = subset(data, !(Year == 2023))\nprint(subset)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n4 FR2019272698 2019   C08K\n5  FR201922671 2019   C08K\n\n# We can also combine multiple conditions\npats_2023 &lt;- subset(data, Year == 2023 | Year == 2002 )\nprint(pats_2023)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n# We can also combine multiple conditions\nsubset &lt;- subset(data, Domain != \"B60C\" & Year &gt;= 2019 )\nprint(subset)\n\n       Pat_num Year Domain\n3 EP2023778962 2023   B29D\n4 FR2019272698 2019   C08K\n5  FR201922671 2019   C08K\n\n\nThe same types of operations can be obtained with the tidyverse environment using the select() and filter() functions. Just as base-R, these functions require two arguments, a first with the dataframe, the second with the elements to select:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# extract all patents with domain B60C\nselected_columns &lt;- data %&gt;% filter(Domain == \"B60C\")\nprint(selected_columns)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n\n# extract the years of the patents with domain B60C\nsubset &lt;- data %&gt;% filter(Domain == \"B60C\") %&gt;% select(Year)\nprint(subset)\n\n  Year\n1 2002\n2 2023\n\n# extract the years of the patents NOT with domain B60C\nsubset &lt;- data %&gt;% filter(!(Domain == \"B60C\")) %&gt;% select(Year)\nprint(subset)\n\n  Year\n1 2023\n2 2019\n3 2019\n\n\n\n\ndropping columns and rows\nInstead of extracting particular rows and columns, we can also decide to drop rows and columns. In base-R this is achieved with the same functions as for selection, with the addition of a “-” to signify that we want to remove the column or row:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# remove the first column:\nmatrix_example[,-1]\n\n     [,1] [,2] [,3]\n[1,]    2    3    4\n[2,]    6    7    8\n[3,]   10   11   12\n\n# remove the first row:\nmatrix_example[-1,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    5    6    7    8\n[2,]    9   10   11   12\n\n# remove multiple rows by their index:\nmatrix_example[-c(1,3),]\n\n[1] 5 6 7 8\n\n# remove multiple columns by their index:\nmatrix_example[,-c(2,4)]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    5    7\n[3,]    9   11\n\n\nWith the use of the dplyr package:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n# Selecting one column\nselected_columns &lt;- data %&gt;% select(-Pat_num)\nprint(selected_columns)\n\n  Year Domain\n1 2002   B60C\n2 2023   B60C\n3 2023   B29D\n4 2019   C08K\n5 2019   C08K\n\n# Selecting multiple columns\nselected_columns &lt;- data %&gt;% select(-Pat_num, -Year)\nprint(selected_columns)\n\n  Domain\n1   B60C\n2   B60C\n3   B29D\n4   C08K\n5   C08K\n\n# Equivalent form of the previous operation:\nselected_columns &lt;- data %&gt;% select(-c(Pat_num, Year))\nprint(selected_columns)\n\n  Domain\n1   B60C\n2   B60C\n3   B29D\n4   C08K\n5   C08K\n\n\n\n\nFurther actions: grouping and summarising\nWith real data we often need to regroup observations by year, organisation, region, or any other entity. For example if we have a set of scientific publications and we want to know how many publications we have per author per year, we need to regroup the observations in both those dimensions. In R we will do this using the tidyverse which will build upon the dplyr package. Mainly we will focus on three functions, group_by, summarise and reframe.\ngroup_by focuses on grouping observations according to a specific value. We can the compute values based on the created groups. For example, if we have a database with researchers and their publications, if we want to know how many publications each of them has, we would first have to create a subset per researcher, count how many publications he/she has, store this information, then move to the next one and so on. group_by allows us to create these subsets and summarise allows us to compute on these sets:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# want to know in which year the first patent in a domain has been filed, and when the last year of filing was.\n# we will group by domain, and then compute the min and max of the years to find the correct dates. \ndata %&gt;% group_by(Domain) %&gt;% summarise(\"first_year\" = min(Year), \"last_year\" = max(Year))\n\n# A tibble: 3 × 3\n  Domain first_year last_year\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 B29D         2023      2023\n2 B60C         2002      2023\n3 C08K         2019      2019\n\n# The arguments \"first_year\" and \"last_year\" will be the names of the columns we are creating in the resulting dataframe.\n\nWe now have a dataframe that has one row for each Domain, with the first and last year as second and third columns. summarise can contain any function, ranging from sum, mean to paste. If you want to simply count the number of occurrences within a group, the n() function will compute this:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# want to know in which year the first patent in a domain has been filed, and when the last year of filing was.\n# we will group by domain, and then compute the min and max of the years to find the correct dates. \ndata %&gt;% group_by(Domain) %&gt;% summarise(\"frequency\" = n())\n\n# A tibble: 3 × 2\n  Domain frequency\n  &lt;chr&gt;      &lt;int&gt;\n1 B29D           1\n2 B60C           2\n3 C08K           2"
  },
  {
    "objectID": "Intro_programming_Databasemanip.html#data-manipulation-with-python",
    "href": "Intro_programming_Databasemanip.html#data-manipulation-with-python",
    "title": "NetworkIsLife",
    "section": "Data manipulation with Python",
    "text": "Data manipulation with Python\n\nSelecting columns and rows\nJust as in R, Python allows for the extraction of columns both by their name and by their index. Locations in dataframes are accessed by the use of [[]], the first argument between brackets will refer to the rows, the second to the columns. If you want to extract all rows for a given column (or simply put, you want to extract a specific column), you would write [[“column_name”]].\n\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 28, 24],\n    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n}\n\ndf = pd.DataFrame(data)\n\n# Extracting one Column in Python\nselected_columns = df[['Name']]\nprint(selected_columns)\n\n      Name\n0    Alice\n1      Bob\n2  Charlie\n3    David\n4      Eve\n\n# Extracting multiple Columns in Python\nselected_columns = df[['Name', 'Age']]\nprint(selected_columns)\n\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   22\n3    David   28\n4      Eve   24\n\n\nUsing the index of the columns is also a possibility. For this case we will need to use the .iloc function. .iloc is used when we are addressing a dataframe by the index of the rows and columns, i.e by the number corresponding to the location in the dataframe. We need to provide two arguments for this function, the rows we want to select and the columns we want to select. On the contrary of the previous example, we need to specify rows when we use .iloc. If we want to extract all rows for a given column, we can do so by using :.\nIf we want to use a combination of both names and indexes, we need to use the .loc function which does not expect numbers as arguments.\n\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 28, 24],\n    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n}\n\ndf = pd.DataFrame(data)\n\n# Extracting Columns by Index in Python\n# We use \":\" in the first argument to specify that we want all rows. We then specify the columns we want (here 0 and 1).\nselected_columns = df.iloc[:,[0, 1]]\nprint(selected_columns)\n\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   22\n3    David   28\n4      Eve   24\n\n# If we do not specify which rows we want, Python will\n# interpret the numbers as being rows and not columns.\nfiltered_rows = df.iloc[[0, 4]]\nprint(filtered_rows)\n\n    Name  Age  Gender\n0  Alice   25  Female\n4    Eve   24  Female\n\n# A combination of both is also possible:\nrow3 = df.loc[0, \"Name\"]\nprint(row3)\n\nAlice\n\n\n\n\nFiltering your dataframe\nWe will use filtering on a pandas dataframe. As discussed in the chapter about basic operations, the logic operators will change compared to the base logic operators. Mainly: we will use & for and, | for or and ~ for not.\n\nimport pandas as pd\n# create a small dataframe\ndata = {\n  'Pat_num': [\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"],\n  'Year': [2002, 2023, 2023, 2019, 2019],\n  'Domain': [\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\"]\n}\ndf = pd.DataFrame(data)\n\n# subset the dataframe to extract only patents from 2023\nsubset = df[df['Year'] == 2023]\nprint(subset)\n\n        Pat_num  Year Domain\n1  WO2023738962  2023   B60C\n2  EP2023778962  2023   B29D\n\n# subset the dataframe to extract patent in domain \"B60C\"\n# with year &gt;= 2019\nsubset = df[(df['Year'] &gt;= 2019) & (df['Domain'] == \"B60C\")]\nprint(subset)\n\n        Pat_num  Year Domain\n1  WO2023738962  2023   B60C\n\n# subset the dataframe to extract patent NOT in domain \"B60C\"\nsubset = df[~(df['Domain'] == \"B60C\")]\nprint(subset)\n\n        Pat_num  Year Domain\n2  EP2023778962  2023   B29D\n3  FR2019272698  2019   C08K\n4   FR201922671  2019   C08K\n\n\n\n\nFurther actions: grouping and summarising\nWith real data we often need to regroup observations by year, organisation, region, or any other entity. For example if we have a set of scientific publications and we want to know how many publications we have per author per year, we need to regroup the observations in both those dimensions. In Python we will do this using the pandas package. Mainly we will focus on four functions, groupby, agg, count and reset_index.\ngroupby focuses on grouping observations according to a specific value. We can the compute values based on the created groups. For example, if we have a database with researchers and their publications, if we want to know how many publications each of them has, we would first have to create a subset per researcher, count how many publications he/she has, store this information, then move to the next one and so on. groupby allows us to create these subsets and agg allows us to compute on these sets:\n\nimport pandas as pd\ndata = {\n  'Pat_num': [\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"],\n  'Year': [2002, 2023, 2023, 2019, 2019],\n  'Domain': [\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\"]\n}\ndf = pd.DataFrame(data)\n\ngrouped_set = df.groupby('Domain')['Year'].agg([min, max]).reset_index()\nprint(grouped_set)\n\n  Domain   min   max\n0   B29D  2023  2023\n1   B60C  2002  2023\n2   C08K  2019  2019\n\ngrouped_set = df.groupby('Domain').count().reset_index()\nprint(grouped_set)\n\n  Domain  Pat_num  Year\n0   B29D        1     1\n1   B60C        2     2\n2   C08K        2     2"
  },
  {
    "objectID": "Intro_programming_basic_operations.html",
    "href": "Intro_programming_basic_operations.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter shows how to use basic operations in both R and Python. We will focus on operations that we encounter in a data analysis setting: arithmetic, comparisons, logic.\nYou are expected to be able to use all the operations described in this chapter.\n\n\nIn both R and Python, basic mathematical operations can be written directly in the\n\n\n\n\n\n\n# In R\n1 + 1\n\n[1] 2\n\n4 * 2\n\n[1] 8\n\n4 / 2\n\n[1] 2\n\n\n\n\n\n\n\n# In python\n1 + 1\n\n2\n\n4 * 2\n\n8\n\n4 / 2\n\n2.0\n\n\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nFor more complex mathematical operators we might need some functions that are not provided in the base version of the software we use. For instance, the ln() function and exp() function are not available in base python. We therefore need to load a package which contains these functions, this will be the maths package in Python. For R, both the ln() and exp() functions are part of the base package, we therefor do not need to load an additional package, we can use these functions right away.\n\n\n\n\n\n\n# Base R\nlog(12)\n\n[1] 2.484907\n\nexp(12)\n\n[1] 162754.8\n\nsqrt(12) # square root\n\n[1] 3.464102\n\n\n\n\n\n\n\nimport math\nmath.log(12)\n\n2.4849066497880004\n\nmath.exp(12)\n\n162754.79141900392\n\nmath.sqrt(12)\n\n3.4641016151377544\n\n\n\n\n\n\n\nComparison operations function in the same way in both languages. The only difference we note is in the assignment of the values to a variable in which it is recommended in R to use &lt;-.\n\n\n\n\n\n\nWarning\n\n\n\nEven though = works for simple operations in R, you might run into trouble with more complex code. For a detailed explanation see this post. We recommend you to always use the &lt;- operator when programming in R.\n\n\n\n\n\n\n# R\nx &lt;- 5\ny &lt;- 5\n# Equal to\nx == y\n\n[1] TRUE\n\n# Not equal to\nx != y\n\n[1] FALSE\n\n# Greater than\nx &gt; y\n\n[1] FALSE\n\n# Less than\nx &lt; y\n\n[1] FALSE\n\n# Greater than or equal to\nx &gt;= y\n\n[1] TRUE\n\n# Less than or equal to\nx &lt;= y\n\n[1] TRUE\n\n\n\n\n\n\n\n# Python\nx = 5\ny = 5\n# Equal to\nx == y\n\nTrue\n\n# Not equal to\nx != y\n\nFalse\n\n# Greater than\nx &gt; y\n\nFalse\n\n# Less than\nx &lt; y\n\nFalse\n\n# Greater than or equal to\nx &gt;= y\n\nTrue\n\n# Less than or equal to\nx &lt;= y\n\nTrue\n\n\n\n\n\n\n\nLogical operators are commonly used in data analysis, especially when sub-setting datasets. For example when we want to extract documents that are from the year 2000 which have the term “sustainability” and the term “climate change” but not the term “fossil fuel”. Combining these operators is important, and so is understanding how they work.\n\n\n\n\n\n\nPay Attention\n\n\n\nSome differences between R and Python become apparent here. In R, TRUE and FALSE must be written in all caps to be recognised as the logical operator. In Python, True and False must start with a capitalized letter. or, and, not should also be written exactly in this manner.If these operators are written differently, they will be recognized as objects.\n\n\n\n\n\n\nx &lt;- 4\ny &lt;- 8\n# Equal (==)\nx == y\n\n[1] FALSE\n\n# And (&)\nx == 4 & y == 8\n\n[1] TRUE\n\n# Or (|)\nx == 4 | y == 8\n\n[1] TRUE\n\n# Not (!)\n!y\n\n[1] FALSE\n\n# Combine\nz &lt;- \"plop\"\nx == 4 & (y == 8 | z == \"plop\")\n\n[1] TRUE\n\n\n\n\n\n\n\nx = 4\ny = 8\n# Equal (==)\nx == y\n\nFalse\n\n# And \nx == 4 and y == 8\n\nTrue\n\n# Or\nx == 4 or y == 8\n\nTrue\n\n# Not\nnot x\n\nFalse\n\n# Combine\nz = \"plop\"\nx == 4 and (y == 8 or z == \"plop\")\n\nTrue\n\n\n\n\nIn data analysis, we usually use operators to subset data. This means that we compare a variable to a value to check if it fits our criteria. For example, if we have a column that contains a year, and we only want observations with the year 2003, we will search for year == 2003. In this setting the R operators we just described will be the same. It is possible that these operators varie when different packages are used in python. For instance, in the context of the pandas package, and becomes &, or becomes |, not becomes ~. We will adress these variations in the database manipulation chapter."
  },
  {
    "objectID": "Intro_programming_basic_operations.html#basic-operations",
    "href": "Intro_programming_basic_operations.html#basic-operations",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter shows how to use basic operations in both R and Python. We will focus on operations that we encounter in a data analysis setting: arithmetic, comparisons, logic.\nYou are expected to be able to use all the operations described in this chapter.\n\n\nIn both R and Python, basic mathematical operations can be written directly in the\n\n\n\n\n\n\n# In R\n1 + 1\n\n[1] 2\n\n4 * 2\n\n[1] 8\n\n4 / 2\n\n[1] 2\n\n\n\n\n\n\n\n# In python\n1 + 1\n\n2\n\n4 * 2\n\n8\n\n4 / 2\n\n2.0\n\n\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nFor more complex mathematical operators we might need some functions that are not provided in the base version of the software we use. For instance, the ln() function and exp() function are not available in base python. We therefore need to load a package which contains these functions, this will be the maths package in Python. For R, both the ln() and exp() functions are part of the base package, we therefor do not need to load an additional package, we can use these functions right away.\n\n\n\n\n\n\n# Base R\nlog(12)\n\n[1] 2.484907\n\nexp(12)\n\n[1] 162754.8\n\nsqrt(12) # square root\n\n[1] 3.464102\n\n\n\n\n\n\n\nimport math\nmath.log(12)\n\n2.4849066497880004\n\nmath.exp(12)\n\n162754.79141900392\n\nmath.sqrt(12)\n\n3.4641016151377544\n\n\n\n\n\n\n\nComparison operations function in the same way in both languages. The only difference we note is in the assignment of the values to a variable in which it is recommended in R to use &lt;-.\n\n\n\n\n\n\nWarning\n\n\n\nEven though = works for simple operations in R, you might run into trouble with more complex code. For a detailed explanation see this post. We recommend you to always use the &lt;- operator when programming in R.\n\n\n\n\n\n\n# R\nx &lt;- 5\ny &lt;- 5\n# Equal to\nx == y\n\n[1] TRUE\n\n# Not equal to\nx != y\n\n[1] FALSE\n\n# Greater than\nx &gt; y\n\n[1] FALSE\n\n# Less than\nx &lt; y\n\n[1] FALSE\n\n# Greater than or equal to\nx &gt;= y\n\n[1] TRUE\n\n# Less than or equal to\nx &lt;= y\n\n[1] TRUE\n\n\n\n\n\n\n\n# Python\nx = 5\ny = 5\n# Equal to\nx == y\n\nTrue\n\n# Not equal to\nx != y\n\nFalse\n\n# Greater than\nx &gt; y\n\nFalse\n\n# Less than\nx &lt; y\n\nFalse\n\n# Greater than or equal to\nx &gt;= y\n\nTrue\n\n# Less than or equal to\nx &lt;= y\n\nTrue\n\n\n\n\n\n\n\nLogical operators are commonly used in data analysis, especially when sub-setting datasets. For example when we want to extract documents that are from the year 2000 which have the term “sustainability” and the term “climate change” but not the term “fossil fuel”. Combining these operators is important, and so is understanding how they work.\n\n\n\n\n\n\nPay Attention\n\n\n\nSome differences between R and Python become apparent here. In R, TRUE and FALSE must be written in all caps to be recognised as the logical operator. In Python, True and False must start with a capitalized letter. or, and, not should also be written exactly in this manner.If these operators are written differently, they will be recognized as objects.\n\n\n\n\n\n\nx &lt;- 4\ny &lt;- 8\n# Equal (==)\nx == y\n\n[1] FALSE\n\n# And (&)\nx == 4 & y == 8\n\n[1] TRUE\n\n# Or (|)\nx == 4 | y == 8\n\n[1] TRUE\n\n# Not (!)\n!y\n\n[1] FALSE\n\n# Combine\nz &lt;- \"plop\"\nx == 4 & (y == 8 | z == \"plop\")\n\n[1] TRUE\n\n\n\n\n\n\n\nx = 4\ny = 8\n# Equal (==)\nx == y\n\nFalse\n\n# And \nx == 4 and y == 8\n\nTrue\n\n# Or\nx == 4 or y == 8\n\nTrue\n\n# Not\nnot x\n\nFalse\n\n# Combine\nz = \"plop\"\nx == 4 and (y == 8 or z == \"plop\")\n\nTrue\n\n\n\n\nIn data analysis, we usually use operators to subset data. This means that we compare a variable to a value to check if it fits our criteria. For example, if we have a column that contains a year, and we only want observations with the year 2003, we will search for year == 2003. In this setting the R operators we just described will be the same. It is possible that these operators varie when different packages are used in python. For instance, in the context of the pandas package, and becomes &, or becomes |, not becomes ~. We will adress these variations in the database manipulation chapter."
  },
  {
    "objectID": "Intro_programming_loops_ifelse.html",
    "href": "Intro_programming_loops_ifelse.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to create loops and program with if-else statements. These chapters only contain elements that we expect you to know at the end of the course. More detailled books and references exist. For more detailes explanations on specific points, please check: R for Data Science.\nLoops are a useful tool for programming. It allows us to automatically repeat operations. For example if we want to compute indicators for each year in a dataset, we can automatically subset per year, compute indicators, store the results and move on to the next year.\n\n\n\n\n\n\nWarning\n\n\n\nThe biggest difference between R and Python resides in the fact that Python starts at 0 while R starts at 1. You can see that in the results below. range(5) gives 0,1,2,3,4 in python."
  },
  {
    "objectID": "Intro_programming_loops_ifelse.html#loops-if-else-and-mapping",
    "href": "Intro_programming_loops_ifelse.html#loops-if-else-and-mapping",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to create loops and program with if-else statements. These chapters only contain elements that we expect you to know at the end of the course. More detailled books and references exist. For more detailes explanations on specific points, please check: R for Data Science.\nLoops are a useful tool for programming. It allows us to automatically repeat operations. For example if we want to compute indicators for each year in a dataset, we can automatically subset per year, compute indicators, store the results and move on to the next year.\n\n\n\n\n\n\nWarning\n\n\n\nThe biggest difference between R and Python resides in the fact that Python starts at 0 while R starts at 1. You can see that in the results below. range(5) gives 0,1,2,3,4 in python."
  },
  {
    "objectID": "Intro_programming_loops_ifelse.html#making-loops",
    "href": "Intro_programming_loops_ifelse.html#making-loops",
    "title": "NetworkIsLife",
    "section": "Making loops",
    "text": "Making loops\n\nFor Loops\n\n\n\nA loop in R starts with the for operator. This is followed by a condition that determines how the loop runs (“the loop runs for…”). We start by defining a variable that will take the different values in the loop. Suppose we want to print the value 1 to 5. This requires a loop that takes a variables that starts at 1, and increases by one with each iteration. The in operator is used to define the values the variables will take.\nIn the following code we will show different ways to loop:\n\n# range of numbers\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n# items in a vector\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\")\nfor (fruit in fruits) {\n  print(fruit)\n}\n\n[1] \"apple\"\n[1] \"banana\"\n[1] \"cherry\"\n[1] \"date\"\n\n# Loop with an index\nlanguages &lt;- c(\"Python\", \"Java\", \"C++\", \"Ruby\")\nfor (i in 1:length(languages)) {\n  cat(\"Language\", i, \"is\", languages[i], \"\\n\")\n}\n\nLanguage 1 is Python \nLanguage 2 is Java \nLanguage 3 is C++ \nLanguage 4 is Ruby \n\n# custom step\nfor (number in seq(1, 10, by = 2)) {\n  cat(\"Odd number:\", number, \"\\n\")\n}\n\nOdd number: 1 \nOdd number: 3 \nOdd number: 5 \nOdd number: 7 \nOdd number: 9 \n\n# Nested loops\nfor (i in 1:3) {\n  for (j in 1:2) {\n    cat(\"i =\", i, \", j =\", j, \"\\n\")\n  }}\n\ni = 1 , j = 1 \ni = 1 , j = 2 \ni = 2 , j = 1 \ni = 2 , j = 2 \ni = 3 , j = 1 \ni = 3 , j = 2 \n\n# break statement\n# it is possible to stop the loop given a certain condition\nnumbers &lt;- c(3, 7, 1, 9, 4, 2)\nfor (num in numbers) {\n  if (num == 9) {\n    cat(\"Found 9. Exiting the loop.\\n\")\n    break\n  }\n  cat(\"Processing\", num, \"\\n\")\n}\n\nProcessing 3 \nProcessing 7 \nProcessing 1 \nFound 9. Exiting the loop.\n\n\n\n\n\n\nA loop in Python starts with the for operator. This is followed by a condition that determines how the loop runs (“the loop runs for…”). We start by defining a variable that will take the different values in the loop. Suppose we want to print the value 0 to 4. This requires a loop that takes a variables that starts at 0, and increases by one with each iteration. The in operator is used to define the values the variables will take.\nIn the following code we will show different ways to loop:\n\n# a range of numbers\n\nfor i in range(5):\n    print(\"Iteration\", i)\n\nIteration 0\nIteration 1\nIteration 2\nIteration 3\nIteration 4\n\n# items in a list\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\nfor fruit in fruits:\n    print(\"I like\", fruit)\n\nI like apple\nI like banana\nI like cherry\nI like date\n\n# with an index\nlanguages = [\"Python\", \"Java\", \"C++\", \"Ruby\"]\n\nfor i, language in enumerate(languages):\n    print(\"Language\", i + 1, \"is\", language)\n\nLanguage 1 is Python\nLanguage 2 is Java\nLanguage 3 is C++\nLanguage 4 is Ruby\n\n# with a custom step\n\nfor number in range(1, 11, 2):\n    print(\"Odd number:\", number)\n\nOdd number: 1\nOdd number: 3\nOdd number: 5\nOdd number: 7\nOdd number: 9\n\n# Nested loops\n\nfor i in range(3):\n    for j in range(2):\n        print(\"i =\", i, \", j =\", j)\n\ni = 0 , j = 0\ni = 0 , j = 1\ni = 1 , j = 0\ni = 1 , j = 1\ni = 2 , j = 0\ni = 2 , j = 1\n\n# a break statement\nnumbers = [3, 7, 1, 9, 4, 2]\n\n\nfor num in numbers:\n    if num == 9:\n        print(\"Found 9. Exiting the loop.\")\n        break\n    print(\"Processing\", num)\n\nProcessing 3\nProcessing 7\nProcessing 1\nFound 9. Exiting the loop.\n\n\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nWhile R, in general, does not care about the indentation of your code, Python does. If the code is not properly indented, the script will not run.\n\n\n\n\nWhile loops\nWhile loops continue to loop as long as a specific condition is satisfied. The therefore differ from the for loops which have a specified stopping point. The danger with these loops is that they can theoretically run forever if the conditions is always verified. The basic logic of these loops is while followed by a condition and then the code to execute while this condition is verified:\n\n\n\n\n# Example 1: Simple while loop\ncount &lt;- 1\nwhile (count &lt;= 5) {\n  cat(\"Iteration\", count, \"\\n\")\n  count &lt;- count + 1}\n\nIteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \nIteration 5 \n\n# Example 3: Loop with a condition and next statement (equivalent to continue in Python)\ni &lt;- 0\nwhile (i &lt; 10) {\n  i &lt;- i + 1\n  if (i %% 2 == 0) {\n    next }  # Skip even numbers\n  cat(i, \"\\n\")}\n\n1 \n3 \n5 \n7 \n9 \n\n# Example 4: Nested while loops\nrow &lt;- 1\nwhile (row &lt;= 3) {\n  col &lt;- 1\n  while (col &lt;= 3) {\n    cat(\"Row\", row, \"Column\", col, \"\\n\")\n    col &lt;- col + 1 }\n  row &lt;- row + 1}\n\nRow 1 Column 1 \nRow 1 Column 2 \nRow 1 Column 3 \nRow 2 Column 1 \nRow 2 Column 2 \nRow 2 Column 3 \nRow 3 Column 1 \nRow 3 Column 2 \nRow 3 Column 3 \n\n\n\n\n\n\n\n# Example 1: Simple while loop\ncount = 1\nwhile count &lt;= 5:\n    print(\"Iteration\", count)\n    count += 1\n\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nIteration 5\n\n# Example 3: Loop with a condition and continue statement\ni = 0\nwhile i &lt; 10:\n    i += 1\n    if i % 2 == 0:\n        continue  # Skip even numbers\n    print(i)\n\n1\n3\n5\n7\n9\n\n# Example 4: Nested while loops\nrow = 1\nwhile row &lt;= 3:\n    col = 1\n    while col &lt;= 3:\n        print(\"Row\", row, \"Column\", col)\n        col += 1\n    row += 1\n\nRow 1 Column 1\nRow 1 Column 2\nRow 1 Column 3\nRow 2 Column 1\nRow 2 Column 2\nRow 2 Column 3\nRow 3 Column 1\nRow 3 Column 2\nRow 3 Column 3"
  },
  {
    "objectID": "Intro_programming_loops_ifelse.html#conditional-statements-if-else-elif",
    "href": "Intro_programming_loops_ifelse.html#conditional-statements-if-else-elif",
    "title": "NetworkIsLife",
    "section": "Conditional statements (if, else, elif)",
    "text": "Conditional statements (if, else, elif)\n\n\n\n\n# Using if statement\nx &lt;- 10\nif (x &gt; 5) {\n  cat(\"x is greater than 5\\n\")}\n\nx is greater than 5\n\n# Using if-else statement\ny &lt;- 3\nif (y &gt; 5) {\n  cat(\"y is greater than 5\\n\")\n} else {\n  cat(\"y is not greater than 5\\n\")}\n\ny is not greater than 5\n\n# Using if-else if-else statement\nz &lt;- 7\nif (z &gt; 10) {\n  cat(\"z is greater than 10\\n\")\n} else if (z &gt; 5) {\n  cat(\"z is greater than 5 but not greater than 10\\n\")\n} else {\n  cat(\"z is not greater than 5\\n\")}\n\nz is greater than 5 but not greater than 10\n\n# Nested if statements\na &lt;- 15\nb &lt;- 6\nif (a &gt; 10) {\n  if (b &gt; 5) {\n    cat(\"Both a and b are greater than 10 and 5, respectively\\n\")\n  } else {\n    cat(\"a is greater than 10, but b is not greater than 5\\n\")}}\n\nBoth a and b are greater than 10 and 5, respectively\n\n# Using a conditional expression (ternary operator)\nage &lt;- 18\nstatus &lt;- if (age &gt;= 18) \"adult\" else \"minor\"\ncat(\"You are an\", status, \"\\n\")\n\nYou are an adult \n\n\n\n\n\n\n\n# Using if statement\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\n\nx is greater than 5\n\n# Using if-else statement\ny = 3\nif y &gt; 5:\n    print(\"y is greater than 5\")\nelse:\n    print(\"y is not greater than 5\")\n\ny is not greater than 5\n\n# Using if-elif-else statement\nz = 7\nif z &gt; 10:\n    print(\"z is greater than 10\")\nelif z &gt; 5:\n    print(\"z is greater than 5 but not greater than 10\")\nelse:\n    print(\"z is not greater than 5\")\n\nz is greater than 5 but not greater than 10\n\n# Nested if statements\na = 15\nb = 6\nif a &gt; 10:\n    if b &gt; 5:\n        print(\"Both a and b are greater than 10 and 5, respectively\")\n    else:\n        print(\"a is greater than 10, but b is not greater than 5\")\n\nBoth a and b are greater than 10 and 5, respectively\n\n# Using a conditional expression (ternary operator)\nage = 18\nstatus = \"adult\" if age &gt;= 18 else \"minor\"\nprint(\"You are an\", status)\n\nYou are an adult\n\n\n\n\n\nMapping functions\nWhen working with data, we often want to apply a function to all rows of a column, or even on a whole dataframe. For example if we want to remove the capital letters from text or round up numbers to a set number of digits. This could be achieved by looping over all the cells in the dataframe, but there are often more efficient ways of doing this. There are functions that allow us to apply another function to each cell of the column, or dataframe, that we select. This function takes care of the looping behind the scenes.\nIn R we have multiple options for this, some in base R, some from different packages. We will discuss the most notable functions here.\napply sapply mapply lapply\nmap\n\n\n\n\n# All these functions are \n# present in base R\n\n# Example 1: apply function\nmatrix_data &lt;- matrix(1:9, nrow = 3)\n\n# Apply a function to rows (axis 1)\nrow_sums &lt;- apply(matrix_data, 1, sum)\nprint(\"Row Sums:\")\n\n[1] \"Row Sums:\"\n\nprint(row_sums)\n\n[1] 12 15 18\n\n# Apply a function to columns (axis 2)\ncol_sums &lt;- apply(matrix_data, 2, sum)\nprint(\"Column Sums:\")\n\n[1] \"Column Sums:\"\n\nprint(col_sums)\n\n[1]  6 15 24\n\n# Example 2: lapply function\ndata_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\n\n# Apply a function to each element of the list\nsquared_list &lt;- lapply(data_list, function(x) x^2)\nprint(\"Squared List:\")\n\n[1] \"Squared List:\"\n\nprint(squared_list)\n\n$a\n[1] 1 4 9\n\n$b\n[1] 16 25 36\n\n$c\n[1] 49 64 81\n\n# Example 3: sapply function\n# Similar to lapply, but attempts to simplify the result\nsquared_vector &lt;- sapply(data_list, function(x) x^2)\nprint(\"Squared Vector:\")\n\n[1] \"Squared Vector:\"\n\nprint(squared_vector)\n\n     a  b  c\n[1,] 1 16 49\n[2,] 4 25 64\n[3,] 9 36 81\n\n# Example 4: tapply function\n# Used for applying a function by factors\nsales_data &lt;- data.frame(product = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n                         sales = c(100, 150, 120, 180, 90))\n\n# Apply a function to calculate total sales by product\ntotal_sales &lt;- tapply(sales_data$sales, sales_data$product, sum)\nprint(\"Total Sales by Product:\")\n\n[1] \"Total Sales by Product:\"\n\nprint(total_sales)\n\n  A   B \n310 330 \n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# Example 1: NumPy equivalent of apply\nmatrix_data = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n\n# Apply a function to rows (axis 1)\nrow_sums = np.apply_along_axis(np.sum, axis=1, arr=matrix_data)\nprint(\"Row Sums:\")\n\nRow Sums:\n\nprint(row_sums)\n\n[12 15 18]\n\n# Apply a function to columns (axis 0)\ncol_sums = np.apply_along_axis(np.sum, axis=0, arr=matrix_data)\nprint(\"Column Sums:\")\n\nColumn Sums:\n\nprint(col_sums)\n\n[ 6 15 24]\n\n# List \ndata_list = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\n\n# Apply a function to each element of the dictionary values\nsquared_list = {key: [x**2 for x in value] for key, value in data_list.items()}\nprint(\"Squared Dictionary:\")\n\nSquared Dictionary:\n\nprint(squared_list)\n\n{'a': [1, 4, 9], 'b': [16, 25, 36], 'c': [49, 64, 81]}\n\n# Example 3: pandas\ndata_dict = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\ndf = pd.DataFrame(data_dict)\n\n# Apply a function to each column (series) and return a DataFrame\nsquared_df = df.apply(lambda x: x**2)\nprint(squared_df)\n\n   a   b   c\n0  1  16  49\n1  4  25  64\n2  9  36  81\n\n# pandas \nsales_data = pd.DataFrame({'product': ['A', 'B', 'A', 'B', 'A'],\n                           'sales': [100, 150, 120, 180, 90]})\n\n# Apply a function to calculate total sales by product\ntotal_sales = sales_data.groupby('product')['sales'].sum()\nprint(\"Total Sales by Product:\")\n\nTotal Sales by Product:\n\nprint(total_sales)\n\nproduct\nA    310\nB    330\nName: sales, dtype: int64\n\n\n\n\n\nThe map() function in R\nIn R we also have the map() function, this function is part of the purrr package. map() is able to apply a function of your chosing to each element of a list, vector or other data strucutr. The basic syntax for the function is:\n\nmap(.x, .f, ...)\n\nThe .x argument is the data structure you wish to apply the function to. The .f argument will be the function you will be applying to the dataframe (for example, gsub(), as.integer(), tolower(), etc.)\nThe main advantage of map() is that it is a one format solution while the base R functions (apply, lappply, sapply and tapply) all have different syntaxes and behaviour. map() works on all and behaves identically on all datastructures.\nHere is an example of how to use this function:\n\nlibrary(purrr)\ntest_data = list(3,9,12,15,18)\n\n# we will apply the sqrt() function to these numbers\n\ntest_data_sqrt = map(test_data, ~ sqrt(.))\nprint(test_data_sqrt)\n\n[[1]]\n[1] 1.732051\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 3.464102\n\n[[4]]\n[1] 3.872983\n\n[[5]]\n[1] 4.242641\n\n\nIt also works for dataframes:\n\ntest_dataframe = data.frame(\"value\" = c(3,9,12,15,18))\ntest_data_sqrt = map(test_dataframe, ~ sqrt(.))\nprint(test_data_sqrt)\n\n$value\n[1] 1.732051 3.000000 3.464102 3.872983 4.242641\n\n\nIt also works for a matrix:\n\ntest_matrix = matrix(c(3,9,12,15,18), nrow = 5, ncol = 1)\ntest_data_sqrt = map(test_matrix, ~ sqrt(.))\nprint(test_data_sqrt)\n\n[[1]]\n[1] 1.732051\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 3.464102\n\n[[4]]\n[1] 3.872983\n\n[[5]]\n[1] 4.242641\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nIt is possible to use map() on a specific colomn in a dataframe. You will need to pay close attention to select this column correctly. Applying the function to a matrix will result in the matrix being reverted back to a list. For this reason we add the unlist() function so that the results can be returned in matrix format.\n\n\n\ntest_dataframe = data.frame(\"value\" = c(3,9,12,15,18), \"year\" = c(2000,2004,2018,2021,2025))\ntest_dataframe$value = map(test_dataframe$value, ~ sqrt(.))\nprint(test_dataframe)\n\n     value year\n1 1.732051 2000\n2        3 2004\n3 3.464102 2018\n4 3.872983 2021\n5 4.242641 2025\n\n\n\ntest_matrix = matrix(c(3,9,12,15,18,2000,2004,2018,2021,2025), nrow = 5, ncol = 2)\ntest_matrix[,1] = unlist(map(test_matrix[,1],  ~ sqrt(.)))\n# Using pipes we can also write an equivalent form:\n# test_matrix[,1] = test_matrix[,1] %&gt;% map( ~ sqrt(.)) %&gt;% unlist()\nprint(test_matrix)\n\n         [,1] [,2]\n[1,] 1.732051 2000\n[2,] 3.000000 2004\n[3,] 3.464102 2018\n[4,] 3.872983 2021\n[5,] 4.242641 2025"
  },
  {
    "objectID": "Intro_programming_functions.html",
    "href": "Intro_programming_functions.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to create a function\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Intro_programming_functions.html#making-functions",
    "href": "Intro_programming_functions.html#making-functions",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to create a function\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Intro_programming_visualisation.html",
    "href": "Intro_programming_visualisation.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to visualize data\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Intro_programming_visualisation.html#data-visualisation",
    "href": "Intro_programming_visualisation.html#data-visualisation",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to visualize data\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html",
    "href": "Intro_programming_DataImportAndPrep.html",
    "title": "Importing in R",
    "section": "",
    "text": "How to import different dataformats in python and R."
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#data-import-and-preparation",
    "href": "Intro_programming_DataImportAndPrep.html#data-import-and-preparation",
    "title": "Importing in R",
    "section": "",
    "text": "How to import different dataformats in python and R."
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#txt-csv-tsv",
    "href": "Intro_programming_DataImportAndPrep.html#txt-csv-tsv",
    "title": "Importing in R",
    "section": "TXT, CSV, TSV",
    "text": "TXT, CSV, TSV\nFor the import of data from txt, csv or tsv formats we will use the read_delim() function from the readr package. The main difference between these data formats resides in the separator of the data. For txt data, this is often undefined and needs to be specified by the user. The main arguments of the read_delim() are the file and the separator. The separator is the character string that defines where a line should be cut. For example, if our raw data looks like this:\nYear;Class;Value 2002;B60C;42 2003;B29K;21 2009;C08K;12\nThen we see that each column is separated by a “;”. By splitting the data whenever there is a “;” we create the following dataframe:\n\n\n\nYear\nClass\nValue\n\n\n\n\n2002\nB60C\n42\n\n\n2003\nB29K\n21\n\n\n2009\nC08K\n12\n\n\n\nSeperators that we commonly find in data are the semicolon: “;”, the comma: “,”, the vertical bar (or pipe) “|”, the space **” “, and tabs which are coded with**”\\t”.\n\n\n\n\n\n\nPay Attention\n\n\n\nEven though csv stands for Comma Separated Values, this does not mean that the separator is always a comma, often enough it is in fact a semicolon. Always check your data to make sure you have the right one.\n\n\n\nlibrary(readr)\n# csv and various other separators\ndata &lt;- read_delim(file, sep = \",\")\n\ndata &lt;- read_delim(file, sep = \"|\")\n\ndata &lt;- read_delim(file, sep = \";\")\n\n# txt (space separated values)\ndata &lt;- read_delim(file, sep = \" \")\n\n# tsv (tab separated values)\ndata &lt;- read_delim(file, sep = \"\\t\")"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#xls-xlsx",
    "href": "Intro_programming_DataImportAndPrep.html#xls-xlsx",
    "title": "Importing in R",
    "section": "XLS, XLSX",
    "text": "XLS, XLSX\nExcel files can also easily be importanted into R with the the readxl package.\n\nlibrary(readxl)\nread_excel(\"sample_data.xlsx\")\n\n# A tibble: 5 × 3\n  Pat_num       Year Domain\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; \n1 WO200214562   2002 B60C  \n2 WO2023738962  2023 B60C  \n3 EP2023778962  2023 B29D  \n4 FR2019272698  2019 C08K  \n5 FR201922671   2019 C08K"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#rdata",
    "href": "Intro_programming_DataImportAndPrep.html#rdata",
    "title": "Importing in R",
    "section": "rdata",
    "text": "rdata\n\n\n# use import function in rstudio, or double click"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#bib",
    "href": "Intro_programming_DataImportAndPrep.html#bib",
    "title": "Importing in R",
    "section": "Bib",
    "text": "Bib\n.bib files are a popular bibliographic data format. This data can be imported and transformed into a dataframe using the bibliometrix package.\n\nlibrary(bibliometrix)\nconvert2df(file, dbsource = \"scopus\", format = \"bibtex\")\n\n\n\n\n\n\n\nPay Attention\n\n\n\nThe bibliometrix package is designed for bibliometic analysis, it might change the names of the columns and the format of some of the data to adjust to what it is supped to do."
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#json",
    "href": "Intro_programming_DataImportAndPrep.html#json",
    "title": "Importing in R",
    "section": "json",
    "text": "json\n\nlibrary(jsonlite)\n\n# Specify the path to the JSON file\njson_file &lt;- \"example_1.json\"\n\n# Import the JSON file\ndata &lt;- fromJSON(json_file)\n\n# Display the imported JSON data\nprint(data)\n\n$fruit\n[1] \"Apple\"\n\n$size\n[1] \"Large\"\n\n$color\n[1] \"Red\""
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#jsonl",
    "href": "Intro_programming_DataImportAndPrep.html#jsonl",
    "title": "Importing in R",
    "section": "jsonl",
    "text": "jsonl\njsonl files are a list of json files. We find this format for exampl in the lens.org database. Since each row is a json, we first read the files as text files and then apply the fromJSON function to extract the information. The result is a list of data objects.\n\nlibrary(jsonlite)\ntmp&lt;-readLines(\"data_test.jsonl\")\n\nWarning in readLines(\"data_test.jsonl\"): incomplete final line found on\n'data_test.jsonl'\n\ntmp &lt;- lapply(tmp, jsonlite::fromJSON)\n\n\n\nClick here to see output\n\n\nprint(tmp)\n\n[[1]]\n[[1]]$lens_id\n[1] \"000-136-263-451-165\"\n\n[[1]]$title\n[1] \"The influence of displaced femoral neck fractures on the surgical incision for hemiarthroplasty using the Hardinge approach\"\n\n[[1]]$publication_type\n[1] \"journal article\"\n\n[[1]]$year_published\n[1] 2002\n\n[[1]]$date_published_parts\n[1] 2002    1\n\n[[1]]$created\n[1] \"2018-05-12T00:07:16.368000+00:00\"\n\n[[1]]$external_ids\n   type              value\n1 magid         2083116342\n2   doi 10.1007/bf03170381\n\n[[1]]$authors\n  first_name last_name initials               ids\n1         M.    Yousef        M magid, 2518579655\n2         E. Masterson        E magid, 2607514729\n                                                                                                                                                                                    affiliations\n1 Mid-Western Regional Hospital, Department of Orthopaedic Surgery, Mid-Western Regional Hospital, Limerick, Ireland., magid, grid, ror, 2799956115, grid.415964.b, 00r5kgd36, grid.415964.b, IE\n2 Mid-Western Regional Hospital, Department of Orthopaedic Surgery, Mid-Western Regional Hospital, Limerick, Ireland., magid, grid, ror, 2799956115, grid.415964.b, 00r5kgd36, grid.415964.b, IE\n\n[[1]]$source\n[[1]]$source$title\n[1] \"Irish Journal of Medical Science\"\n\n[[1]]$source$type\n[1] \"Journal\"\n\n[[1]]$source$publisher\n[1] \"Springer Science and Business Media LLC\"\n\n[[1]]$source$issn\n        type    value\n1      print 00211265\n2 electronic 18634362\n3      print 03321029\n4      print 0790231x\n5      print 07902328\n\n[[1]]$source$country\n[1] \"Ireland\"\n\n[[1]]$source$asjc_codes\n[1] \"2700\"\n\n[[1]]$source$asjc_subjects\n[1] \"General Medicine\"\n\n\n[[1]]$fields_of_study\n[1] \"Surgery\"                \"Surgical incision\"      \"Hardinge approach\"     \n[4] \"Femoral Neck Fractures\" \"Medicine\"              \n\n[[1]]$volume\n[1] \"171\"\n\n[[1]]$issue\n[1] \"1\"\n\n[[1]]$languages\n[1] \"en\"\n\n[[1]]$source_urls\n  type                                                          url\n1 html         https://link.springer.com/article/10.1007/BF03170381\n2 &lt;NA&gt; https://link.springer.com/content/pdf/10.1007/BF03170381.pdf\n\n[[1]]$start_page\n[1] \"31\"\n\n[[1]]$end_page\n[1] \"31\"\n\n[[1]]$author_count\n[1] 2\n\n\n[[2]]\n[[2]]$lens_id\n[1] \"000-150-493-965-760\"\n\n[[2]]$title\n[1] \"Peroxidase activity in soybeans following inoculation with Phytophthora sojae.\"\n\n[[2]]$publication_type\n[1] \"journal article\"\n\n[[2]]$year_published\n[1] 2006\n\n[[2]]$date_published_parts\n[1] 2006    1\n\n[[2]]$created\n[1] \"2018-05-12T03:57:41.820000+00:00\"\n\n[[2]]$external_ids\n   type                     value\n1   doi 10.1007/s11046-005-0721-y\n2  pmid                  16389483\n3 magid                2006874664\n\n[[2]]$authors\n  first_name last_name initials               ids\n1    Jose C.    Melgar       JC magid, 2634470543\n2  Thomas S.     Abney       TS magid, 2677788953\n3 Richard A.  Vierling       RA magid, 1981461697\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       affiliations\n1                                                                                                                                                                                                                                                                      Purdue University, Botany and Plant Pathology Department, Purdue University, West Lafayette, IN 47907, USA., magid, grid, fundref, wikidata, ror, 219193219, grid.169077.e, 100006377, q217741, 02dqehb95, grid.169077.e, US\n2 United States Department of Agriculture, Agricultural Research Service, Crop Production and Pathology Unit, Agricultural Research Service, U.S. Department of Agriculture, West Lafayette, USA, Crop Production and Pathology Unit, Agricultural Research Service, U.S. Department of Agriculture, West Lafayette, USA, magid, grid, fundref, ror, 1336096307, grid.417548.b, 100000199, 01na82s61, grid, fundref, ror, grid.463419.d, 100007917, 02d2m2044, grid.417548.b, grid.463419.d, US, US\n3                                                                                                                                                                                                                                                           Purdue University, Botany & Plant Pathology Department and Agronomy Department, Purdue University, West Lafayette, USA, magid, grid, fundref, wikidata, ror, 219193219, grid.169077.e, 100006377, q217741, 02dqehb95, grid.169077.e, US\n\n[[2]]$source\n[[2]]$source$title\n[1] \"Mycopathologia\"\n\n[[2]]$source$type\n[1] \"Journal\"\n\n[[2]]$source$publisher\n[1] \"Springer Science and Business Media LLC\"\n\n[[2]]$source$issn\n        type    value\n1      print 0301486x\n2 electronic 15730832\n\n[[2]]$source$country\n[1] \"Netherlands\"\n\n[[2]]$source$asjc_codes\n[1] \"2404\" \"2402\" \"1110\" \"1102\" \"3401\"\n\n[[2]]$source$asjc_subjects\n[1] \"Agronomy and Crop Science\"             \n[2] \"Applied Microbiology and Biotechnology\"\n[3] \"Microbiology\"                          \n[4] \"Plant Science\"                         \n[5] \"Veterinary (miscalleneous)\"            \n[6] \"Veterinary (miscellaneous)\"            \n\n\n[[2]]$fields_of_study\n[1] \"Gene\"               \"Horticulture\"       \"Peroxidase\"        \n[4] \"Inoculation\"        \"Phytophthora sojae\" \"Phytophthora\"      \n[7] \"Botany\"             \"Biology\"           \n\n[[2]]$volume\n[1] \"161\"\n\n[[2]]$issue\n[1] \"1\"\n\n[[2]]$languages\n[1] \"en\"\n\n[[2]]$references\n               lens_id\n1  004-042-571-377-321\n2  005-317-066-947-203\n3  020-042-216-048-371\n4  032-446-103-377-888\n5  034-601-439-886-731\n6  062-700-560-648-136\n7  066-443-980-641-011\n8  088-133-232-334-221\n9  106-456-828-269-00X\n10 168-522-110-138-272\n11 192-341-655-765-695\n\n[[2]]$mesh_terms\n    mesh_heading       qualifier_name mesh_id qualifier_id\n1    Peroxidases           metabolism D010544      Q000378\n2   Phytophthora growth & development D010838      Q000254\n3 Plant Diseases         microbiology D010935      Q000382\n4 Plant Proteins           metabolism D010940      Q000378\n5      Seedlings           enzymology D036226      Q000201\n6       Soybeans           enzymology D013025      Q000201\n\n[[2]]$chemicals\n  substance_name registry_number mesh_id\n1 Plant Proteins               0 D010940\n2    Peroxidases     EC 1.11.1.- D010544\n\n[[2]]$source_urls\n  type                                                         url\n1 html https://link.springer.com/article/10.1007/s11046-005-0721-y\n2 html                https://www.ncbi.nlm.nih.gov/pubmed/16389483\n3 html                  https://europepmc.org/article/MED/16389483\n\n[[2]]$references_count\n[1] 11\n\n[[2]]$scholarly_citations_count\n[1] 12\n\n[[2]]$start_page\n[1] \"37\"\n\n[[2]]$end_page\n[1] \"42\"\n\n[[2]]$scholarly_citations\n [1] \"027-668-154-909-322\" \"031-369-742-281-943\" \"034-566-544-087-375\"\n [4] \"041-477-983-598-256\" \"041-552-411-498-281\" \"042-968-597-273-042\"\n [7] \"045-790-949-621-105\" \"062-868-088-589-29X\" \"107-788-244-832-10X\"\n[10] \"132-744-976-798-754\" \"153-187-658-237-284\" \"160-817-571-772-644\"\n\n[[2]]$author_count\n[1] 3\n\n\n[[3]]\n[[3]]$lens_id\n[1] \"000-161-224-328-417\"\n\n[[3]]$title\n[1] \"Astronomy: Planets in chaos\"\n\n[[3]]$publication_type\n[1] \"journal article\"\n\n[[3]]$year_published\n[1] 2014\n\n[[3]]$date_published\n[1] \"2014-07-02T00:00:00.000000+00:00\"\n\n[[3]]$date_published_parts\n[1] 2014    7    2\n\n[[3]]$created\n[1] \"2018-05-12T07:57:28.771000+00:00\"\n\n[[3]]$external_ids\n   type           value\n1 magid      2014980602\n2  pmid        24990727\n3   doi 10.1038/511022a\n\n[[3]]$open_access\n[[3]]$open_access$colour\n[1] \"bronze\"\n\n\n[[3]]$authors\n  first_name  last_name initials               ids affiliations\n1        Ann Finkbeiner        A magid, 2633978770         NULL\n\n[[3]]$source\n[[3]]$source$title\n[1] \"Nature\"\n\n[[3]]$source$type\n[1] \"Journal\"\n\n[[3]]$source$publisher\n[1] \"Springer Science and Business Media LLC\"\n\n[[3]]$source$issn\n        type    value\n1 electronic 14764687\n2      print 00280836\n\n[[3]]$source$country\n[1] \"United Kingdom\"\n\n[[3]]$source$asjc_codes\n[1] \"1000\"\n\n[[3]]$source$asjc_subjects\n[1] \"Multidisciplinary\"\n\n\n[[3]]$fields_of_study\n[1] \"Exoplanetology\"           \"Physics\"                 \n[3] \"Astronomy\"                \"Star (graph theory)\"     \n[5] \"Planetary science\"        \"CHAOS (operating system)\"\n[7] \"Planet\"                   \"Astrobiology\"            \n\n[[3]]$volume\n[1] \"511\"\n\n[[3]]$issue\n[1] \"7507\"\n\n[[3]]$languages\n[1] \"en\"\n\n[[3]]$references\n              lens_id\n1 002-401-641-595-121\n2 002-872-990-703-478\n3 031-880-830-498-497\n4 082-960-570-095-64X\n5 149-177-951-839-803\n\n[[3]]$source_urls\n     type                                                           url\n1 unknown                   https://www.nature.com/articles/511022a.pdf\n2    html                       https://www.nature.com/articles/511022a\n3    html http://ui.adsabs.harvard.edu/abs/2014Natur.511...22F/abstract\n4    html                    https://europepmc.org/article/MED/24990727\n\n[[3]]$abstract\n[1] \"The discovery of thousands of star systems wildly different from our own has demolished ideas about how planets form. Astronomers are searching for a whole new theory.\"\n\n[[3]]$references_count\n[1] 5\n\n[[3]]$scholarly_citations_count\n[1] 3\n\n[[3]]$start_page\n[1] \"22\"\n\n[[3]]$end_page\n[1] \"24\"\n\n[[3]]$scholarly_citations\n[1] \"020-515-110-880-837\" \"084-952-180-195-058\" \"086-879-116-036-663\"\n\n[[3]]$author_count\n[1] 1\n\n[[3]]$is_open_access\n[1] TRUE"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#txt-csv-tsv-1",
    "href": "Intro_programming_DataImportAndPrep.html#txt-csv-tsv-1",
    "title": "Importing in R",
    "section": "TXT, CSV, TSV",
    "text": "TXT, CSV, TSV\npackage required openpyxl, pandas\n\nimport pandas as pd\n\n# Import CSV File\ncsv_file = \"sample_data.csv\"\ndf_csv = pd.read_csv(csv_file, sep = \";\")\n\n\n# Import TXT File (Space-Separated)\ntxt_file = \"sample_data.txt\"\ndf_txt = pd.read_csv(txt_file, sep=\" \")\n\n# Import TSV File (Tab-Separated)\ntsv_file = \"sample_data.tsv\"\ndf_tsv = pd.read_csv(tsv_file, sep=\"\\t\")\n\n\n# Import XLS (Excel) File\nxls_file = \"sample_data.xlsx\"\ndf_xls = pd.read_excel(xls_file)\nprint(df_xls)\n\n# Import JSON File\n#json_file = \"sample_data.json\"\n#df_json = pd.read_json(json_file)"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#xlsxlsx-excel-data-format",
    "href": "Intro_programming_DataImportAndPrep.html#xlsxlsx-excel-data-format",
    "title": "Importing in R",
    "section": "XLS/XLSX (excel data format)",
    "text": "XLS/XLSX (excel data format)\nExcel files can be imported with help from the pandas package. If you do not specify a sheet to import, the first sheet will be taken. You can specify the sheet with the argument: sheet_name=‘Sheet1’\n\nimport pandas as pd\n# Import XLS (Excel) File\ndf_xls = pd.read_excel(\"sample_data.xlsx\")\nprint(df_xls)"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#json-and-jsonl",
    "href": "Intro_programming_DataImportAndPrep.html#json-and-jsonl",
    "title": "Importing in R",
    "section": "Json and jsonl",
    "text": "Json and jsonl\n\n# Import JSON File\n#json_file = \"sample_data.json\"\n#df_json = pd.read_json(json_file)\n\n\n# import jsonl\nimport jsonlines\n\n# First we open the file and provide it with a name (data_from_jsonl)\nwith jsonlines.open('data_test.jsonl', 'r') as data_from_jsonl:\n      # extract the information from each row and put into object\n     results = [row for row in data_from_jsonl] \ndata_from_jsonl.close() # close the file again"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#data-conformity-in-r",
    "href": "Intro_programming_DataImportAndPrep.html#data-conformity-in-r",
    "title": "Importing in R",
    "section": "Data conformity in R",
    "text": "Data conformity in R\nBefore you start working on your analysis, you want to make sure the data is conform to your expectations. This means:\n\nThe number of observation is correct, and the variables you need are included.\nMaking sure each column is of the right type (years are dates, text are factors, numbers are integers or reals, …)\nThere are no trailing spaces\n“.” and “,” aren’t mixed up\nThe units of measurement are correct (km vs. miles, K€ vs 1.000, celsius vs fahrenheit etc.)\nAny missing values have been dealt with, or you have a valid reason to ignore them\nheaders are easy to use (no spaces or other characters that will make your life difficult).\n\nIn this section we will look into checking your data and adjusting it in preparation of your analysis.\n\nChecking data types\nThere are many different data formats. Most frequently we encounter numbers, strings, factors, dates and booleans. Consider the following dataset:\n\n\n\nID\nYear\nCool Domain\nFrequency\nAlive\n\n\n\n\nID1\n2002\nB60C\n42\nTRUE\n\n\nID2\n2003\nB29K\n21\nFALSE\n\n\nID3\n2009\nC08K\n12\nTRUE\n\n\n\nThe first column has textual data that is used as a key for each row. The Year should be considered either as an integer or a date. This will depend upon the type of analysis you want to perform. Specifically time series analysis will require this to be a date, while it’s enough for other analyses to ensure that it is an integer. The most common issue with this type of observation is that the year is considered as text instead of a number. If you see any ““” next to your numbers this is a bad sign which indicates that it is in fact, text.\nThe “Cool Domain” contains classifications which we want to be textual values. “Frequency” will need to be an integer while “Alive” we want to be a boolean. For each of the formats there is a function to test whether or not the format is the one we expect. These functions are usually in the form of is.numerical(), is.logical(), is.character(). The argument is then simply the column of the dataframe or an observation. Suppose we create the previous dataframe in R:\n\ndata &lt;- data.frame(ID = c(\"ID1\", \"ID2\", \"ID3\"), Year = c(2002, 2003, 2009), `Cool Domain` = c(\"B60C\", \"B29K\",\"B29K\"), Frequency = c(42, 21, 12),Alive = c(TRUE, FALSE, TRUE))\n\n\nprint(data)\n\n   ID Year Cool.Domain Frequency Alive\n1 ID1 2002        B60C        42  TRUE\n2 ID2 2003        B29K        21 FALSE\n3 ID3 2009        B29K        12  TRUE\n\n\nWe can then check the format of each variables:\n\nis.numeric(data$Frequency)\n\n[1] TRUE\n\nis.character(data$ID)\n\n[1] TRUE\n\nis.logical(data$Alive)\n\n[1] TRUE\n\n\nThis can also be done in a more consise way with the summary() function. This compute, for each of the columns, some statistics and provides the class of the class of the variables. This comes in handy as it allows us verify not only the class of the variables but also the distribution:\n\nsummary(data)\n\n      ID                 Year      Cool.Domain          Frequency   \n Length:3           Min.   :2002   Length:3           Min.   :12.0  \n Class :character   1st Qu.:2002   Class :character   1st Qu.:16.5  \n Mode  :character   Median :2003   Mode  :character   Median :21.0  \n                    Mean   :2005                      Mean   :25.0  \n                    3rd Qu.:2006                      3rd Qu.:31.5  \n                    Max.   :2009                      Max.   :42.0  \n   Alive        \n Mode :logical  \n FALSE:1        \n TRUE :2        \n                \n                \n                \n\n\nWhen the variable is a character, the output shows the length (number of observations) and the class. For numeric variables the output shows the distribution of the data (min, max, etc.). Not all text variables are created equal. It is possible to ensure that text is considered as a factor. A factor is basically a category, dummy variables in regressions are usually programmed as factors.\n\ndata2 &lt;- data.frame(ID = c(\"ID1\", \"ID2\", \"ID3\", \"ID4\"), Year = c(2002, 2003, 2009, 2010), `Cool Domain` = c(\"B60C\", \"B29K\",\"B29K\", \"B60C\"), Frequency = c(42, 21, 12, NA),Alive = c(TRUE, FALSE, FALSE, FALSE), Country = as.factor(c(\"France\", \"France\", \"Germany\", \"Netherlands\")))\n\nsummary(data2)\n\n      ID                 Year      Cool.Domain          Frequency   \n Length:4           Min.   :2002   Length:4           Min.   :12.0  \n Class :character   1st Qu.:2003   Class :character   1st Qu.:16.5  \n Mode  :character   Median :2006   Mode  :character   Median :21.0  \n                    Mean   :2006                      Mean   :25.0  \n                    3rd Qu.:2009                      3rd Qu.:31.5  \n                    Max.   :2010                      Max.   :42.0  \n                                                      NA's   :1     \n   Alive                Country \n Mode :logical   France     :2  \n FALSE:3         Germany    :1  \n TRUE :1         Netherlands:1  \n                                \n                                \n                                \n                                \n\n\nIn this output we see that the NA is taken into account and counted separedly in the date. It also shows that “Country” is considered as a factor. The summary then shows how many observations we have for each of the factors.\nGiven that we created the dataframe ourselves we can be relatively sure that the data was is the right format. However, often enough when we download data from online sources, we run into issues. If the format is not as expected there are different functions that allows us to transform the format of the data. Mainly we use as.numeric(), as.character() and as.logical(). These functions require only one argument which is the column we are trying to convert:\n\n# transform into a numeric value\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\n# transform in to a character string\ndata$Frequency &lt;- as.character(data$ID)\n\n# transform into a boolean\ndata$Frequency &lt;- as.logical(data$Alive)\n\nAs an example consider we will create a dataframe that mimics data that was not imported correctly:\n\nMessy_data &lt;- data.frame(ID = c(\"ID1\", \"ID2\", \"ID3\", \"ID4\"), Year = c(\"2002\", \"2003\", \"2009\", \"2010\"), `Cool Domain` = c(\"B60C\", \"B29K\",\"B29K\", \"B60C\"), Frequency = c(\"42\", \"21\", \"12\", \"NA\"),Alive = c(\"TRUE\", \"FALSE\", \"FALSE\", \"FALSE\"), Country = c(\"France\", \"France\", \"Germany\", \"Netherlands\"))\n\nsummary(Messy_data)\n\n      ID                Year           Cool.Domain         Frequency        \n Length:4           Length:4           Length:4           Length:4          \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n    Alive             Country         \n Length:4           Length:4          \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\nThe output clearly shows that all variables are considered as characters which we do not want. We cannot comput the average on the Frequency while R thinks that it is text. This also shows that there is a difference between the string NA (“NA”) and the operator NA.\n\n# transform into a numerical value\nMessy_data$Year &lt;- as.numeric(Messy_data$Year)\nMessy_data$Frequency &lt;- as.numeric(Messy_data$Frequency)\n\nWarning: NAs introduced by coercion\n\n# transform into a boolean\nMessy_data$Alive &lt;- as.logical(Messy_data$Alive)\n# transform into a factor (category)\nMessy_data$Country &lt;- as.factor(Messy_data$Country)\n\nsummary(Messy_data)\n\n      ID                 Year      Cool.Domain          Frequency   \n Length:4           Min.   :2002   Length:4           Min.   :12.0  \n Class :character   1st Qu.:2003   Class :character   1st Qu.:16.5  \n Mode  :character   Median :2006   Mode  :character   Median :21.0  \n                    Mean   :2006                      Mean   :25.0  \n                    3rd Qu.:2009                      3rd Qu.:31.5  \n                    Max.   :2010                      Max.   :42.0  \n                                                      NA's   :1     \n   Alive                Country \n Mode :logical   France     :2  \n FALSE:3         Germany    :1  \n TRUE :1         Netherlands:1  \n                                \n                                \n                                \n                                \n\n\nNow that our data has the right format and we are sure we have all the observations we might want to do some work to normalise the column names. This is done to make programming and refercing the columns easier. Whenever the is a space in the name of the column, we need to refer to the column using `Column name with spaces` which is not ideal. It’s generally good practice to not have any spaces. We can use the colnames() function to change some (or all of the column names).\n\n# change on column name:\ncolnames(Messy_data)[3] &lt;- \"Domain\"\n\n# change multiple names:\ncolnames(Messy_data)[c(1,2,5)] &lt;- c(\"Identifier\", \"Date\", \"Active\")\n\n# check\nsummary(Messy_data)\n\n  Identifier             Date         Domain            Frequency   \n Length:4           Min.   :2002   Length:4           Min.   :12.0  \n Class :character   1st Qu.:2003   Class :character   1st Qu.:16.5  \n Mode  :character   Median :2006   Mode  :character   Median :21.0  \n                    Mean   :2006                      Mean   :25.0  \n                    3rd Qu.:2009                      3rd Qu.:31.5  \n                    Max.   :2010                      Max.   :42.0  \n                                                      NA's   :1     \n   Active               Country \n Mode :logical   France     :2  \n FALSE:3         Germany    :1  \n TRUE :1         Netherlands:1  \n                                \n                                \n                                \n                                \n\n\nThe Janitor package offers an automated solution for many header issues.\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nMessy_data &lt;- data.frame(ID = c(\"ID1\", \"ID2\", \"ID3\", \"ID4\"), Year = c(\"2002\", \"2003\", \"2009\", \"2010\"), `Cool Domain` = c(\"B60C\", \"B29K\",\"B29K\", \"B60C\"), Frequency = c(\"42\", \"21\", \"12\", \"NA\"),Alive = c(\"TRUE\", \"FALSE\", \"FALSE\", \"FALSE\"), Country = c(\"France\", \"France\", \"Germany\", \"Netherlands\"))\n# the column names from the initial dataframe\nprint(colnames(Messy_data))\n\n[1] \"ID\"          \"Year\"        \"Cool.Domain\" \"Frequency\"   \"Alive\"      \n[6] \"Country\"    \n\n# Cleaning the column names\nMessy_data &lt;- clean_names(Messy_data)\nprint(colnames(Messy_data))\n\n[1] \"id\"          \"year\"        \"cool_domain\" \"frequency\"   \"alive\"      \n[6] \"country\"    \n\n\nYou can see that capital letter have been adjusted, some characters (µ, “)” ) have been replaced, spaces have been removed and replaced with “_“. This makes it easier and faster to refer to the columns and the dataframes when manipulating the data on a day-to-day basis.\nOnce we are happy with the data formats we can move on take care of other issues.\n\nData transformation\nIt often happens that we are not fully happy with the data that we receive or simply that we need apply some modifications to suit our needs. The most frequent of these operations are\n\nMathematical transformation of numerical data (multiplication, rounding up)\nExtract years, days or months from dates\nTrim whitespaces from data\nRename categories within the data\n\nThe main difficulty with the actions is to identify how to apply the functions. Some functions can be applied directly to a whole column while other nedd to be applied using mapping functions. Let’s start with mathematical transformations:\n\n# we start by creating a dataset to work with\ndata &lt;- data.frame(Identifier = c(\"ID1\", \"ID2\", \"ID3\", \"ID4 \"), Date = c(2002, 2003, 2009, 2010), Domain = c(\"B60C\", \"B29K\",\"B29K \", \"B60C\"), Distance = c(62.1371, 124.274, 93.2057, 186.411), Active = c(TRUE, FALSE, FALSE, FALSE), Country = as.factor(c(\"France\", \"France\", \"Germany\", \"Netherlands\")))\n\n# The Distance is given in miles in this data, we would like to have kilometers. The formula to transform a mile to kilometers is a multiplication by 1.609.\n\ndata$KM &lt;- data$Distance * 1.609344\n\nprint(data)\n\n  Identifier Date Domain Distance Active     Country        KM\n1        ID1 2002   B60C  62.1371   TRUE      France  99.99997\n2        ID2 2003   B29K 124.2740  FALSE      France 199.99962\n3        ID3 2009  B29K   93.2057  FALSE     Germany 150.00003\n4       ID4  2010   B60C 186.4110  FALSE Netherlands 299.99942\n\n\nWe can also round this up. The round() function takes two arguments, a first will be the data, the second will be the number of decimals:\n\ndata$KM &lt;- round(data$KM, 2)\nprint(data)\n\n  Identifier Date Domain Distance Active     Country  KM\n1        ID1 2002   B60C  62.1371   TRUE      France 100\n2        ID2 2003   B29K 124.2740  FALSE      France 200\n3        ID3 2009  B29K   93.2057  FALSE     Germany 150\n4       ID4  2010   B60C 186.4110  FALSE Netherlands 300\n\n\nWe notice that for some of the domains there is a trailing whitespace, for example we have “B29K” in the data. These can be removed with the trimws() function. These whitespaces can also be created when we split data. It’s good practise to always check this and remove any space to avoid unnecessary mistakes.\n\ndata$Domain &lt;- trimws(data$Domain)\nprint(data)\n\n  Identifier Date Domain Distance Active     Country  KM\n1        ID1 2002   B60C  62.1371   TRUE      France 100\n2        ID2 2003   B29K 124.2740  FALSE      France 200\n3        ID3 2009   B29K  93.2057  FALSE     Germany 150\n4       ID4  2010   B60C 186.4110  FALSE Netherlands 300\n\n\nSuppose that we want to change the labels for the countries. It’s much more common to use ISO-2 level codes for countries. The gsub() function for the replacement of text. The gsub() function requires three arguments: the patter to search, the pattern to replace with and the data. Caution while using this function since it searches for the pattern even withing words. If one searches for the string “land” to replace it with “country”, then the string “netherlands” will become “nethercountrys”.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n  data$Country &lt;- gsub(\"France\", \"FR\", data$Country)\n  data$Country &lt;- gsub(\"Netherlands\" , \"NL\",data$Country)\n  data$Country &lt;- gsub(\"Germany\" ,\"DE\",data$Country)\n\nprint(data)\n\n  Identifier Date Domain Distance Active Country  KM\n1        ID1 2002   B60C  62.1371   TRUE      FR 100\n2        ID2 2003   B29K 124.2740  FALSE      FR 200\n3        ID3 2009   B29K  93.2057  FALSE      DE 150\n4       ID4  2010   B60C 186.4110  FALSE      NL 300\n\n\nIn the special case of categorical variables, we can also use the dplyr package to replace strings using the case_when function. It is a flexible and powerful function that allows you to perform conditional operations on a vector or data frame, similar to a series of nested if-else statements. It’s especially useful when you need to create or modify variables in your data based on specific conditions. The syntax follows:\n\ncase_when(\n  condition1 ~ result1,\n  condition2 ~ result2,\n  condition3 ~ result3,\n  ...\n)\n\nIn the case of changing categories, if we want to switch the name of the country for the ISO identifyers of the countries:\n\nlibrary(tidyverse)\ndata &lt;- data.frame(Identifier = c(\"ID1\", \"ID2\", \"ID3\", \"ID4 \"), Date = c(2002, 2003, 2009, 2010), Domain = c(\"B60C\", \"B29K\",\"B29K \", \"B60C\"), Distance = c(62.1371, 124.274, 93.2057, 186.411), Active = c(TRUE, FALSE, FALSE, FALSE), Country = as.factor(c(\"France\", \"France\", \"Germany\", \"Netherlands\")))\n\ndata$Country &lt;- case_when(\n  data$Country == \"France\" ~ \"FR\",\n  data$Country == \"Netherlands\" ~ \"NL\",\n  data$Country == \"Germany\" ~ \"DE\"\n)\n\nprint(data)\n\n  Identifier Date Domain Distance Active Country\n1        ID1 2002   B60C  62.1371   TRUE      FR\n2        ID2 2003   B29K 124.2740  FALSE      FR\n3        ID3 2009  B29K   93.2057  FALSE      DE\n4       ID4  2010   B60C 186.4110  FALSE      NL\n\n\n\n\n\nExtract substrings\n\n\nMissing values\nIt is common to encounter missing values in your data. There are multiple methods to deal with missing data ranging for simply removing them to using statistical methods to replace the missing values with other values. Whatever you decide to do with your missing data, think thoroughly about the consequences. Always consider why the data is missing, are the missing observations random or is there a pattern to the missing observations? For example if, suppose we are working on a dataset with pollution measurements around the world. It’s possible that missing values come from countries in a specific region of the world. Removing these observations will result in excluding a whole region from the analysis. If the missing observations are more randomly dispersed over different regions in the world this might be less of an issue. The first step to take when dealing with missing values is to check for patterns in the data. Based on the type of missing values you are dealing with, you can decide to on the most efficient method to treat them:\n\nRemove the missing values\nLinear interpolation\nK-means\n\nThis book on missing values and This book with a chapter on how to handle this king of data"
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#data-conformity-in-python",
    "href": "Intro_programming_DataImportAndPrep.html#data-conformity-in-python",
    "title": "Importing in R",
    "section": "Data conformity in Python",
    "text": "Data conformity in Python\nBefore you start working on your analysis, you want to make sure the data is conform to your expectations. This means:\n\nThe number of observation is correct, and the variables you need are included.\nMaking sure each column is of the right type (years are dates, text are factors, numbers are integers or reals, …)\nThere are no trailing spaces\n“.” and “,” aren’t mixed up\nThe units of measurement are correct (km vs. miles, K€ vs 1.000, celsius vs fahrenheit etc.)\nAny missing values have been dealt with, or you have a valid reason to ignore them\nheaders are easy to use (no spaces or other characters that will make your life difficult).\n\nIn this section we will look into checking your data and adjusting it in preparation of your analysis.\n\nChecking data types\nThere are many different data formats. Most frequently we encounter numbers, strings, factors, dates and booleans. Consider the following dataset:\n\n\n\nID\nYear\nCool Domain\nFrequency\nAlive\n\n\n\n\nID1\n2002\nB60C\n42\nTRUE\n\n\nID2\n2003\nB29K\n21\nFALSE\n\n\nID3\n2009\nC08K\n12\nTRUE\n\n\n\nThe first column has textual data that is used as a key for each row. The Year should be considered either as an integer or a date. This will depend upon the type of analysis you want to perform. Specifically time series analysis will require this to be a date, while it’s enough for other analyses to ensure that it is an integer. The most common issue with this type of observation is that the year is considered as text instead of a number. If you see any ““” next to your numbers this is a bad sign which indicates that it is in fact, text.\nThe “Cool Domain” contains classifications which we want to be textual values. “Frequency” will need to be an integer while “Alive” we want to be a boolean. For each of the formats there is a function to test whether or not the format is the one we expect. These functions are usually in the form of is_numeric_dtype(), is_bool_dtype(), is_string_dtype(). The argument is then simply the column of the dataframe or an observation. Suppose we create the previous dataframe in Python:\n\nimport pandas as pd\ndata = {\n  'ID' : [\"ID1\", \"ID2\", \"ID3\"], \n  'Year' : [2002, 2003, 2009], \n  'Cool Domain' : [\"B60C\", \"B29K\",\"B29K\"], \n  'Frequency' : [42, 21, 12],\n  'Alive' : [True, False, True]\n}\n\ndataframe = pd.DataFrame(data)\n# check if the information in the column Year is numeric\npd.api.types.is_numeric_dtype(dataframe['Year'])\n\nTrue\n\n# check if the information in the column is logic\npd.api.types.is_bool_dtype(dataframe['Alive'])\n\nTrue\n\n# check if the information in the column is character\npd.api.types.is_string_dtype(dataframe['ID'])\n\nTrue\n\n\n\npd: This is an alias for the pandas library, which is a popular data manipulation library in Python. The alias is used for convenience.\napi: This is the namespace within the pandas library where various utility functions and types are organized. It’s considered part of the public API of the library, which means that it’s stable and can be used by developers without concerns about major changes between different versions of pandas.\ntypes: This is a subnamespace within the api namespace that provides functions and types related to data types and data type checking.\nis_numeric_dtype(): This is a function provided by the pandas library in the types subnamespace. It’s used to check if a given data type is numeric.\n\nThis can also be done in a more consise way with the dataframe.describe() function. This compute, for each of the columns, some statistics and provides the class of the class of the variables. This comes in handy as it allows us verify not only the class of the variables but also the distribution:\n\ndata = {\n  'ID' : [\"ID1\", \"ID2\", \"ID3\"], \n  'Year' : [2002, 2003, 2009], \n  'Cool Domain' : [\"B60C\", \"B29K\",\"B29K\"], \n  'Frequency' : [42, 21, 12],\n  'Alive' : [True, False, True]\n}\ndataframe = pd.DataFrame(data)\ndataframe.describe()\n\n              Year  Frequency\ncount     3.000000   3.000000\nmean   2004.666667  25.000000\nstd       3.785939  15.394804\nmin    2002.000000  12.000000\n25%    2002.500000  16.500000\n50%    2003.000000  21.000000\n75%    2006.000000  31.500000\nmax    2009.000000  42.000000\n\n\nMissing values in a dataframe will be automatically ignored. We can also check the types of each variable to ensure that it corresponds to what we want (integers are integers, text is text, etc). The latter can be done with the .dftypes function.\n\nimport pandas as pd\ndata = {\n  'ID' : [\"ID1\", \"ID2\", \"ID3\", \"ID4\"], \n  'Year' : [2002, 2003, 2009, 2010], \n  'Cool Domain' : [\"B60C\", \"B29K\",\"B29K\", \"B60C\"], \n  'Frequency' : [42, 21, 12, None],\n  'Weight' : [12.1, 34.2, 21.3, 93.2],\n  'Alive' : [True, False, False, False],\n  'Country' : [\"France\", \"France\", \"Germany\", \"Netherlands\"]\n}\ndf = pd.DataFrame(data)\ndf.describe()\ndf.dtypes\n\nIn this output we can see that the None value is ignored and the statistics are computed. The format of the variables is shown: ID is considered “object” signifying that has no specific nature (object is a generic format for anything used in Python). int64 represents an integer, float64 represents a reel number, bool is a boolean operator.\nGiven that we created the dataframe ourselves we can be relatively sure that the data was is the right format. However, often enough when we download data from online sources, we run into issues. If the format is not as expected there are different functions that allows us to transform the format of the data. Mainly we use .astype(float), .astype(int) and .astype(str).\n\ndata = {\n  'ID' : [\"ID1\", \"ID2\", \"ID3\", \"ID4\"], \n  'Year' : [2002, 2003, 2009, 2010], \n  'Cool Domain' : [\"B60C\", \"B29K\",\"B29K\", \"B60C\"], \n  'Frequency' : [42, 21, 12, 12],\n  'Weight' : [12.1, 34.2, 21.3, 93.2],\n  'Alive' : [True, False, False, False],\n  'Country' : [\"France\", \"France\", \"Germany\", \"Netherlands\"]\n}\ndf = pd.DataFrame(data)\n# Change the data type of 'ID' to string\ndf['ID'] = df['ID'].astype(str)\n\n# Change the data type of 'Weight' to uppercase string\ndf['Weight'] = df['Weight'].astype(float)\n\n# Change the data type of 'Frequency' to int\n# We first need to take care of the missing value\ndf['Frequency'] = df['Frequency'].astype(int)\ndf.dtypes\n\nID              object\nYear             int64\nCool Domain     object\nFrequency        int64\nWeight         float64\nAlive             bool\nCountry         object\ndtype: object\n\n\nNow that our data has the right format and we are sure we have all the observations we might want to do some work to normalise the column names. This is done to make programming and refercing the columns easier. Whenever the is a space in the name of the column, we need to refer to the column using `Column name with spaces` which is not ideal. It’s generally good practice to not have any spaces. We can use the pyjanitor package to automate this process:\n\nimport pandas as pd\nimport janitor\n\ndata = {'Column 1': [1, 2, None, 4],\n        'Column (2)': [None, 6, 7, 8],\n        'PM(µ &gt;3)': [9, 10, 11, 12]}\ndf = pd.DataFrame(data)\nprint(df)\n\n   Column 1  Column (2)  PM(µ &gt;3)\n0       1.0         NaN         9\n1       2.0         6.0        10\n2       NaN         7.0        11\n3       4.0         8.0        12\n\n# Using clean_pandas\ndf = df.clean_names()\nprint(df)\n\n   column_1  column_2_  pm_µ_&gt;3_\n0       1.0        NaN         9\n1       2.0        6.0        10\n2       NaN        7.0        11\n3       4.0        8.0        12\n\n\n\n\nData transformation"
  },
  {
    "objectID": "Network_analysis_basics.html",
    "href": "Network_analysis_basics.html",
    "title": "The basics of Network Analysis",
    "section": "",
    "text": "We are familiar with the statistical processing of object data (sum, mean, variance, histograms, etc.). However, there is a different kind of data with a high informational value: relational data. We are referring here to data that indicates a relationship, e.g a collaborative relationship, a citation link, a financial investment, a social link, etc. Relational data is rich in information. Relational data show their informational richness with different tools and methods. In fact, in relational data we’re mainly interested in the structure of these relationships and the role that entities play in this structure. This idea is illustrated in Figure 1.\n\nThe bar chart shows the frequency of patent technology classifications in a portfolio. The diagram provides information on the fields in which players seek to protect their inventions. Code B60C1/00 corresponds to “Tires characterized by chemical composition”, code C08K003/04 corresponds to “Use of inorganic substances as adjuvants” and so on. The network is generated by connecting two classifications present on the same patent. Each node in this network is therefore a classification, and each link indicates that both classifications were observed on the same patent.\nThe network representation of these classifications enables us to learn more about the combination of classifications. We can see which classifications are often combined, which are never combined, which play a central role, and so on. What interests us in networks is the structure. Analyzing structure allows us to quantify which nodes group together in communities, which nodes are central, which nodes connect different parts of the network (or clusters) and much more.\nHowever, to get the most out of this data, we need specific metrics and methods. These tools fall into a field known as Social Network Analysis (SNA). The name is misleading, coming from the pioneering methods that began work on social systems analysis. The methods have subsequently been employed in every field of science, but the name hasn’t really changed.\nWhatever the data source, the methods and indicators are the same. Interpretation, however, depends on the data: a co-classification link, a shareholding, a citation or a collaboration are not interpreted in the same way. And even between the same types of data, interpretation can differ. For example, a collaboration link in a project funded by the French National Research Agency (ANR) is not interpreted in the same way as a collaboration link in a European project. It is therefore essential to understand both the theory (indicators and methods) associated with SNA and the origin of the data. To this end, this manual has a dual objective. The first is to present the theoretical aspects of network analysis. The second is to present use cases mobilizing different data sources. These cases are divided into three parts:\n\nPresent the origin of the data, how to interpret the data\nPresent how the network was generated (what information was mobilized, what clean-ups were carried out, etc.).\nAnalysis of the network with an interpretation in line with the use case and the data mobilized.\n\nParticular attention is paid to the explanation of citation analysis in patents, and textual analysis, which present a higher level of complexity than other data sources.\nThis manual is aimed at Masters students and intelligence professionals who wish to include network analysis in their toolbox.\n\n\n\nIn its most basic form, a network is an object based on a set of nodes and links. The nature of the nodes and links is unlimited. A network can be created from any type of object with an interaction link. Network analysis can be applied to the analysis of social interactions: social interactions between cows, interactions between brain areas, equity investment links between firms, financial exposures between banks, collaborations between firms, links between criminals - the list goes on.\n\n\n\n\n\n\n\n\n\nFigure 2 shows the construction of a network in its simplest form. Objects are represented by circles we call “nodes” and interactions by connections we call “links”. The network here shows object 1 interacting with object 2. This could be two researchers co-authoring a paper, two firms collaborating on a project, etc.\n\n\n\n\n\n\nWarning\n\n\n\nUn réseau est la somme d’objets et d’interactions entre ces objets.\n\n\nThe concept is relatively simple, and we may be tempted to create networks from any kind of data. In order to avoid errors and misinterpretations, it is important to be able to identify the type of network we are creating and how this impacts on the validity or choice of indicators for analysis. In this section, we present the different types of networks and introduce a few vocabulary elements that will serve as a basis for the cases applied in the following.\n\n\n\n\nIn the simplest cases, an interaction between two objects can go either way. Suppose a collaborative link between two researchers: if \\(\\textit{A}\\) collaborates with \\(\\textit{B}\\), then \\(\\textit{B}\\) collaborates with \\(\\textit{A}\\). Another example is a network of criminals, a network based on participants in conferences or projects.\nIf the interaction between objects takes place in both directions, we speak of a \\(\\textbf{nondirected}\\) network. Classically, this type of interaction is represented by a line without arrows between objects. Figure \\(\\ref{nw_simple_nondirected}\\) gives an example of this type of network.\n\n\n\n\n\n\n\na. Unweighted Network\n\n\n\n\n\n\n\n\n\n\n\n\nb. Weighted Network\n\n\n\n\n\n\nIt is possible to quantify the link between objects, in which case we speak of a weighted network. If we take the example of a collaboration network, a collaboration link can be weighted by the number of collaborations between two actors. The number is then associated as a weight on the link and can be visualized by a different link thickness. In figure \\(\\ref{nw_simple_non_dirige_pondere}\\) we have the same structure, but different weights on the links. The weights are visualized by the link thickness and by the number displayed next to the links. It is often important to associate this type of information, both for the relevance of the calculation of certain indicators, and for visualization purposes. The latter makes it easier to see the structure of interactions between objects, and quickly identifies the densest areas of the network.\nIt’s possible to observe a link in a network that points from a node to itself. This is called a \\(\\textbf{loop}\\) and is represented in figure \\(\\ref{nw_simple_non_dirige_pondere}\\) by a red broken line. Let’s assume that each node in the illustrated network represents a country, and each link represents collaboration between players in the territory. The network shows us that two actors from country \\(\\textit{A}\\) collaborate with actors from country \\(\\textit{C}\\). In this context, the loop indicates that actors in country \\(\\textit{C}\\) are also working together.\n\n\n\nWhen the interaction goes from one node to another, the network is \\(\\textbf{directed}\\). A financial transaction is directed from one account to another. Quotations are another example: a document quoting a pre-existing document. So is the infection of one person by another. The virus spreads from the infected individual to the healthy one. The direction must be unilateral for the network to be considered directed. Figure \\(\\ref{nw_simple_dirige}\\) shows an example of a directed network. By convention, to indicate the direction of interaction, the link is represented by an arrow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this illustration, the node \\(\\textit{A}\\) interacts with three other nodes, with varying intensity. However, no node interacts with it. Node \\(\\textit{B}\\), on the other hand, both receives and gives interactions (with node \\(\\textit{C}\\)). Suppose we’re dealing with a financial network in which each node is a bank account held by an individual. In this case, node $textit{A}$ sends money to three other individuals, sending the largest amount (3 units) to node \\(\\textit{B}\\). The latter receives money from two individuals and sends money to individual \\(\\textit{C}\\).\nThe fact that a network is directed does not prevent two-way interaction. The difference with an undirected (or bilateral) network is that this interaction is split into two directed interactions. In figure \\(\\ref{nw_simple_directed_loop}\\) this is visualized between nodes \\(\\textit{D}\\) and \\(\\text{E}\\) by two red directed links. It is therefore possible that the weight of the link from \\(\\textit{D}\\) to \\(\\textit{E}\\) is different from the weight of \\(\\textit{E}\\) to \\(\\textit{D}\\). In an undirected network, the weight is unique.\nAs in an undirected network, a loop on a node is possible, with the difference that this loop is directed.\n\n\n\nIt is possible to create a network containing different types of objects. Suppose we want to analyze the career of inventors by mapping the companies they have worked for. This implies having two types of nodes: inventors and companies. When two types of nodes are interconnected in the same network, this network is called a \\(\\textit{bi-modal}\\) network.\n\n\n\n\n\n\n\n\n\nAn example is given in figure \\(\\ref{Reseau_bimodal}\\). We have three inventors and two companies. Two inventors are common to both companies, and one inventor is specific to one company. It’s important to note that in this type of network, there is no direct link between companies or inventors. Indeed, a link between inventors would represent a different type of link: the network would no longer be solely bi-modal, but would also be a multi-graph (network with more types of links, see \\(\\ref{muli_graph}\\)). In the network represented here, we have only one type of link: the company membership link.\nHowever, it is possible to transform a bi-modal network into a uni-modal one. The idea is to create a link between two inventors who have belonged to the same companies, and to create a link between two companies that have employed the same inventor. The nature of the link changes, but we’re trying to show the same thing. In figure \\(\\ref{Reseau_bimodal_ecalte}\\) the transformation is visualized.\n\n\n\n\n\n\n\n\n\nThe first network is a simple link between the two firms, with a weight of two because they have employed two inventors in common. The second network is composed solely of inventors. Here, the blue inventor has one firm in common with the orange inventor, and the orange inventor has two firms in common with the grey inventor.\nThis type of transformation facilitates analysis and, above all, the calculation of indicators that are more complex to calculate in an n-modal network.\n\n\n\nLet’s suppose we want to represent different types of interaction in the same network. This translates into the possible existence of two (or more) links between two objects in the same network. Figure \\(\\ref{Network_two_types_of_links}\\) visualizes this idea with two links between nodes \\(\\textit{C}\\) and \\(\\textit{D}\\). The first link is shown in black, the second in red. A network containing different types of links is called a \\(\\textbf{multi-graph}\\).\nAs with any network, it can be analyzed visually or by calculating indicators. In the particular case of a multigraph, the vast majority of indicators would be false (or incalculable) if two types of link co-existed. We must therefore be careful with multigraphs. While a visual analysis can be carried out without too many constraints, an analysis using indicators requires vigilance with regard to the indicators calculated (check in the software that the indicators take into account the different links). Indeed, if the software doesn’t distinguish between them, each link will be treated as identical, undermining the additional informational value of the multigraph.\n\n\n\n\n\n\n\n\n\nLet’s assume that the nodes in this network are companies, with black links representing collaborative links in patents, and red links representing collaborations in these scientific publications. We can read in this network that firm \\(\\textit{A}\\) has co-authored a paper with firms \\(\\textit{E}\\) and \\(\\textit{B}\\) but has co-filed a patent with firm \\(\\textit{C}\\). Although these are collaborative links in both cases, the implications are not at all the same. A collaborative link in patents implies an intellectual property parte, whereas publication shows above all a fundamental research activity between the two entities. A visual analysis here is interesting and relevant. However, the calculation of classic indicators would be wrong, as it considers both types of link to be identical. To overcome this problem, it is possible to create two networks from the first, each with only one type of link. The alternative relies on the use of more complex indicators, which are far more difficult to interpret and limit the impact these analyses can have on decision-making.\nMore effective than the multigraph are multiplex networks and interconnected networks, which we will describe in detail below.\n\n\n\nA different approach to multigraphs and bi-modal graphs is to consider each typology of links or nodes as a specific network, and to create links between networks. There are two types of multi-layer network, the multiplex network and the interconnected network.\nA different approach to the multigraph is to consider each link typology in a specific network and create links between networks. For example, in the network shown in figure \\(\\ref{nw_multiplex}\\) we have two networks represented simultaneously. Each network is also called layer here, referring to the idea that each layer handles a particular interaction [@kivela2014multilayer]. Thus, links connecting nodes in the same network are called intra-layer links. In this type of network, links between layers (inter-layer) exist only to notify the presence of the same node in different layers of the network [@sole2013spectral].\n\nWe can thus imagine co-patenting in a first layer and co-publishing in a second layer. Visualizing this type of network is possible for small networks, but quickly becomes unmanageable for larger ones. For the latter, a layered network remains the best solution. To calculate indicators, a multiplex network is possible, provided the software can handle them (R, Python, networkX).\n\nUnlike multi-layer networks, which assign a layer to each type of interaction, the interconnected network considers one object per layer. For example, the first network may consist of all interactions in a given region, the second network a different region. As a result, inter-layer links can be made between different nodes, unlike in a multiplex network. For example, in the network shown in figure \\(\\ref{nw_interconnected}\\), company A in region 1 collaborates with company B in region 2.\n\nBoth the multiplex and the interconnected network require specific algorithms for calculating indicators, and few software packages are able to manage this type of network.\n\n\n\n\nThe analysis of connections between nodes often raises the question of distance. A node that is close to all the nodes in the network is, a priori, an important node. Many centrality indicators are based on notions of distance between a node and other nodes in the network. By the same token, to judge the size of a network, we’d like to know the distance separating the most outlying nodes. In other cases, it may be important to know whether it’s simply possible to navigate from one node to another in a network.\nTo answer these questions, we first need to introduce the notion of “path”.\n\n\nIn network analysis, a is a sequence of nodes through which you must pass to reach a given node from a given node. Suppose we’re looking for a path between node and node in the network shown in figure \\(\\ref{Notion_de_chemin_bilat}\\). As the network is undirected, we can take the direction of the link as we wish. Thus, we start from node to node , then continue from node to node . The path connecting nodes G and A is therefore G, F, A.\nIf the network is directed, a path follows the direction of the links. So, in figure \\(\\ref{Notion_of_path_unilat}\\), the only path between and is A \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) C. Since the link between A and B is in the opposite direction, we cannot go from A to B directly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn both examples, the path between the two nodes is unique. However, several paths can coexist. The more nodes and links a network contains, the greater the number of paths. A key factor in differentiating these paths will then be their length, or the separating the nodes.\nThe distance between two nodes is given by the length of the path separating two nodes. The length is then given by the number of links separating the two nodes. For example, in figure two different paths are given to connect nodes G and C. The first path, in figure \\(\\ref{notion_distance_bi}\\) has a length of 4, giving a distance of 4 between the two nodes. The second path in figure \\(\\ref{notion_distance_uni}\\) has a distance of 6, giving a distance of 6 between G and C. Both distances and paths are valid, but we’ll often use the . This is calculated by identifying all possible paths between two nodes and retaining the shortest distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the case where no path exists between two nodes, by convention, we consider the distance between the two nodes to be \\(\\infty\\)..\nA special case of a path is the . A loop is a path leading from a node to itself. In figure \\(\\ref{Notion_loop}\\), a loop is shown in a directed network and in an undirected network. Loops can be problematic in some cases when we want to calculate distances or identify paths. For example, to calculate the distance between A and C, a correct path would be:\n\nA \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) A \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) C\n\nTheoretically, there are an infinite number of paths between A and C, as we could include the loop A - D - E - B - A an infinite number of times. In some cases, software will require an acyclic network, which implies a loop-free network, to enable algorithms to run.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we’ll start by presenting a number of indicators. Initially, we’ll focus on indicators at node level.\nIn order to illustrate the different indicators, we will mobilize a network extracted from real data. This is a collaborative network extracted from research projects funded by the French National Research Agency around 5G technologies. The network is given in figure \\(\\ref{nw_fil_rouge}\\).\\\n\nWhen analyzing a network, the first valuable piece of information is the number of connections for each node. This provides a measure of the number of collaborators (or number of citations, investors, co-authors, etc.). This indicator is called the .\n\n\n\n\n\n\nDefinition: degree\n\n\n\nThe degree of a node is the number of nodes it connects to. We can measure the degree by counting the number of links.\n\n\nIn the example of the network in figure \\(\\ref{nw_simple_degre}\\), node has two connections (with and ), so its degree is 2. We then note \\(d_E = 2\\). The nodes and have a degree of 2, and a degree of 3.\n\n\n\n\n\n\n\n\n\nIf we calculate the degrees of the network nodes in figure \\(\\ref{nw_fil_rouge}\\) we can visualize the degree by a color gradient. In figure \\(\\ref{nw_fil_rouge_degre}\\) we visualize the degree of the nodes by a green gradient. The darker the color, the higher the degree.\n\nThe degree visualization shows that some nodes, even if central, have a relatively low degree (ETIS, IRCICA). Orange has the highest degree, and therefore the largest number of employees, followed by the Institut Mines Telecom (which is more off-center).\nThe degree is a simple but effective measure for differentiating the positioning of nodes in a network.\nThe Degree in a directed network\nIf the network we’re analyzing is a directed one, we need to take into account the direction of interaction. A node can have both incoming and outgoing links. It therefore seems natural to propose two degrees in a directed network.\n\n\n\n\n\n\nDefinition: in-degree\n\n\n\nThe in-degree of a node corresponds to the number of links pointing to the node.\n\n\n\n\n\n\n\n\nDefinition: out-degree\n\n\n\nThe in-degree of a node corresponds to the number of links originating from the node, pointing to other nodes.\n\n\nFor example, in figure \\(\\ref{Reseau_degre_dirige}\\), the incoming degree of node is \\(2\\), since two links point to it. Since no links point to other nodes from \\(E\\), its outgoing degree is \\(0\\). The logic is therefore the same as for the classic degree, with the only difference being the directions.\n\n\n\n\n\n\n\n\n\nDegree in a weighted network\nThe weights of interactions can be included in the degree calculation, so we speak of . The latter is obtained by summing the weights of the node’s links. For example, in the figure \\(\\ref{Reseau_degre_pondere}\\), node E has two links, one with a weight of 1 and one with a weight of 2. The weighted degree of node E is therefore 3.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the aims of network analysis is to identify nodes with an important position in the network structure. Important can be defined in different ways: in the case of degree, it’s the number of links, but we can also look at things in a more structural way. If we take the network shown in figure , it’s clear that the position of Orange and CEA differ. The role of these two players is therefore different from a structural point of view. Orange has a more position than CEA. Intuitively, we can consider that the centrality of a node is linked to its importance in a network, as it reflects a higher level of interconnectivity than a node positioned more on the periphery. In other words, the smaller the distance between a node and all the other nodes in the network, the more central the node is in the network.\nThe idea of centrality is reflected in various indicators that measure the centrality of a node with a slightly different objective. We’ll introduce some of these indicators below.\nBetweenness Centrality\nA first vision of centrality is given in terms of flows in the network. Let’s suppose that the network represents interactions between individuals, and that a virus is circulating between them. A person on the periphery of the network is less likely to receive the virus, as it has to pass through a large number of nodes before reaching him or her. A central player is located on a large number of paths, exposing him to a greater risk of the virus reaching him before it reaches the entire network.\nIf we return to our collaboration network, we can consider that collaboration involves the exchange of ideas and information. These elements pass from node to node (possibly degrading or enriching themselves) in order to spread throughout the entire network. In the context of R&D, the pooling of the knowledge of different players can result in innovation. The logic then is that the position of a node is important if it is able to capture the information flows passing through the network.\nTo measure the importance of a node from a flow capture point of view, we’re going to identify the number of on which a node is positioned. More precisely, we’ll identify the number of shortest paths between each pair of nodes, and how many of them a given node is positioned on.\nFormally, the betweenness centrality is computed by:\\\n\n\n\n\n\n\nBetweennes Centrality formula:\n\n\n\n\\[\\begin{equation}\n    BC = \\sum_{k \\neq j, i \\in \\{k,j\\}} \\frac{\\frac{P_i(kj)}{P(kj)}}{\\frac{(n-1)\\cdot(n-2)}{2}}\\\\\n\\end{equation}\\]\n\n\nThe nominator calculates the number of paths between nodes \\(k\\) and \\(j\\) (\\(P(kj)\\)) and the number of paths between \\(k\\) and \\(j\\) on which node \\(i\\) is positioned (\\(P_i(kj)\\)). The ratio therefore gives the fraction of paths between \\(k\\) and \\(j\\) on which \\(i\\) is positioned.\nThe denominator is there to normalize the value for the size of the network. Betweenness Centrality therefore has a value between 0 and 1, the higher the value, the more central the node. This indicator was proposed by [@Freeman77].\nExample\nLet’s take an example of calculation using the network in figure \\(\\ref{reseau_BC_calcul}\\).\n\n\n\n\n\n\n\n\n\nLet’s calculate the BC of node \\(C\\). We need to start by identifying the shortest paths between all the nodes and then count how many of these paths node \\(C\\) is positioned on. In the table \\(\\ref{table_example_BC}\\), the first column lists each pair of nodes. There’s only one path between \\(A\\) and \\(B\\) (\\(A \\mapsto C \\mapsto B\\)). There is therefore a single shortest path (\\(P(AB) = 1\\)) and node \\(C\\) is positioned on this path (\\(P_C(AB)\\)). This calculation is repeated for every other pair of nodes as shown in the table below:\n\n\n\n\n\n\n\n\nLink\nNumber of shortest paths with C \\(P_i(kj)\\)\nNumber of shortest paths \\(P(kj)\\)\n\n\n\n\nA-B\n1\n1\n\n\nA-J\n1\n1\n\n\nB-J\n1\n1\n\n\nA-G\n1\n1\n\n\nB-G\n1\n1\n\n\nB-I\n1\n1\n\n\nA-I\n1\n1\n\n\nB-H\n1\n1\n\n\nA-H\n1\n1\n\n\n\nThe final centrality score of node \\(C\\) is given by:\n\\[\\begin{equation}\nBC_C = \\frac{1+1+1+1+1+1+1+1+1}{\\frac{(n-1)\\cdot(n-2)}{2}} = \\frac{9}{15} = 0.6\n\\end{equation}\\]\nif we apply this calculation to the collaboration network, and color the nodes according to the betweenness centrality score, we obtain the network in figure \\(\\ref{Reseau_fil_rouge_BC}\\).\\.\nThis centrality indicator highlights a number of players, notably Orange, CEA and ITM, with high centrality. The CEA, which is located on the periphery, positions itself on short paths by connecting to more central players, reducing the distance between itself and the other players in the network.\nThe BC thus makes it possible to identify players who act as links between different parts of the network. The CEA, for example, is the only actor linking the actors to its left with the rest of the network. As such, it controls the flow of information between these two parts of the network. This position is called . The BC makes it easier to identify actors with this particular (and valuable) position."
  },
  {
    "objectID": "Network_analysis_basics.html#introduction",
    "href": "Network_analysis_basics.html#introduction",
    "title": "The basics of Network Analysis",
    "section": "",
    "text": "We are familiar with the statistical processing of object data (sum, mean, variance, histograms, etc.). However, there is a different kind of data with a high informational value: relational data. We are referring here to data that indicates a relationship, e.g a collaborative relationship, a citation link, a financial investment, a social link, etc. Relational data is rich in information. Relational data show their informational richness with different tools and methods. In fact, in relational data we’re mainly interested in the structure of these relationships and the role that entities play in this structure. This idea is illustrated in Figure 1.\n\nThe bar chart shows the frequency of patent technology classifications in a portfolio. The diagram provides information on the fields in which players seek to protect their inventions. Code B60C1/00 corresponds to “Tires characterized by chemical composition”, code C08K003/04 corresponds to “Use of inorganic substances as adjuvants” and so on. The network is generated by connecting two classifications present on the same patent. Each node in this network is therefore a classification, and each link indicates that both classifications were observed on the same patent.\nThe network representation of these classifications enables us to learn more about the combination of classifications. We can see which classifications are often combined, which are never combined, which play a central role, and so on. What interests us in networks is the structure. Analyzing structure allows us to quantify which nodes group together in communities, which nodes are central, which nodes connect different parts of the network (or clusters) and much more.\nHowever, to get the most out of this data, we need specific metrics and methods. These tools fall into a field known as Social Network Analysis (SNA). The name is misleading, coming from the pioneering methods that began work on social systems analysis. The methods have subsequently been employed in every field of science, but the name hasn’t really changed.\nWhatever the data source, the methods and indicators are the same. Interpretation, however, depends on the data: a co-classification link, a shareholding, a citation or a collaboration are not interpreted in the same way. And even between the same types of data, interpretation can differ. For example, a collaboration link in a project funded by the French National Research Agency (ANR) is not interpreted in the same way as a collaboration link in a European project. It is therefore essential to understand both the theory (indicators and methods) associated with SNA and the origin of the data. To this end, this manual has a dual objective. The first is to present the theoretical aspects of network analysis. The second is to present use cases mobilizing different data sources. These cases are divided into three parts:\n\nPresent the origin of the data, how to interpret the data\nPresent how the network was generated (what information was mobilized, what clean-ups were carried out, etc.).\nAnalysis of the network with an interpretation in line with the use case and the data mobilized.\n\nParticular attention is paid to the explanation of citation analysis in patents, and textual analysis, which present a higher level of complexity than other data sources.\nThis manual is aimed at Masters students and intelligence professionals who wish to include network analysis in their toolbox."
  },
  {
    "objectID": "Network_analysis_basics.html#theory---network-analysis",
    "href": "Network_analysis_basics.html#theory---network-analysis",
    "title": "The basics of Network Analysis",
    "section": "",
    "text": "In its most basic form, a network is an object based on a set of nodes and links. The nature of the nodes and links is unlimited. A network can be created from any type of object with an interaction link. Network analysis can be applied to the analysis of social interactions: social interactions between cows, interactions between brain areas, equity investment links between firms, financial exposures between banks, collaborations between firms, links between criminals - the list goes on.\n\n\n\n\n\n\n\n\n\nFigure 2 shows the construction of a network in its simplest form. Objects are represented by circles we call “nodes” and interactions by connections we call “links”. The network here shows object 1 interacting with object 2. This could be two researchers co-authoring a paper, two firms collaborating on a project, etc.\n\n\n\n\n\n\nWarning\n\n\n\nUn réseau est la somme d’objets et d’interactions entre ces objets.\n\n\nThe concept is relatively simple, and we may be tempted to create networks from any kind of data. In order to avoid errors and misinterpretations, it is important to be able to identify the type of network we are creating and how this impacts on the validity or choice of indicators for analysis. In this section, we present the different types of networks and introduce a few vocabulary elements that will serve as a basis for the cases applied in the following.\n\n\n\n\nIn the simplest cases, an interaction between two objects can go either way. Suppose a collaborative link between two researchers: if \\(\\textit{A}\\) collaborates with \\(\\textit{B}\\), then \\(\\textit{B}\\) collaborates with \\(\\textit{A}\\). Another example is a network of criminals, a network based on participants in conferences or projects.\nIf the interaction between objects takes place in both directions, we speak of a \\(\\textbf{nondirected}\\) network. Classically, this type of interaction is represented by a line without arrows between objects. Figure \\(\\ref{nw_simple_nondirected}\\) gives an example of this type of network.\n\n\n\n\n\n\n\na. Unweighted Network\n\n\n\n\n\n\n\n\n\n\n\n\nb. Weighted Network\n\n\n\n\n\n\nIt is possible to quantify the link between objects, in which case we speak of a weighted network. If we take the example of a collaboration network, a collaboration link can be weighted by the number of collaborations between two actors. The number is then associated as a weight on the link and can be visualized by a different link thickness. In figure \\(\\ref{nw_simple_non_dirige_pondere}\\) we have the same structure, but different weights on the links. The weights are visualized by the link thickness and by the number displayed next to the links. It is often important to associate this type of information, both for the relevance of the calculation of certain indicators, and for visualization purposes. The latter makes it easier to see the structure of interactions between objects, and quickly identifies the densest areas of the network.\nIt’s possible to observe a link in a network that points from a node to itself. This is called a \\(\\textbf{loop}\\) and is represented in figure \\(\\ref{nw_simple_non_dirige_pondere}\\) by a red broken line. Let’s assume that each node in the illustrated network represents a country, and each link represents collaboration between players in the territory. The network shows us that two actors from country \\(\\textit{A}\\) collaborate with actors from country \\(\\textit{C}\\). In this context, the loop indicates that actors in country \\(\\textit{C}\\) are also working together.\n\n\n\nWhen the interaction goes from one node to another, the network is \\(\\textbf{directed}\\). A financial transaction is directed from one account to another. Quotations are another example: a document quoting a pre-existing document. So is the infection of one person by another. The virus spreads from the infected individual to the healthy one. The direction must be unilateral for the network to be considered directed. Figure \\(\\ref{nw_simple_dirige}\\) shows an example of a directed network. By convention, to indicate the direction of interaction, the link is represented by an arrow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this illustration, the node \\(\\textit{A}\\) interacts with three other nodes, with varying intensity. However, no node interacts with it. Node \\(\\textit{B}\\), on the other hand, both receives and gives interactions (with node \\(\\textit{C}\\)). Suppose we’re dealing with a financial network in which each node is a bank account held by an individual. In this case, node $textit{A}$ sends money to three other individuals, sending the largest amount (3 units) to node \\(\\textit{B}\\). The latter receives money from two individuals and sends money to individual \\(\\textit{C}\\).\nThe fact that a network is directed does not prevent two-way interaction. The difference with an undirected (or bilateral) network is that this interaction is split into two directed interactions. In figure \\(\\ref{nw_simple_directed_loop}\\) this is visualized between nodes \\(\\textit{D}\\) and \\(\\text{E}\\) by two red directed links. It is therefore possible that the weight of the link from \\(\\textit{D}\\) to \\(\\textit{E}\\) is different from the weight of \\(\\textit{E}\\) to \\(\\textit{D}\\). In an undirected network, the weight is unique.\nAs in an undirected network, a loop on a node is possible, with the difference that this loop is directed.\n\n\n\nIt is possible to create a network containing different types of objects. Suppose we want to analyze the career of inventors by mapping the companies they have worked for. This implies having two types of nodes: inventors and companies. When two types of nodes are interconnected in the same network, this network is called a \\(\\textit{bi-modal}\\) network.\n\n\n\n\n\n\n\n\n\nAn example is given in figure \\(\\ref{Reseau_bimodal}\\). We have three inventors and two companies. Two inventors are common to both companies, and one inventor is specific to one company. It’s important to note that in this type of network, there is no direct link between companies or inventors. Indeed, a link between inventors would represent a different type of link: the network would no longer be solely bi-modal, but would also be a multi-graph (network with more types of links, see \\(\\ref{muli_graph}\\)). In the network represented here, we have only one type of link: the company membership link.\nHowever, it is possible to transform a bi-modal network into a uni-modal one. The idea is to create a link between two inventors who have belonged to the same companies, and to create a link between two companies that have employed the same inventor. The nature of the link changes, but we’re trying to show the same thing. In figure \\(\\ref{Reseau_bimodal_ecalte}\\) the transformation is visualized.\n\n\n\n\n\n\n\n\n\nThe first network is a simple link between the two firms, with a weight of two because they have employed two inventors in common. The second network is composed solely of inventors. Here, the blue inventor has one firm in common with the orange inventor, and the orange inventor has two firms in common with the grey inventor.\nThis type of transformation facilitates analysis and, above all, the calculation of indicators that are more complex to calculate in an n-modal network.\n\n\n\nLet’s suppose we want to represent different types of interaction in the same network. This translates into the possible existence of two (or more) links between two objects in the same network. Figure \\(\\ref{Network_two_types_of_links}\\) visualizes this idea with two links between nodes \\(\\textit{C}\\) and \\(\\textit{D}\\). The first link is shown in black, the second in red. A network containing different types of links is called a \\(\\textbf{multi-graph}\\).\nAs with any network, it can be analyzed visually or by calculating indicators. In the particular case of a multigraph, the vast majority of indicators would be false (or incalculable) if two types of link co-existed. We must therefore be careful with multigraphs. While a visual analysis can be carried out without too many constraints, an analysis using indicators requires vigilance with regard to the indicators calculated (check in the software that the indicators take into account the different links). Indeed, if the software doesn’t distinguish between them, each link will be treated as identical, undermining the additional informational value of the multigraph.\n\n\n\n\n\n\n\n\n\nLet’s assume that the nodes in this network are companies, with black links representing collaborative links in patents, and red links representing collaborations in these scientific publications. We can read in this network that firm \\(\\textit{A}\\) has co-authored a paper with firms \\(\\textit{E}\\) and \\(\\textit{B}\\) but has co-filed a patent with firm \\(\\textit{C}\\). Although these are collaborative links in both cases, the implications are not at all the same. A collaborative link in patents implies an intellectual property parte, whereas publication shows above all a fundamental research activity between the two entities. A visual analysis here is interesting and relevant. However, the calculation of classic indicators would be wrong, as it considers both types of link to be identical. To overcome this problem, it is possible to create two networks from the first, each with only one type of link. The alternative relies on the use of more complex indicators, which are far more difficult to interpret and limit the impact these analyses can have on decision-making.\nMore effective than the multigraph are multiplex networks and interconnected networks, which we will describe in detail below.\n\n\n\nA different approach to multigraphs and bi-modal graphs is to consider each typology of links or nodes as a specific network, and to create links between networks. There are two types of multi-layer network, the multiplex network and the interconnected network.\nA different approach to the multigraph is to consider each link typology in a specific network and create links between networks. For example, in the network shown in figure \\(\\ref{nw_multiplex}\\) we have two networks represented simultaneously. Each network is also called layer here, referring to the idea that each layer handles a particular interaction [@kivela2014multilayer]. Thus, links connecting nodes in the same network are called intra-layer links. In this type of network, links between layers (inter-layer) exist only to notify the presence of the same node in different layers of the network [@sole2013spectral].\n\nWe can thus imagine co-patenting in a first layer and co-publishing in a second layer. Visualizing this type of network is possible for small networks, but quickly becomes unmanageable for larger ones. For the latter, a layered network remains the best solution. To calculate indicators, a multiplex network is possible, provided the software can handle them (R, Python, networkX).\n\nUnlike multi-layer networks, which assign a layer to each type of interaction, the interconnected network considers one object per layer. For example, the first network may consist of all interactions in a given region, the second network a different region. As a result, inter-layer links can be made between different nodes, unlike in a multiplex network. For example, in the network shown in figure \\(\\ref{nw_interconnected}\\), company A in region 1 collaborates with company B in region 2.\n\nBoth the multiplex and the interconnected network require specific algorithms for calculating indicators, and few software packages are able to manage this type of network.\n\n\n\n\nThe analysis of connections between nodes often raises the question of distance. A node that is close to all the nodes in the network is, a priori, an important node. Many centrality indicators are based on notions of distance between a node and other nodes in the network. By the same token, to judge the size of a network, we’d like to know the distance separating the most outlying nodes. In other cases, it may be important to know whether it’s simply possible to navigate from one node to another in a network.\nTo answer these questions, we first need to introduce the notion of “path”.\n\n\nIn network analysis, a is a sequence of nodes through which you must pass to reach a given node from a given node. Suppose we’re looking for a path between node and node in the network shown in figure \\(\\ref{Notion_de_chemin_bilat}\\). As the network is undirected, we can take the direction of the link as we wish. Thus, we start from node to node , then continue from node to node . The path connecting nodes G and A is therefore G, F, A.\nIf the network is directed, a path follows the direction of the links. So, in figure \\(\\ref{Notion_of_path_unilat}\\), the only path between and is A \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) C. Since the link between A and B is in the opposite direction, we cannot go from A to B directly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn both examples, the path between the two nodes is unique. However, several paths can coexist. The more nodes and links a network contains, the greater the number of paths. A key factor in differentiating these paths will then be their length, or the separating the nodes.\nThe distance between two nodes is given by the length of the path separating two nodes. The length is then given by the number of links separating the two nodes. For example, in figure two different paths are given to connect nodes G and C. The first path, in figure \\(\\ref{notion_distance_bi}\\) has a length of 4, giving a distance of 4 between the two nodes. The second path in figure \\(\\ref{notion_distance_uni}\\) has a distance of 6, giving a distance of 6 between G and C. Both distances and paths are valid, but we’ll often use the . This is calculated by identifying all possible paths between two nodes and retaining the shortest distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the case where no path exists between two nodes, by convention, we consider the distance between the two nodes to be \\(\\infty\\)..\nA special case of a path is the . A loop is a path leading from a node to itself. In figure \\(\\ref{Notion_loop}\\), a loop is shown in a directed network and in an undirected network. Loops can be problematic in some cases when we want to calculate distances or identify paths. For example, to calculate the distance between A and C, a correct path would be:\n\nA \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) A \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) C\n\nTheoretically, there are an infinite number of paths between A and C, as we could include the loop A - D - E - B - A an infinite number of times. In some cases, software will require an acyclic network, which implies a loop-free network, to enable algorithms to run.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we’ll start by presenting a number of indicators. Initially, we’ll focus on indicators at node level.\nIn order to illustrate the different indicators, we will mobilize a network extracted from real data. This is a collaborative network extracted from research projects funded by the French National Research Agency around 5G technologies. The network is given in figure \\(\\ref{nw_fil_rouge}\\).\\\n\nWhen analyzing a network, the first valuable piece of information is the number of connections for each node. This provides a measure of the number of collaborators (or number of citations, investors, co-authors, etc.). This indicator is called the .\n\n\n\n\n\n\nDefinition: degree\n\n\n\nThe degree of a node is the number of nodes it connects to. We can measure the degree by counting the number of links.\n\n\nIn the example of the network in figure \\(\\ref{nw_simple_degre}\\), node has two connections (with and ), so its degree is 2. We then note \\(d_E = 2\\). The nodes and have a degree of 2, and a degree of 3.\n\n\n\n\n\n\n\n\n\nIf we calculate the degrees of the network nodes in figure \\(\\ref{nw_fil_rouge}\\) we can visualize the degree by a color gradient. In figure \\(\\ref{nw_fil_rouge_degre}\\) we visualize the degree of the nodes by a green gradient. The darker the color, the higher the degree.\n\nThe degree visualization shows that some nodes, even if central, have a relatively low degree (ETIS, IRCICA). Orange has the highest degree, and therefore the largest number of employees, followed by the Institut Mines Telecom (which is more off-center).\nThe degree is a simple but effective measure for differentiating the positioning of nodes in a network.\nThe Degree in a directed network\nIf the network we’re analyzing is a directed one, we need to take into account the direction of interaction. A node can have both incoming and outgoing links. It therefore seems natural to propose two degrees in a directed network.\n\n\n\n\n\n\nDefinition: in-degree\n\n\n\nThe in-degree of a node corresponds to the number of links pointing to the node.\n\n\n\n\n\n\n\n\nDefinition: out-degree\n\n\n\nThe in-degree of a node corresponds to the number of links originating from the node, pointing to other nodes.\n\n\nFor example, in figure \\(\\ref{Reseau_degre_dirige}\\), the incoming degree of node is \\(2\\), since two links point to it. Since no links point to other nodes from \\(E\\), its outgoing degree is \\(0\\). The logic is therefore the same as for the classic degree, with the only difference being the directions.\n\n\n\n\n\n\n\n\n\nDegree in a weighted network\nThe weights of interactions can be included in the degree calculation, so we speak of . The latter is obtained by summing the weights of the node’s links. For example, in the figure \\(\\ref{Reseau_degre_pondere}\\), node E has two links, one with a weight of 1 and one with a weight of 2. The weighted degree of node E is therefore 3.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the aims of network analysis is to identify nodes with an important position in the network structure. Important can be defined in different ways: in the case of degree, it’s the number of links, but we can also look at things in a more structural way. If we take the network shown in figure , it’s clear that the position of Orange and CEA differ. The role of these two players is therefore different from a structural point of view. Orange has a more position than CEA. Intuitively, we can consider that the centrality of a node is linked to its importance in a network, as it reflects a higher level of interconnectivity than a node positioned more on the periphery. In other words, the smaller the distance between a node and all the other nodes in the network, the more central the node is in the network.\nThe idea of centrality is reflected in various indicators that measure the centrality of a node with a slightly different objective. We’ll introduce some of these indicators below.\nBetweenness Centrality\nA first vision of centrality is given in terms of flows in the network. Let’s suppose that the network represents interactions between individuals, and that a virus is circulating between them. A person on the periphery of the network is less likely to receive the virus, as it has to pass through a large number of nodes before reaching him or her. A central player is located on a large number of paths, exposing him to a greater risk of the virus reaching him before it reaches the entire network.\nIf we return to our collaboration network, we can consider that collaboration involves the exchange of ideas and information. These elements pass from node to node (possibly degrading or enriching themselves) in order to spread throughout the entire network. In the context of R&D, the pooling of the knowledge of different players can result in innovation. The logic then is that the position of a node is important if it is able to capture the information flows passing through the network.\nTo measure the importance of a node from a flow capture point of view, we’re going to identify the number of on which a node is positioned. More precisely, we’ll identify the number of shortest paths between each pair of nodes, and how many of them a given node is positioned on.\nFormally, the betweenness centrality is computed by:\\\n\n\n\n\n\n\nBetweennes Centrality formula:\n\n\n\n\\[\\begin{equation}\n    BC = \\sum_{k \\neq j, i \\in \\{k,j\\}} \\frac{\\frac{P_i(kj)}{P(kj)}}{\\frac{(n-1)\\cdot(n-2)}{2}}\\\\\n\\end{equation}\\]\n\n\nThe nominator calculates the number of paths between nodes \\(k\\) and \\(j\\) (\\(P(kj)\\)) and the number of paths between \\(k\\) and \\(j\\) on which node \\(i\\) is positioned (\\(P_i(kj)\\)). The ratio therefore gives the fraction of paths between \\(k\\) and \\(j\\) on which \\(i\\) is positioned.\nThe denominator is there to normalize the value for the size of the network. Betweenness Centrality therefore has a value between 0 and 1, the higher the value, the more central the node. This indicator was proposed by [@Freeman77].\nExample\nLet’s take an example of calculation using the network in figure \\(\\ref{reseau_BC_calcul}\\).\n\n\n\n\n\n\n\n\n\nLet’s calculate the BC of node \\(C\\). We need to start by identifying the shortest paths between all the nodes and then count how many of these paths node \\(C\\) is positioned on. In the table \\(\\ref{table_example_BC}\\), the first column lists each pair of nodes. There’s only one path between \\(A\\) and \\(B\\) (\\(A \\mapsto C \\mapsto B\\)). There is therefore a single shortest path (\\(P(AB) = 1\\)) and node \\(C\\) is positioned on this path (\\(P_C(AB)\\)). This calculation is repeated for every other pair of nodes as shown in the table below:\n\n\n\n\n\n\n\n\nLink\nNumber of shortest paths with C \\(P_i(kj)\\)\nNumber of shortest paths \\(P(kj)\\)\n\n\n\n\nA-B\n1\n1\n\n\nA-J\n1\n1\n\n\nB-J\n1\n1\n\n\nA-G\n1\n1\n\n\nB-G\n1\n1\n\n\nB-I\n1\n1\n\n\nA-I\n1\n1\n\n\nB-H\n1\n1\n\n\nA-H\n1\n1\n\n\n\nThe final centrality score of node \\(C\\) is given by:\n\\[\\begin{equation}\nBC_C = \\frac{1+1+1+1+1+1+1+1+1}{\\frac{(n-1)\\cdot(n-2)}{2}} = \\frac{9}{15} = 0.6\n\\end{equation}\\]\nif we apply this calculation to the collaboration network, and color the nodes according to the betweenness centrality score, we obtain the network in figure \\(\\ref{Reseau_fil_rouge_BC}\\).\\.\nThis centrality indicator highlights a number of players, notably Orange, CEA and ITM, with high centrality. The CEA, which is located on the periphery, positions itself on short paths by connecting to more central players, reducing the distance between itself and the other players in the network.\nThe BC thus makes it possible to identify players who act as links between different parts of the network. The CEA, for example, is the only actor linking the actors to its left with the rest of the network. As such, it controls the flow of information between these two parts of the network. This position is called . The BC makes it easier to identify actors with this particular (and valuable) position."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Programming",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Topic_modelling.html",
    "href": "Topic_modelling.html",
    "title": "Topic Modelling",
    "section": "",
    "text": "We will use two data types for this tutorial:\n\nLexisNexis from which we have extracted articles about climate anxiety\nLens.org from which we have extracted patents about water desalination\n\nThe aims are:\n\nLearn how to perform topic modelling using R\nLearn how to prepare the data for topic modelling\nLearn how to adjust text preparation to optimize your results for a given question\nDecide on the optimal number of topics in your data\nConnect the topics to other variables in your dataset (players, time, inventors,…)\n\nThe datasets can be found on blackboard. If, howerver, you prefer working on your own data/topic we will first start with a quick brief on how to export data from LexisNexis and import this data into R.\n\n\n\n\nEven though exporting data from LexisUni is quite straightforward, if we want to export in a format that can be directly read by R and used for textmining, we need to tick the right boxes. We will go through those steps now:\nStart with building a query in the advances query section. Make sure you check the AND and OR operators. \nBelow the text elements you can adapt the data range if required.\n\nThen click on search to launch the search process. Once the results are shown, make sure to click the switch to remove Group Duplicates (this should be on “On”). At this point adjust any other filter. Be particulary cautious with the language. Many language models are trained for one language so remove any document not in that language if you use a mono-lingual model.\n Once you’re happy with the dataset, go to the “download button”:  A window will appear, this is were you need to be sure to click the same boxes as in the following screenshots. Ensure that the format is Word (docx) and that the documents are grouped into one.\n\nThe second screen should look like this: (these should be the basic options):\n\nYou can then click on “download”.\nDo this as many times are required to download all the articles (100 per 100). Creating an account with your uu email will make this process slightly more streamlined and faster.\nPut all the files into one folder. We will then use the LexisNexisTools package to import and format this data to make it directly usable.\nFirst we need to search for all the documents, using the list.files() function we create a list of all the .docx documents in the “LN” folder. Note that the LN folder is created by me to store the files, replace LN with the name of the folder in which you store the files. If the files are stored directly at the root of the current WD, use ““.\n\nmy_files &lt;- list.files(pattern = \".docx\", path = \"LN\", full.names = TRUE, recursive = TRUE, ignore.case = TRUE)\n\nAt this stage the data is not yet imported, we merely created a list with paths to the files. We will now load those files into R with the lnt_read() function. This will load the data into a specific object with a specific format that we cannot use directly. We therefore transform this object with the lnt_convent() function. We specify “to =”data.frame” so that the result is a dataframe that we can use.\n\nlibrary(LexisNexisTools)\ndat &lt;- lnt_read(my_files) #Object of class 'LNT output'\nLN_dataframe = lnt_convert(dat, to = \"data.frame\")\n\nWe now have the data loaded in R and ready for use. We will perform two extra actions before we continue to ensure that we don’t run into trouble later on. We start with the removal of identical articles (even though the switch is on, there are still some left…) and we remove the articles that are too short for the analysis. We use the nchar() function to count the number of characters in each article and remove those with less than 200 characters.\n\nLN_dataframe = unique(LN_dataframe)\nLN_dataframe$length = nchar(LN_dataframe$Article)\nLN_dataframe = subset(LN_dataframe, LN_dataframe$length &gt;= 200)\n\nThe dataset is now ready for use.\n\n# loading our custom functions\nlibrary(tm)\nlibrary(udpipe)\nlibrary(tidyverse)\nlibrary(textstem)\nclean_text_lemma &lt;- function(text){\n  #text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with \"\".\n  text &lt;- gsub(\"\\\\.\", \"\", text)\n  text &lt;- gsub(\"\\\\;\", \"\", text)\n  text &lt;- gsub(\"\\\\-\", \"\", text)\n  text &lt;- gsub(\"\\\\+\", \"plus\", text)\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  # here we apply lemmatization instead of stemming:\n  lemma_dico &lt;-  make_lemma_dictionary(text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\n  text &lt;- lemmatize_strings(text, dictionary = lemma_dico)\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n  text &lt;- gsub(\"  \", \" \", text)\n}\nc_value = function(words){\n# we need to compute the number of higher order terms and the frequency of these terms\n# we initiate two empty columns in which we can store this information\nwords$terms_containing_term &lt;- 0\nwords$Sum_freq_higher_terms &lt;- 0\n# We make sure the data has a dataframe format\nwords = as.data.frame(words)\n# we now loop over all the words to check how often they are nested\nfor(i in 1:dim(words)[1]){\n  # first we check in which term the term is nested\n  # if the term is part of another string it will return TRUE, FALSE otherwise\n  # The str_detect() function searches for a pattern in a second string and returns\n  # True if the pattern is part of the string\n  words$tmp = stringr::str_detect(words$keyword, words[i,1])\n  \n  # We then only keep the words that contain the word we are searching for\n  tmp = subset(words, words$keyword != words[i,1] & words$tmp == TRUE)\n  # The number of strings in which the pattern is nested is then simply the \n  # dimension of the dataframe we just found\n  words[i,4] &lt;- dim(tmp)[1]\n  # the sum of the frequencies is simply the sum of the individual frequencies\n  words[i,5] &lt;- sum(tmp$freq)\n}\n# now compute the c-value\n# we first adda column that will contain this value\nwords$C_value = 0\n# then we check if there are nested terms or not and apply the formula accordingly\nwords$C_value = case_when(\n    words$terms_containing_term == 0 ~ log2(words$ngram) * words$freq,\n    #keyword | n_gram | freq dataset | terms count terms | sum_freq_high\n    words$terms_containing_term != 0 ~ (log2(words$ngram) * (words$freq - (1/words$terms_containing_term) * words$Sum_freq_higher_terms))\n)\n# to make this work with the other functions we remove the \"tmp\" column...\nwords = words[,-6]\n#... and we reorganise the columns so that we do not have to adjust the other functions\nwords = words[,c(1,3,2,4,5,6)]\nreturn(words)\n}\nterm_extraction = function(Text_data, max_gram, min_freq){\n  # we need three elements of importance for the function to run\n  # the data, the max ngram and the minimum frequency\n  Text_df = as.data.frame(Text_data[1,])\n  x &lt;- udpipe_annotate(ud_model, x = as.character(Text_df[1,2]), doc_id = Text_df[1,1])\n  x &lt;- as.data.frame(x)\n  \n  stats &lt;- keywords_rake(x = x, term = \"lemma\", group = \"doc_id\", \n                         relevant = x$upos %in% c(\"NOUN\", \"ADJ\"))\n  stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword))\n  \n  \n  x$phrase_tag &lt;- as_phrasemachine(x$upos, type = \"upos\")\n  stats &lt;- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \n                            pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                            is_regex = TRUE, detailed = FALSE)\n  \n  stats = subset(stats, stats$ngram &gt;= 1 & stats$ngram &lt;= max_gram)\n  stats = subset(stats, stats$freq &gt;= min_freq)\n  return(stats)\n}\n# specify the dictionary\nmy_dico = c(\"a method\", \"patent\")\n# load the language model\nlibrary(udpipe)\nud_model &lt;- udpipe::udpipe_download_model(language = \"english\")\n#save(ud_model, file = \"ud_model.rdata\")\nud_model &lt;- udpipe::udpipe_load_model(ud_model)\n\n\n\n\n\nWe start by prepping the text as we did in the first tutorial. We use a loop since it is likely that errors occur in this process that will push us to adjust parameters. The use of a loop makes it easier to pause and continue where we left of. Once the code is stabilised you can use the map() function from tidyverse to make this more efficient and run faster. Or even use multi-threading with the foreach package.\nColumn 11 contains the text, we start by lemmatizing the text.\n\nlibrary(textstem)\nLN_dataframe = LN_dataframe[-c(382,431, 464, 1087),]\nfor(i in 1:dim(LN_dataframe)[1]){\n  LN_dataframe[i,11] &lt;-clean_text_lemma(LN_dataframe[i,11])\n  # now er extract the terms\n  tmp &lt;- term_extraction(LN_dataframe[i,c(1,11)], max_gram = 4, min_freq =  1)\n  # and compute the c_value\n  tmp &lt;- c_value(tmp)\n  # we remove words with low values\n  tmp &lt;- subset(tmp, tmp$C_value &gt; 0)\n  # we combine the tokens\n  tmp$keyword &lt;- apply(as.matrix(tmp$keyword), 1, gsub, pattern = \" \", replacement = \"_\")\n  # and we put the text back into the document\n  LN_dataframe[i,11] &lt;- paste(tmp$keyword, collapse = \" \")\n}\n# we save the result in a .rdata file in case we make a mistake later\nsave(LN_dataframe, file = \"LN_dataframe_POS_lemmcleaning.rdata\")\n\nNow that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the DocumentTermMatrix() function from the tm package. This function has one argument which is a dataframe with the text of the corpus.\n\nlibrary(tm)\n# create the document-Term-matrix\ndtm &lt;- DocumentTermMatrix(LN_dataframe$Article)\n\nBased on this matrix we will now try to define how many topics we need to extract. For this we use the FindTopicsNumber and FindTopicsNumber_plot from the ldatuning package.\nThe FindTopicsNumber functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The seq() function creates a vector which starts at the from arguments, stops at the to argument. The size of the steps is set by the by argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, …) we would write seq(from = 2, to = 60, by = 5).\nWe then specify the metrics we want to compute, we have discussed these in the lecture.\n\n\n\n\n\n\nWarning\n\n\n\nFor an unknown reason, the “Griffiths2004” function does not work on mac OSX. It should work for windows users.\n\n\nThe mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algo is stuck, still running or finished.\n\nlibrary(ldatuning)\ntopic_num &lt;- FindTopicsNumber(\n  dtm,\n  topics = seq(from = 2, to = 60, by = 5),\n  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  mc.cores = 8L,\n  verbose = TRUE\n)\nFindTopicsNumber_plot(topic_num)\n\nNow that we have an idea of how many topics we need, so let’s extract the topics. We will use the LDA function from the topicmodels package.\n\nlibrary(topicmodels)\nlibrary(tidytext)\n# perform topic modelling\ntopics_model &lt;- LDA(dtm, k = 7)\n# get the betas\nbetas &lt;- tidytext::tidy(topics_model, matrix = \"beta\")\n# subset the betas for results\nbetas2&lt;- subset(betas, betas$topic %in% c(2,3,4,5,7))\n\nWe now have the topics and the betas for each topic-term couple. We only keep the highest values for each topic:\n\nap_top_terms &lt;- betas2 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nAnd now we visualise some of the results:\n\nlibrary(ggplot2)\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\nAdjust the parameters of the previous text cleaning functions. Adjust the dictionnary, maybe switch the noun phrases option ((A|N)*N(P+D*(A|N)*N)*) to an equivalent that includes actions.\n\n\n\nNow try the same logic on patent data. Check the results step by step. Adjust the dictionary etc. to suit patent data. What do you look for in the case of patent data, how is this different from news data?"
  },
  {
    "objectID": "Topic_modelling.html#exporting-and-importing-lexisuni-data",
    "href": "Topic_modelling.html#exporting-and-importing-lexisuni-data",
    "title": "Topic Modelling",
    "section": "",
    "text": "Even though exporting data from LexisUni is quite straightforward, if we want to export in a format that can be directly read by R and used for textmining, we need to tick the right boxes. We will go through those steps now:\nStart with building a query in the advances query section. Make sure you check the AND and OR operators. \nBelow the text elements you can adapt the data range if required.\n\nThen click on search to launch the search process. Once the results are shown, make sure to click the switch to remove Group Duplicates (this should be on “On”). At this point adjust any other filter. Be particulary cautious with the language. Many language models are trained for one language so remove any document not in that language if you use a mono-lingual model.\n Once you’re happy with the dataset, go to the “download button”:  A window will appear, this is were you need to be sure to click the same boxes as in the following screenshots. Ensure that the format is Word (docx) and that the documents are grouped into one.\n\nThe second screen should look like this: (these should be the basic options):\n\nYou can then click on “download”.\nDo this as many times are required to download all the articles (100 per 100). Creating an account with your uu email will make this process slightly more streamlined and faster.\nPut all the files into one folder. We will then use the LexisNexisTools package to import and format this data to make it directly usable.\nFirst we need to search for all the documents, using the list.files() function we create a list of all the .docx documents in the “LN” folder. Note that the LN folder is created by me to store the files, replace LN with the name of the folder in which you store the files. If the files are stored directly at the root of the current WD, use ““.\n\nmy_files &lt;- list.files(pattern = \".docx\", path = \"LN\", full.names = TRUE, recursive = TRUE, ignore.case = TRUE)\n\nAt this stage the data is not yet imported, we merely created a list with paths to the files. We will now load those files into R with the lnt_read() function. This will load the data into a specific object with a specific format that we cannot use directly. We therefore transform this object with the lnt_convent() function. We specify “to =”data.frame” so that the result is a dataframe that we can use.\n\nlibrary(LexisNexisTools)\ndat &lt;- lnt_read(my_files) #Object of class 'LNT output'\nLN_dataframe = lnt_convert(dat, to = \"data.frame\")\n\nWe now have the data loaded in R and ready for use. We will perform two extra actions before we continue to ensure that we don’t run into trouble later on. We start with the removal of identical articles (even though the switch is on, there are still some left…) and we remove the articles that are too short for the analysis. We use the nchar() function to count the number of characters in each article and remove those with less than 200 characters.\n\nLN_dataframe = unique(LN_dataframe)\nLN_dataframe$length = nchar(LN_dataframe$Article)\nLN_dataframe = subset(LN_dataframe, LN_dataframe$length &gt;= 200)\n\nThe dataset is now ready for use.\n\n# loading our custom functions\nlibrary(tm)\nlibrary(udpipe)\nlibrary(tidyverse)\nlibrary(textstem)\nclean_text_lemma &lt;- function(text){\n  #text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with \"\".\n  text &lt;- gsub(\"\\\\.\", \"\", text)\n  text &lt;- gsub(\"\\\\;\", \"\", text)\n  text &lt;- gsub(\"\\\\-\", \"\", text)\n  text &lt;- gsub(\"\\\\+\", \"plus\", text)\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  # here we apply lemmatization instead of stemming:\n  lemma_dico &lt;-  make_lemma_dictionary(text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\n  text &lt;- lemmatize_strings(text, dictionary = lemma_dico)\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n  text &lt;- gsub(\"  \", \" \", text)\n}\nc_value = function(words){\n# we need to compute the number of higher order terms and the frequency of these terms\n# we initiate two empty columns in which we can store this information\nwords$terms_containing_term &lt;- 0\nwords$Sum_freq_higher_terms &lt;- 0\n# We make sure the data has a dataframe format\nwords = as.data.frame(words)\n# we now loop over all the words to check how often they are nested\nfor(i in 1:dim(words)[1]){\n  # first we check in which term the term is nested\n  # if the term is part of another string it will return TRUE, FALSE otherwise\n  # The str_detect() function searches for a pattern in a second string and returns\n  # True if the pattern is part of the string\n  words$tmp = stringr::str_detect(words$keyword, words[i,1])\n  \n  # We then only keep the words that contain the word we are searching for\n  tmp = subset(words, words$keyword != words[i,1] & words$tmp == TRUE)\n  # The number of strings in which the pattern is nested is then simply the \n  # dimension of the dataframe we just found\n  words[i,4] &lt;- dim(tmp)[1]\n  # the sum of the frequencies is simply the sum of the individual frequencies\n  words[i,5] &lt;- sum(tmp$freq)\n}\n# now compute the c-value\n# we first adda column that will contain this value\nwords$C_value = 0\n# then we check if there are nested terms or not and apply the formula accordingly\nwords$C_value = case_when(\n    words$terms_containing_term == 0 ~ log2(words$ngram) * words$freq,\n    #keyword | n_gram | freq dataset | terms count terms | sum_freq_high\n    words$terms_containing_term != 0 ~ (log2(words$ngram) * (words$freq - (1/words$terms_containing_term) * words$Sum_freq_higher_terms))\n)\n# to make this work with the other functions we remove the \"tmp\" column...\nwords = words[,-6]\n#... and we reorganise the columns so that we do not have to adjust the other functions\nwords = words[,c(1,3,2,4,5,6)]\nreturn(words)\n}\nterm_extraction = function(Text_data, max_gram, min_freq){\n  # we need three elements of importance for the function to run\n  # the data, the max ngram and the minimum frequency\n  Text_df = as.data.frame(Text_data[1,])\n  x &lt;- udpipe_annotate(ud_model, x = as.character(Text_df[1,2]), doc_id = Text_df[1,1])\n  x &lt;- as.data.frame(x)\n  \n  stats &lt;- keywords_rake(x = x, term = \"lemma\", group = \"doc_id\", \n                         relevant = x$upos %in% c(\"NOUN\", \"ADJ\"))\n  stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword))\n  \n  \n  x$phrase_tag &lt;- as_phrasemachine(x$upos, type = \"upos\")\n  stats &lt;- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \n                            pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                            is_regex = TRUE, detailed = FALSE)\n  \n  stats = subset(stats, stats$ngram &gt;= 1 & stats$ngram &lt;= max_gram)\n  stats = subset(stats, stats$freq &gt;= min_freq)\n  return(stats)\n}\n# specify the dictionary\nmy_dico = c(\"a method\", \"patent\")\n# load the language model\nlibrary(udpipe)\nud_model &lt;- udpipe::udpipe_download_model(language = \"english\")\n#save(ud_model, file = \"ud_model.rdata\")\nud_model &lt;- udpipe::udpipe_load_model(ud_model)"
  },
  {
    "objectID": "Topic_modelling.html#topic-modelling-on-news-data",
    "href": "Topic_modelling.html#topic-modelling-on-news-data",
    "title": "Topic Modelling",
    "section": "",
    "text": "We start by prepping the text as we did in the first tutorial. We use a loop since it is likely that errors occur in this process that will push us to adjust parameters. The use of a loop makes it easier to pause and continue where we left of. Once the code is stabilised you can use the map() function from tidyverse to make this more efficient and run faster. Or even use multi-threading with the foreach package.\nColumn 11 contains the text, we start by lemmatizing the text.\n\nlibrary(textstem)\nLN_dataframe = LN_dataframe[-c(382,431, 464, 1087),]\nfor(i in 1:dim(LN_dataframe)[1]){\n  LN_dataframe[i,11] &lt;-clean_text_lemma(LN_dataframe[i,11])\n  # now er extract the terms\n  tmp &lt;- term_extraction(LN_dataframe[i,c(1,11)], max_gram = 4, min_freq =  1)\n  # and compute the c_value\n  tmp &lt;- c_value(tmp)\n  # we remove words with low values\n  tmp &lt;- subset(tmp, tmp$C_value &gt; 0)\n  # we combine the tokens\n  tmp$keyword &lt;- apply(as.matrix(tmp$keyword), 1, gsub, pattern = \" \", replacement = \"_\")\n  # and we put the text back into the document\n  LN_dataframe[i,11] &lt;- paste(tmp$keyword, collapse = \" \")\n}\n# we save the result in a .rdata file in case we make a mistake later\nsave(LN_dataframe, file = \"LN_dataframe_POS_lemmcleaning.rdata\")\n\nNow that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the DocumentTermMatrix() function from the tm package. This function has one argument which is a dataframe with the text of the corpus.\n\nlibrary(tm)\n# create the document-Term-matrix\ndtm &lt;- DocumentTermMatrix(LN_dataframe$Article)\n\nBased on this matrix we will now try to define how many topics we need to extract. For this we use the FindTopicsNumber and FindTopicsNumber_plot from the ldatuning package.\nThe FindTopicsNumber functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The seq() function creates a vector which starts at the from arguments, stops at the to argument. The size of the steps is set by the by argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, …) we would write seq(from = 2, to = 60, by = 5).\nWe then specify the metrics we want to compute, we have discussed these in the lecture.\n\n\n\n\n\n\nWarning\n\n\n\nFor an unknown reason, the “Griffiths2004” function does not work on mac OSX. It should work for windows users.\n\n\nThe mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algo is stuck, still running or finished.\n\nlibrary(ldatuning)\ntopic_num &lt;- FindTopicsNumber(\n  dtm,\n  topics = seq(from = 2, to = 60, by = 5),\n  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  mc.cores = 8L,\n  verbose = TRUE\n)\nFindTopicsNumber_plot(topic_num)\n\nNow that we have an idea of how many topics we need, so let’s extract the topics. We will use the LDA function from the topicmodels package.\n\nlibrary(topicmodels)\nlibrary(tidytext)\n# perform topic modelling\ntopics_model &lt;- LDA(dtm, k = 7)\n# get the betas\nbetas &lt;- tidytext::tidy(topics_model, matrix = \"beta\")\n# subset the betas for results\nbetas2&lt;- subset(betas, betas$topic %in% c(2,3,4,5,7))\n\nWe now have the topics and the betas for each topic-term couple. We only keep the highest values for each topic:\n\nap_top_terms &lt;- betas2 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nAnd now we visualise some of the results:\n\nlibrary(ggplot2)\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\nAdjust the parameters of the previous text cleaning functions. Adjust the dictionnary, maybe switch the noun phrases option ((A|N)*N(P+D*(A|N)*N)*) to an equivalent that includes actions.\n\n\n\nNow try the same logic on patent data. Check the results step by step. Adjust the dictionary etc. to suit patent data. What do you look for in the case of patent data, how is this different from news data?"
  }
]
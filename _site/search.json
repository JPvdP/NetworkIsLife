[
  {
    "objectID": "DAFS_prog_week_1.html",
    "href": "DAFS_prog_week_1.html",
    "title": "Week 1: The programming basics",
    "section": "",
    "text": "For this first session, the goal for you is to learn the following operations:\n\n\n\n\n\n\n\n\n\n\n\nR\nPython\nExample R\nExample Python\n\n\n\n\nLoad a library\n\n\nlibrary(dplyr)\nimport pandas as pd\n\n\nCreate a variable\n\n\nvalue &lt;- 42\nvalue = 42\n\n\nDifference between text, values, and variables\n\n\nplop &lt;- “42” plop &lt;- 42 plop &lt;- fourtytwo\nplop = “42” plop = 42 plop = fourtytwo\n\n\nPaste\n\n\npaste(“the value is”, plop)\nf”The value is {plop}\n\n\nPrint\n\n\nprint(paste(“the value is”, plop))\nprint(f”The value is {plop}“)\n\n\nBasic arithmetics\n\n\n42 + 48\n42 + 48\n\n\nVectors and lists\n\n\nc(1,2,3,4,68)\n[1,2,3,4,68]\n\n\nUse a function\n\n\nmax(c(1,2,3,4,68))\nmax([1,2,3,4,68])\n\n\nUse a function from a script\n\n\n\n\n\n\nUse logic operators\n\n\na != b\na != b\n\n\n\n\n\nR and Python come with basic, build-in functions. This means that when you launch R/Python you can use these functions directly. This is the case for the print function for example.\n\n\n\n\nprint(\"This is how we use the print function in R\")\n\n[1] \"This is how we use the print function in R\"\n\n\n\n\n\n\n\nprint(\"This is how we use the print function in Python\")\n\nThis is how we use the print function in Python\n\n\n\n\nThink of Python as a basic toolbox. This toolbox comes with some essential tools, like a hammer (the print() function), a screwdriver (the len() function), and a measuring tape (the max() function). These tools are available right away because they’re built into the toolbox (Python).\nHowever, sometimes you need more specialized tools that aren’t in the basic toolbox. For example, if you want to build a bookshelf, you might need a power drill or a saw. These tools aren’t included by default in the basic toolbox. In the world of Python, these extra tools are called packages.\nA package is like an extra toolbox full of new tools (functions) that someone else has created for a specific purpose. For example:\nIf you want to work with data in tables (like Excel), you’d install the pandas package. If you want to make graphs, you might need the matplotlib package. When you install a package, it’s like going to the store, buying the specialized toolbox, and adding it to your existing set of tools. Once installed, you can use the new tools (functions) it provides.\nLet’s see how we install packages in R and python:\n\n\n\n\ninstall.packages(\"dplyr\")\n\n\n\n\n\n\npip install pandas\n\n\n\nWe only have to install packages once. Once they are installed we only need to thell R/Python that we want to use the functions from these packages. we do this in the following way:\n\n\n\n\nlibrary(dplyr)\n\n\n\n\n\n\nimport pandas\n\n\n\nYou only have to load the packages once when you start working. Reload is only necessary if you have quit R/Python. Usually when you get the following error messages, this means that you did not load the package and therefore R/python cannot find the function you are trying to load:\n\n\n\n\nx = add.image(plop)\nError in add.image(plop) : could not find function \"add.image\"\n\n\n\n\n\n\ndf = pd.Dataframe()\nNameError: name 'pd' is not defined\n\n\n\n\n\n\n\n\n\nExercises part 1:\n\n\n\n\nInstall required packages in R:\nInstall the following packages in R: tidyverse, ggplot2, igraph\nInstall required packages in Python: Install the following packages in Python: pandas, numpy, matplotlib\n\n\n\n\n\n\nA variable is like a container or a labeled storage box that holds data or information. In programming, we use variables to store values—such as numbers, text, or more complex data—so that we can easily refer to and manipulate them throughout our code.\nWhy do we create variables?\n\nConvenience: Instead of typing the same value repeatedly, we store it in a variable and use the variable’s name. For example, if you store a value like 42 in a variable called age, you can use age throughout your program instead of the number 42.\nFlexibility: Variables allow us to use information that may change over time. If you store a value in a variable, you can update the variable’s content later without having to rewrite your entire program. For example, a variable temperature could hold the temperature of a room, which you might update as the temperature changes.\nReusability: Once you’ve stored a value in a variable, you can reuse it as many times as you want in your program. This avoids repetition and makes the code cleaner.\n\n\n\n\n\n# without variables\nprint(20 + 30)\n\n[1] 50\n\n# With variables\n\na &lt;- 20\nb &lt;- 30\nresult &lt;- a + b\nprint(result)\n\n[1] 50\n\n\n\n\n\n\n\n# without variables\nprint(20 + 30)\n\n50\n\n# With variables\n\na = 20\nb = 30\nresult = a + b\nprint(result)\n\n50\n\n\n\n\nCreating the variables a, b and result, allows us to use them later in the code. Beyond simple numerical values, other variables are created in an identical manner. In R, using “&lt;-” assigns the value/object on the right to the variable on the left. In python the logic is the same, but we use the “=” operator.\n\n\n\n\nSmall_matrix &lt;- matrix(0,nrow = 3, ncol = 3)\n# this creates a variables called Small_matrix that is an empty 3x3 matrix.\nprint(Small_matrix)\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\n\n\n\n\n\n# in python we first need to load a package to use a function to create a matrix\n# we load numpy\nimport numpy as np\n\n# then we create a matrix called \"small_matrix\"\n# this matrix is created by the \"zeros\" function from the np (numpy) package\nsmall_matrix = np.zeros((3, 3))\nprint(small_matrix)\n\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\n\n\n\nIn the examples above, we create a new variables small_matrix which contains a matrix. We can then use this later by referring to the name of the object. Below we add 3 to each cell of the matrix:\n\n\n\n\nSmall_matrix &lt;- Small_matrix + 3\n\n\n\n\n\n\nSmall_matrix = Small_matrix + 3\n\n\n\nBy writing the same name on the left and righ, the object Small_matrix is replaced by Small_matrix + 3 which means tha we cannot go back to the initial Small_matrix filled with 0’s.\nIf we want to keep both variables, we need can create a new variables:\n\n\n\n\nSmall_matrix_3 &lt;- Small_matrix + 3\n\n\n\n\n\n\nSmall_matrix_3 = Small_matrix + 3\n\n\n\nThis creates a new object with the name Small_matrix_3.\n\n\n\n\n\n\nImportant:\n\n\n\nWhat is the difference between a variable and an object? Why are small_matrix and Small_matrix_3 considered both a variable and an object?\n\n\n\n\nA variable is a name used to store data. It acts like a label or container for a value. For example, when we write small_matrix = np.zeros((3, 3)), small_matrix is a variable that holds the matrix.\nAn object is an instance of a class in Python. In this case, the value stored in small_matrix is an object of the numpy.ndarray class. This object has properties (like its shape) and methods (like matrix operations) that you can use.\n\nTherefore, small_matrix is a variable (a name for storing data) and also an object (the data itself, which is a matrix object).\n\n\n\n\n\n\n\n\n\nExercises part 2:\n\n\n\n\nCreate a Matrix:\nIn both R and Python, write a code to create a 4x4 matrix filled with zeros. What is the variable’s name, and what type of object does it store?\nVariable Assignment:\nAssign the number 10 to a variable named my_number. What happens if you assign a new value, like 20, to the same variable afterward? Explain the behavior of variables in Python.\nMatrix Operations:\nPerform an operation that adds 5 to each element in the matrix you created in question 1.\n\nRemember to think about how variables and objects work together in Python as you complete each task.\n\n\n\n\n\nA variable is like a labeled container that holds information (data). You give this container a name, and you can store anything inside it—like numbers, words, or even more complex data. An important distinction to make is the difference between numerical values and textual values. When we create a variable and a assign a number, this means we can later use this for mathematical operations. This needs to be differentiated from assigning the textual value “42”.\n\n\n\n\n# assigning a number\nvariable_numeric &lt;- 42 \n# assigning text\nvariable_text &lt;- \"42\"\n\n\n\n\n\n\n# assigning a number\nvariable_numeric = 42 \n# assigning text\nvariable_text = \"42\"\n\n\n\nAny value between “ is understood as text by python and R. This means that if we try to multiply variable by a number, we get an error in R and an unexpected output in Python.\n\n\n\n\nvariable_text &lt;- \"42\"\nvariable_text * 2\n# Error in variable_text * 2 : non-numeric argument to binary operator\n\n\n\n\n\n\nvariable_text = \"42\"\nvariable_text * 2\n'4242'\n\n\n\nA binary operator is for example +, -, /. When we try to use non-numeric values (here, “42”) then we get this error message. In python, the output is not an error, the string is simply multiplied literlly, resulting in a new string that is twice the previous sring: “4242” (which is not a number, still a string).\nIt’s good practice, especially when loading data from an external source for the first time, to check the format of the data, i.e ensuring that what you want to be numbers are numbers, and what you want to be text is indeed, text.\n\n\n\n\nvariable_text &lt;- \"42\"\n# check if the variable is numeric:\nis.numeric(variable_text)\n# check if the variable is text:\nis.character(variable_text)\n# Error in variable_text * 2 : non-numeric argument to binary operator\n\n\n\n\n\n\nvariable_text = \"42\"\n# check if the variable is numeric:\nisinstance(variable_text, float)\n# check if the variable is text:\nisinstance(variable_text, str)\n\n\n\n\n\n\n\n\n\n1. Integer (int)\n\n\n\n\nRepresents whole numbers, both positive and negative, without decimals.\nExample: 5, -42, 100\n\n\n\n\nRepresents numbers that have a decimal point. It can store both positive and negative decimal numbers.\nExample: 3.14, -0.001, 42.0\n\n\n\n\n\nRepresents a sequence of characters (text), typically enclosed in single or double quotes.\nExample: \"Hello, world!\", 'Python'\n\n\n\n\n\nRepresents a value that is either True or False. Booleans are often used in conditions and logical operations.\nExample: True, False\n\n\n\n\n\nIn Python, logical values are handled by the boolean (bool) type, where logical conditions return True or False.\nExample: a == b might evaluate to True or False.\n\n\n\n\n\n\n\n\n\n\nExercises part 3:\n\n\n\n\nCheck numeric values:\nAssign numeric values to 2 variables. You can name them however you want and assign any numerical value. Check the format of the variable\nCheck textual values:\nAssign a textual value to 2 variables. You can name them however you want and assign any numerical value. Check the format of the variable.\n\n\n\n\n\n\nLogical operators are commonly used in data analysis, especially when sub-setting datasets. For example when we want to extract documents that are from the year 2000 which have the term “sustainability” and the term “climate change” but not the term “fossil fuel”. Combining these operators is important, and so is understanding how they work.\n\n\n\n\n\n\nPay Attention\n\n\n\nSome differences between R and Python become apparent here. In R, TRUE and FALSE must be written in all caps to be recognised as the logical operator. In Python, True and False must start with a capitalized letter. or, and, not should also be written exactly in this manner.If these operators are written differently, they will be recognized as objects.\n\n\n\n\n\n\nx &lt;- 4\ny &lt;- 8\n# Equal (==)\nx == y\n\n[1] FALSE\n\n# And (&)\nx == 4 & y == 8\n\n[1] TRUE\n\n# Or (|)\nx == 4 | y == 8\n\n[1] TRUE\n\n# Not (!)\n!y\n\n[1] FALSE\n\n# Combine\nz &lt;- \"plop\"\nx == 4 & (y == 8 | z == \"plop\")\n\n[1] TRUE\n\n\n\n\n\n\n\nx = 4\ny = 8\n# Equal (==)\nx == y\n\nFalse\n\n# And \nx == 4 and y == 8\n\nTrue\n\n# Or\nx == 4 or y == 8\n\nTrue\n\n# Not\nnot x\n\nFalse\n\n# Combine\nz = \"plop\"\nx == 4 and (y == 8 or z == \"plop\")\n\nTrue\n\n\n\n\nIn data analysis, we usually use operators to subset data. This means that we compare a variable to a value to check if it fits our criteria. For example, if we have a column that contains a year, and we only want observations with the year 2003, we will search for year == 2003. In this setting the R operators we just described will be the same. It is possible that these operators varie when different packages are used in python. For instance, in the context of the pandas package, and becomes &, or becomes |, not becomes ~. We will adress these variations in the database manipulation chapter.\n\n\n\n\n\n\nExercises part 4.\n\n\n\n\nEquality and Inequality:\n\nIn both Python and R, write a condition that checks if the variable x is equal to 10 and print \"Equal\" if it is, otherwise print \"Not equal\".\n\nLogical AND:\n\nCreate two variables, age and has_permission. Write a condition that checks if age is greater than or equal to 18 and has_permission is True (or TRUE in R). Print \"Access granted\" if both conditions are met, otherwise print \"Access denied\".\n\nLogical OR:\n\nIn both Python and R, write a condition that checks if a number n is less than 5 or greater than 20. If either condition is True, print \"Out of bounds\", otherwise print \"Within bounds\".\n\nNOT operator:\n\nCreate a variable logged_in that is either True or False. Write a condition using the not operator (or ! in R) to print \"Access restricted\" if the user is not logged in.\n\nComplex Condition:\n\nIn both Python and R, write a condition that checks whether a number x is between 10 and 50 and either x is even or x is divisible by 7. Print a message that reflects the result.\n\n\n\n\n\n\n\nThe print function is crucial in programming because it allows developers to display information to the user or themselves during the development process. It provides immediate feedback, helping programmers debug code by checking the values of variables, verifying the flow of execution, or ensuring that certain conditions are met. Without print, it would be challenging to observe how the program behaves internally, making it a vital tool for both learning and real-world software development.\nA very basic example of this is printng out the value of a variable in the context of what it represents. Imagine that we are working on a project in which we start with some raw data, that we clean, step by step before we can start using it for a regression. In this process we remove missing values, outdates values, we might removes some regions etc. To make sure we don’t remove to much data, or even just to be sure we don’t make a mistake, we can decide to print out some information at different steps. The print function allows us to print out this type of information:\n\n\n\n\nplop &lt;- 42\n# print a numeric value\nprint(plop)\n# print text, remember to use \"\" for text\nprint(\"Step 3 is done\")\n\n\n\n\n\n\nplop = 42\n# print a numeric value\nprint(plop)\n# print text, remember to use \"\" for text\nprint(\"Step 3 is done\")\n\n\n\nWe can make things more interesting by combining values and text. For this we use the paste function in R and the “+” operator in python:\n\n\n\n\n# lets create a value at random and then print out the sentence: \"the number generated was x\"\n# we use the runif function to generate a value\n# this function takes three arguments, n, min and max. \n# n is the number of numbers we want to generate, min and max are the boundaries for the value\nx &lt;- runif(1, min = 0, max = 5)\n# here we generate one number with a value between 0  and 5.\n# Now we want to print out the sentence \"the number generated was x\"\n# for this we are going to paste the text \"the number generated was\" and the value of x:\npaste(\"The number generated was \", x)\n\n[1] \"The number generated was  2.56757443072274\"\n\n# the paste function can take as many argument as you want, it will paste all of them together\n# Now if we want to print the result:\nprint(paste(\"The number generated was \", x))\n\n[1] \"The number generated was  2.56757443072274\"\n\n\n\n\n\n\n\nimport random\n\n# Generate a random value between 0 and 5 using random.uniform()\nx = random.uniform(0, 5)\n\n# Print out the sentence \"The number generated was x\" using an f-string (formatted string)\n# to print a combination of text and variables, put an \"f\" before the string\n# put the name of the variable between {}:\nmessage = f\"The number generated was  {x}\"\nprint(message)\n\nThe number generated was  1.5100214984687903\n\n\n\n\n\n\n\n\n\n\nExercises: Using paste() and print()\n\n\n\n\nConcatenating Text and Numbers:\n\nGenerate a random number between 1 and 100 using the runif() function in R or random.uniform() in Python. Print a sentence that says, “The generated number is x”, where x is the random number.\n\nrithmetic Results in Sentences:\n\n\nCreate two variables a and b, with values of your choice. Calculate the sum, difference, and product of a and b, then print the results in complete sentences, e.g., “The sum of a and b is x”.\n\n\nBoolean Comparisons in Text:\n\n\nCreate two numbers, x and y. Write an expression that checks if x is greater than y. Print a sentence that says, “It is TRUE/FALSE that x is greater than y”, depending on the result of the comparison.\n\n\n\n\n\n\nR and python can be quite different when it comes to using functions. We will discuss them separately.\n\n\n\n\nCalling a function always works in the same way in R, the construction always follows:\n\nName of the function: paste(), print(), matrix(), data.frame(), as.numeric()\nBetween parenthesis we add the arguments of the function: matrix(nrow = 10, ncol = 3, 0)\n\n\nWhich arguments a function needs, and the format of these arguments can always be found in the help of rstudio. If you’re unsure how a function works, look it up, there is also always an example at the end of the help page.\nWe can combine multiple functions in one line of code. The following creates a matrix and then transforms the matrix into a dataframe.\n\nThesis_data_subset &lt;- as.data.frame(matrix(0, nrow = 20, ncol = 3))\n\nAn alternative exists in base R to make this easier to read. We can use what is called a pipe: “|&gt;”:\n\nThesis_data_subset &lt;-  matrix(0, nrow = 20, ncol = 3) |&gt; as.data.frame()\n\nThis line creates the matrix and then applies the as.data.frame() function. Equivalent to this is the pipe that we find in the tidyverse:\n\nlibrary(tidyverse)\nThesis_data_subset &lt;-  matrix(0, nrow = 20, ncol = 3) %&gt;% as.data.frame()\n\nThe latter is often found in data science with R manuals.\n\n\n\nWe do not expect you to make your own functions. We only want you to be able to identify costum functions in a script.\nIn R, a function is defined by the function function…let me clarify:\n\nmy_own_function &lt;- function(argument){\n  return(something)\n}\n\nWe create here a new function called my_own_function, which has one argument. Everything between brackets is the codes that is executed when the function is called.\nFor example, we could make a function that returns a greeting when supplied with a name. In other words, when you enter a name, the function will return “Hi, name”:\n\ngreet &lt;- function(name){\n  print(paste(\"Hi\", name))\n}\n\nWe create a function called “greet” which takes one argument which is the name. It then returns the text “Hi, name”.\nIf you see a function in a script, all you have to do is run the code once. This will load the function, just like the import function. You are then ready to use the function.\n\n\n\n\n\n\nWhen you see something between parentheses in Python, it usually indicates:\nYou’re calling a function, and the value between parentheses is an argument being passed to the function. For example, print(“Hello”) calls the print() function with the argument “Hello”.\nIn Python, dot notation (.) is used to access attributes or methods (functions) of objects. For example, numpy is a package that provides arrays and various mathematical functions, and you access specific functionality within numpy using dot notation. For example:\n\nimport numpy as np\n# Create a NumPy array\ndata = np.array([1, 2, 3, 4])\n# Access the shape of the array using dot notation\nprint(data.shape)  # Outputs the shape (dimensions) of the array\n\nIn this case, data is a NumPy array object, and data.shape is an attribute (a property) that tells us the dimensions of the array. The dot (.) is used to access this attribute from the data object.\nSimilarly, if you call a method of an object (a function that belongs to the object), you also use dot notation with parentheses:\n\n# Using a method to reshape the array\ndata_reshaped = data.reshape(2, 2)  # Calls the reshape method to change the array dimensions\n\n\nnp.array(): This is a function from the numpy module that creates an array.\ndata.shape: This accesses an attribute of the array data (no parentheses because it’s an attribute, not a function).\ndata.reshape(): This calls a method that modifies the array (parentheses are used because it’s a function).\n\n\n\n\n\n\n\nImportant:\n\n\n\n\nParentheses are used to call functions, and the values inside them are arguments.\nDot notation is used to access methods or attributes of an object, like using np.array() to create a NumPy array or data.shape to get the dimensions of an array.\nMethods (functions that belong to objects) also use parentheses, but attributes (like data.shape) don’t.\n\n\n\n\n\n\nWe do not expect you to make your own functions. We only want you to be able to identify costum functions in a script.\nIn Python, functions are defined using the def keyword, followed by the function name, parentheses (), and a colon :. Inside the parentheses, you can specify any input parameters (arguments) that the function will take. The code that the function executes is indented, and you can use the return statement to send back a result:\n\ndef function_name(parameters):\n    # Code block (indented)\n    return result\n\nFor example, we could make a function that returns a greeting when supplied with a name. In other words, when you enter a name, the function will return “Hi, name”:\n\ndef greet(name):\n    return f\"Hi, {name}!\"\n\nWe create a function called “greet” which takes one argument which is the name. It then returns the text “Hi, name”.\nIf you see a function in a script, all you have to do is run the code once. This will load the function, just like the import function. You are then ready to use the function.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Week 1: The programming basics"
    ]
  },
  {
    "objectID": "DAFS_prog_week_1.html#loading-and-installing-libaries",
    "href": "DAFS_prog_week_1.html#loading-and-installing-libaries",
    "title": "Week 1: The programming basics",
    "section": "",
    "text": "R and Python come with basic, build-in functions. This means that when you launch R/Python you can use these functions directly. This is the case for the print function for example.\n\n\n\n\nprint(\"This is how we use the print function in R\")\n\n[1] \"This is how we use the print function in R\"\n\n\n\n\n\n\n\nprint(\"This is how we use the print function in Python\")\n\nThis is how we use the print function in Python\n\n\n\n\nThink of Python as a basic toolbox. This toolbox comes with some essential tools, like a hammer (the print() function), a screwdriver (the len() function), and a measuring tape (the max() function). These tools are available right away because they’re built into the toolbox (Python).\nHowever, sometimes you need more specialized tools that aren’t in the basic toolbox. For example, if you want to build a bookshelf, you might need a power drill or a saw. These tools aren’t included by default in the basic toolbox. In the world of Python, these extra tools are called packages.\nA package is like an extra toolbox full of new tools (functions) that someone else has created for a specific purpose. For example:\nIf you want to work with data in tables (like Excel), you’d install the pandas package. If you want to make graphs, you might need the matplotlib package. When you install a package, it’s like going to the store, buying the specialized toolbox, and adding it to your existing set of tools. Once installed, you can use the new tools (functions) it provides.\nLet’s see how we install packages in R and python:\n\n\n\n\ninstall.packages(\"dplyr\")\n\n\n\n\n\n\npip install pandas\n\n\n\nWe only have to install packages once. Once they are installed we only need to thell R/Python that we want to use the functions from these packages. we do this in the following way:\n\n\n\n\nlibrary(dplyr)\n\n\n\n\n\n\nimport pandas\n\n\n\nYou only have to load the packages once when you start working. Reload is only necessary if you have quit R/Python. Usually when you get the following error messages, this means that you did not load the package and therefore R/python cannot find the function you are trying to load:\n\n\n\n\nx = add.image(plop)\nError in add.image(plop) : could not find function \"add.image\"\n\n\n\n\n\n\ndf = pd.Dataframe()\nNameError: name 'pd' is not defined\n\n\n\n\n\n\n\n\n\nExercises part 1:\n\n\n\n\nInstall required packages in R:\nInstall the following packages in R: tidyverse, ggplot2, igraph\nInstall required packages in Python: Install the following packages in Python: pandas, numpy, matplotlib",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Week 1: The programming basics"
    ]
  },
  {
    "objectID": "DAFS_prog_week_1.html#create-a-variable",
    "href": "DAFS_prog_week_1.html#create-a-variable",
    "title": "Week 1: The programming basics",
    "section": "",
    "text": "A variable is like a container or a labeled storage box that holds data or information. In programming, we use variables to store values—such as numbers, text, or more complex data—so that we can easily refer to and manipulate them throughout our code.\nWhy do we create variables?\n\nConvenience: Instead of typing the same value repeatedly, we store it in a variable and use the variable’s name. For example, if you store a value like 42 in a variable called age, you can use age throughout your program instead of the number 42.\nFlexibility: Variables allow us to use information that may change over time. If you store a value in a variable, you can update the variable’s content later without having to rewrite your entire program. For example, a variable temperature could hold the temperature of a room, which you might update as the temperature changes.\nReusability: Once you’ve stored a value in a variable, you can reuse it as many times as you want in your program. This avoids repetition and makes the code cleaner.\n\n\n\n\n\n# without variables\nprint(20 + 30)\n\n[1] 50\n\n# With variables\n\na &lt;- 20\nb &lt;- 30\nresult &lt;- a + b\nprint(result)\n\n[1] 50\n\n\n\n\n\n\n\n# without variables\nprint(20 + 30)\n\n50\n\n# With variables\n\na = 20\nb = 30\nresult = a + b\nprint(result)\n\n50\n\n\n\n\nCreating the variables a, b and result, allows us to use them later in the code. Beyond simple numerical values, other variables are created in an identical manner. In R, using “&lt;-” assigns the value/object on the right to the variable on the left. In python the logic is the same, but we use the “=” operator.\n\n\n\n\nSmall_matrix &lt;- matrix(0,nrow = 3, ncol = 3)\n# this creates a variables called Small_matrix that is an empty 3x3 matrix.\nprint(Small_matrix)\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\n\n\n\n\n\n# in python we first need to load a package to use a function to create a matrix\n# we load numpy\nimport numpy as np\n\n# then we create a matrix called \"small_matrix\"\n# this matrix is created by the \"zeros\" function from the np (numpy) package\nsmall_matrix = np.zeros((3, 3))\nprint(small_matrix)\n\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\n\n\n\nIn the examples above, we create a new variables small_matrix which contains a matrix. We can then use this later by referring to the name of the object. Below we add 3 to each cell of the matrix:\n\n\n\n\nSmall_matrix &lt;- Small_matrix + 3\n\n\n\n\n\n\nSmall_matrix = Small_matrix + 3\n\n\n\nBy writing the same name on the left and righ, the object Small_matrix is replaced by Small_matrix + 3 which means tha we cannot go back to the initial Small_matrix filled with 0’s.\nIf we want to keep both variables, we need can create a new variables:\n\n\n\n\nSmall_matrix_3 &lt;- Small_matrix + 3\n\n\n\n\n\n\nSmall_matrix_3 = Small_matrix + 3\n\n\n\nThis creates a new object with the name Small_matrix_3.\n\n\n\n\n\n\nImportant:\n\n\n\nWhat is the difference between a variable and an object? Why are small_matrix and Small_matrix_3 considered both a variable and an object?\n\n\n\n\nA variable is a name used to store data. It acts like a label or container for a value. For example, when we write small_matrix = np.zeros((3, 3)), small_matrix is a variable that holds the matrix.\nAn object is an instance of a class in Python. In this case, the value stored in small_matrix is an object of the numpy.ndarray class. This object has properties (like its shape) and methods (like matrix operations) that you can use.\n\nTherefore, small_matrix is a variable (a name for storing data) and also an object (the data itself, which is a matrix object).\n\n\n\n\n\n\n\n\n\nExercises part 2:\n\n\n\n\nCreate a Matrix:\nIn both R and Python, write a code to create a 4x4 matrix filled with zeros. What is the variable’s name, and what type of object does it store?\nVariable Assignment:\nAssign the number 10 to a variable named my_number. What happens if you assign a new value, like 20, to the same variable afterward? Explain the behavior of variables in Python.\nMatrix Operations:\nPerform an operation that adds 5 to each element in the matrix you created in question 1.\n\nRemember to think about how variables and objects work together in Python as you complete each task.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Week 1: The programming basics"
    ]
  },
  {
    "objectID": "DAFS_prog_week_1.html#text-values-and-variables",
    "href": "DAFS_prog_week_1.html#text-values-and-variables",
    "title": "Week 1: The programming basics",
    "section": "",
    "text": "A variable is like a labeled container that holds information (data). You give this container a name, and you can store anything inside it—like numbers, words, or even more complex data. An important distinction to make is the difference between numerical values and textual values. When we create a variable and a assign a number, this means we can later use this for mathematical operations. This needs to be differentiated from assigning the textual value “42”.\n\n\n\n\n# assigning a number\nvariable_numeric &lt;- 42 \n# assigning text\nvariable_text &lt;- \"42\"\n\n\n\n\n\n\n# assigning a number\nvariable_numeric = 42 \n# assigning text\nvariable_text = \"42\"\n\n\n\nAny value between “ is understood as text by python and R. This means that if we try to multiply variable by a number, we get an error in R and an unexpected output in Python.\n\n\n\n\nvariable_text &lt;- \"42\"\nvariable_text * 2\n# Error in variable_text * 2 : non-numeric argument to binary operator\n\n\n\n\n\n\nvariable_text = \"42\"\nvariable_text * 2\n'4242'\n\n\n\nA binary operator is for example +, -, /. When we try to use non-numeric values (here, “42”) then we get this error message. In python, the output is not an error, the string is simply multiplied literlly, resulting in a new string that is twice the previous sring: “4242” (which is not a number, still a string).\nIt’s good practice, especially when loading data from an external source for the first time, to check the format of the data, i.e ensuring that what you want to be numbers are numbers, and what you want to be text is indeed, text.\n\n\n\n\nvariable_text &lt;- \"42\"\n# check if the variable is numeric:\nis.numeric(variable_text)\n# check if the variable is text:\nis.character(variable_text)\n# Error in variable_text * 2 : non-numeric argument to binary operator\n\n\n\n\n\n\nvariable_text = \"42\"\n# check if the variable is numeric:\nisinstance(variable_text, float)\n# check if the variable is text:\nisinstance(variable_text, str)\n\n\n\n\n\n\n\n\n\n1. Integer (int)\n\n\n\n\nRepresents whole numbers, both positive and negative, without decimals.\nExample: 5, -42, 100\n\n\n\n\nRepresents numbers that have a decimal point. It can store both positive and negative decimal numbers.\nExample: 3.14, -0.001, 42.0\n\n\n\n\n\nRepresents a sequence of characters (text), typically enclosed in single or double quotes.\nExample: \"Hello, world!\", 'Python'\n\n\n\n\n\nRepresents a value that is either True or False. Booleans are often used in conditions and logical operations.\nExample: True, False\n\n\n\n\n\nIn Python, logical values are handled by the boolean (bool) type, where logical conditions return True or False.\nExample: a == b might evaluate to True or False.\n\n\n\n\n\n\n\n\n\n\nExercises part 3:\n\n\n\n\nCheck numeric values:\nAssign numeric values to 2 variables. You can name them however you want and assign any numerical value. Check the format of the variable\nCheck textual values:\nAssign a textual value to 2 variables. You can name them however you want and assign any numerical value. Check the format of the variable.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Week 1: The programming basics"
    ]
  },
  {
    "objectID": "DAFS_prog_week_1.html#logic-operators",
    "href": "DAFS_prog_week_1.html#logic-operators",
    "title": "Week 1: The programming basics",
    "section": "",
    "text": "Logical operators are commonly used in data analysis, especially when sub-setting datasets. For example when we want to extract documents that are from the year 2000 which have the term “sustainability” and the term “climate change” but not the term “fossil fuel”. Combining these operators is important, and so is understanding how they work.\n\n\n\n\n\n\nPay Attention\n\n\n\nSome differences between R and Python become apparent here. In R, TRUE and FALSE must be written in all caps to be recognised as the logical operator. In Python, True and False must start with a capitalized letter. or, and, not should also be written exactly in this manner.If these operators are written differently, they will be recognized as objects.\n\n\n\n\n\n\nx &lt;- 4\ny &lt;- 8\n# Equal (==)\nx == y\n\n[1] FALSE\n\n# And (&)\nx == 4 & y == 8\n\n[1] TRUE\n\n# Or (|)\nx == 4 | y == 8\n\n[1] TRUE\n\n# Not (!)\n!y\n\n[1] FALSE\n\n# Combine\nz &lt;- \"plop\"\nx == 4 & (y == 8 | z == \"plop\")\n\n[1] TRUE\n\n\n\n\n\n\n\nx = 4\ny = 8\n# Equal (==)\nx == y\n\nFalse\n\n# And \nx == 4 and y == 8\n\nTrue\n\n# Or\nx == 4 or y == 8\n\nTrue\n\n# Not\nnot x\n\nFalse\n\n# Combine\nz = \"plop\"\nx == 4 and (y == 8 or z == \"plop\")\n\nTrue\n\n\n\n\nIn data analysis, we usually use operators to subset data. This means that we compare a variable to a value to check if it fits our criteria. For example, if we have a column that contains a year, and we only want observations with the year 2003, we will search for year == 2003. In this setting the R operators we just described will be the same. It is possible that these operators varie when different packages are used in python. For instance, in the context of the pandas package, and becomes &, or becomes |, not becomes ~. We will adress these variations in the database manipulation chapter.\n\n\n\n\n\n\nExercises part 4.\n\n\n\n\nEquality and Inequality:\n\nIn both Python and R, write a condition that checks if the variable x is equal to 10 and print \"Equal\" if it is, otherwise print \"Not equal\".\n\nLogical AND:\n\nCreate two variables, age and has_permission. Write a condition that checks if age is greater than or equal to 18 and has_permission is True (or TRUE in R). Print \"Access granted\" if both conditions are met, otherwise print \"Access denied\".\n\nLogical OR:\n\nIn both Python and R, write a condition that checks if a number n is less than 5 or greater than 20. If either condition is True, print \"Out of bounds\", otherwise print \"Within bounds\".\n\nNOT operator:\n\nCreate a variable logged_in that is either True or False. Write a condition using the not operator (or ! in R) to print \"Access restricted\" if the user is not logged in.\n\nComplex Condition:\n\nIn both Python and R, write a condition that checks whether a number x is between 10 and 50 and either x is even or x is divisible by 7. Print a message that reflects the result.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Week 1: The programming basics"
    ]
  },
  {
    "objectID": "DAFS_prog_week_1.html#the-print-and-paste-functions",
    "href": "DAFS_prog_week_1.html#the-print-and-paste-functions",
    "title": "Week 1: The programming basics",
    "section": "",
    "text": "The print function is crucial in programming because it allows developers to display information to the user or themselves during the development process. It provides immediate feedback, helping programmers debug code by checking the values of variables, verifying the flow of execution, or ensuring that certain conditions are met. Without print, it would be challenging to observe how the program behaves internally, making it a vital tool for both learning and real-world software development.\nA very basic example of this is printng out the value of a variable in the context of what it represents. Imagine that we are working on a project in which we start with some raw data, that we clean, step by step before we can start using it for a regression. In this process we remove missing values, outdates values, we might removes some regions etc. To make sure we don’t remove to much data, or even just to be sure we don’t make a mistake, we can decide to print out some information at different steps. The print function allows us to print out this type of information:\n\n\n\n\nplop &lt;- 42\n# print a numeric value\nprint(plop)\n# print text, remember to use \"\" for text\nprint(\"Step 3 is done\")\n\n\n\n\n\n\nplop = 42\n# print a numeric value\nprint(plop)\n# print text, remember to use \"\" for text\nprint(\"Step 3 is done\")\n\n\n\nWe can make things more interesting by combining values and text. For this we use the paste function in R and the “+” operator in python:\n\n\n\n\n# lets create a value at random and then print out the sentence: \"the number generated was x\"\n# we use the runif function to generate a value\n# this function takes three arguments, n, min and max. \n# n is the number of numbers we want to generate, min and max are the boundaries for the value\nx &lt;- runif(1, min = 0, max = 5)\n# here we generate one number with a value between 0  and 5.\n# Now we want to print out the sentence \"the number generated was x\"\n# for this we are going to paste the text \"the number generated was\" and the value of x:\npaste(\"The number generated was \", x)\n\n[1] \"The number generated was  2.56757443072274\"\n\n# the paste function can take as many argument as you want, it will paste all of them together\n# Now if we want to print the result:\nprint(paste(\"The number generated was \", x))\n\n[1] \"The number generated was  2.56757443072274\"\n\n\n\n\n\n\n\nimport random\n\n# Generate a random value between 0 and 5 using random.uniform()\nx = random.uniform(0, 5)\n\n# Print out the sentence \"The number generated was x\" using an f-string (formatted string)\n# to print a combination of text and variables, put an \"f\" before the string\n# put the name of the variable between {}:\nmessage = f\"The number generated was  {x}\"\nprint(message)\n\nThe number generated was  1.5100214984687903\n\n\n\n\n\n\n\n\n\n\nExercises: Using paste() and print()\n\n\n\n\nConcatenating Text and Numbers:\n\nGenerate a random number between 1 and 100 using the runif() function in R or random.uniform() in Python. Print a sentence that says, “The generated number is x”, where x is the random number.\n\nrithmetic Results in Sentences:\n\n\nCreate two variables a and b, with values of your choice. Calculate the sum, difference, and product of a and b, then print the results in complete sentences, e.g., “The sum of a and b is x”.\n\n\nBoolean Comparisons in Text:\n\n\nCreate two numbers, x and y. Write an expression that checks if x is greater than y. Print a sentence that says, “It is TRUE/FALSE that x is greater than y”, depending on the result of the comparison.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Week 1: The programming basics"
    ]
  },
  {
    "objectID": "DAFS_prog_week_1.html#using-functions",
    "href": "DAFS_prog_week_1.html#using-functions",
    "title": "Week 1: The programming basics",
    "section": "",
    "text": "R and python can be quite different when it comes to using functions. We will discuss them separately.\n\n\n\n\nCalling a function always works in the same way in R, the construction always follows:\n\nName of the function: paste(), print(), matrix(), data.frame(), as.numeric()\nBetween parenthesis we add the arguments of the function: matrix(nrow = 10, ncol = 3, 0)\n\n\nWhich arguments a function needs, and the format of these arguments can always be found in the help of rstudio. If you’re unsure how a function works, look it up, there is also always an example at the end of the help page.\nWe can combine multiple functions in one line of code. The following creates a matrix and then transforms the matrix into a dataframe.\n\nThesis_data_subset &lt;- as.data.frame(matrix(0, nrow = 20, ncol = 3))\n\nAn alternative exists in base R to make this easier to read. We can use what is called a pipe: “|&gt;”:\n\nThesis_data_subset &lt;-  matrix(0, nrow = 20, ncol = 3) |&gt; as.data.frame()\n\nThis line creates the matrix and then applies the as.data.frame() function. Equivalent to this is the pipe that we find in the tidyverse:\n\nlibrary(tidyverse)\nThesis_data_subset &lt;-  matrix(0, nrow = 20, ncol = 3) %&gt;% as.data.frame()\n\nThe latter is often found in data science with R manuals.\n\n\n\nWe do not expect you to make your own functions. We only want you to be able to identify costum functions in a script.\nIn R, a function is defined by the function function…let me clarify:\n\nmy_own_function &lt;- function(argument){\n  return(something)\n}\n\nWe create here a new function called my_own_function, which has one argument. Everything between brackets is the codes that is executed when the function is called.\nFor example, we could make a function that returns a greeting when supplied with a name. In other words, when you enter a name, the function will return “Hi, name”:\n\ngreet &lt;- function(name){\n  print(paste(\"Hi\", name))\n}\n\nWe create a function called “greet” which takes one argument which is the name. It then returns the text “Hi, name”.\nIf you see a function in a script, all you have to do is run the code once. This will load the function, just like the import function. You are then ready to use the function.\n\n\n\n\n\n\nWhen you see something between parentheses in Python, it usually indicates:\nYou’re calling a function, and the value between parentheses is an argument being passed to the function. For example, print(“Hello”) calls the print() function with the argument “Hello”.\nIn Python, dot notation (.) is used to access attributes or methods (functions) of objects. For example, numpy is a package that provides arrays and various mathematical functions, and you access specific functionality within numpy using dot notation. For example:\n\nimport numpy as np\n# Create a NumPy array\ndata = np.array([1, 2, 3, 4])\n# Access the shape of the array using dot notation\nprint(data.shape)  # Outputs the shape (dimensions) of the array\n\nIn this case, data is a NumPy array object, and data.shape is an attribute (a property) that tells us the dimensions of the array. The dot (.) is used to access this attribute from the data object.\nSimilarly, if you call a method of an object (a function that belongs to the object), you also use dot notation with parentheses:\n\n# Using a method to reshape the array\ndata_reshaped = data.reshape(2, 2)  # Calls the reshape method to change the array dimensions\n\n\nnp.array(): This is a function from the numpy module that creates an array.\ndata.shape: This accesses an attribute of the array data (no parentheses because it’s an attribute, not a function).\ndata.reshape(): This calls a method that modifies the array (parentheses are used because it’s a function).\n\n\n\n\n\n\n\nImportant:\n\n\n\n\nParentheses are used to call functions, and the values inside them are arguments.\nDot notation is used to access methods or attributes of an object, like using np.array() to create a NumPy array or data.shape to get the dimensions of an array.\nMethods (functions that belong to objects) also use parentheses, but attributes (like data.shape) don’t.\n\n\n\n\n\n\nWe do not expect you to make your own functions. We only want you to be able to identify costum functions in a script.\nIn Python, functions are defined using the def keyword, followed by the function name, parentheses (), and a colon :. Inside the parentheses, you can specify any input parameters (arguments) that the function will take. The code that the function executes is indented, and you can use the return statement to send back a result:\n\ndef function_name(parameters):\n    # Code block (indented)\n    return result\n\nFor example, we could make a function that returns a greeting when supplied with a name. In other words, when you enter a name, the function will return “Hi, name”:\n\ndef greet(name):\n    return f\"Hi, {name}!\"\n\nWe create a function called “greet” which takes one argument which is the name. It then returns the text “Hi, name”.\nIf you see a function in a script, all you have to do is run the code once. This will load the function, just like the import function. You are then ready to use the function.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Week 1: The programming basics"
    ]
  },
  {
    "objectID": "Topic_modelling_bert_practise.html",
    "href": "Topic_modelling_bert_practise.html",
    "title": "Create the model (detailed)",
    "section": "",
    "text": "We start by loading required packages\n\nimport pandas as pd\nfrom bertopic import BERTopic\n\nOnce the packages have loaded correctly we need to import some data. For the purpose of this workshop we will import a csv file, but you can technically import any data format that contains text. In the following line, replace the name of the file with yours. For simplicity, the data is stored at the root where python in running. For windows users this is the folder that has the same name as your login user name.\n\n\n\ndata_full = pd.read_csv(\"entr_eco_publications.csv\", sep = \";\")\n\nThe input data for bertopic needs to have a specific format: it need to be a list. We therefore transform our dataframe into a list. In addition, we only need the column that has the text, in our case the “abstract” column.\n\n# We first select only the column with the text:\ndata = data_full['Abstract']\n# We transform the dataframe into a list:\ndata = data.apply(lambda row: ' '.join(row.values.astype(str)),axis=1).tolist() \n\nNow that the data is in the right format we start with the setting of the parameters of the model.\n\n\n\nFirst up are the embeddings we want to use for the data. Embeddings in text mining are representations of words or phrases in a numerical form. These numerical representations capture the semantic meaning of the words or phrases, allowing machines to understand and process text more effectively. Think of embeddings as a way to convert words into vectors of numbers, where similar words are represented by similar vectors. This enables algorithms to analyze, compare, and make predictions based on the meaning of the text rather than just the raw text itself. There are multiple ways to do this, some embeddings have specific purposes (models for patents for instance), others are more general, some focus on one language, others on multiple. In the following image some words are positioned in space. The closer the words, the more proximity between them. What we download are basically the coordinates of the words in this space and replace each word with it’s coordinates. Doing this allows us to take into account the distance between the words rather than considering each word at the same distance from each other word.\n\nFor this tutorial we pick a small, low-weight embedding model to not waste too much time on computation. Other models are available, check Hugging Face for inspiration.\n\nfrom sentence_transformers import SentenceTransformer\nembed_model = SentenceTransformer(\"all-MiniLM-L6-V2\")\n\n\n\n\n\n\n\nImportant\n\n\n\nYou choice in this step resides in the model of embeddings that you pick.\n\n\nthe embed_model object contains the coordinates for the documents in an n-dimensional space. The model we used has X dimensions and therefore our document have X-dimensional vectors. Clustering data with this many dimensions presents many issues, we therefore now move on to reduce the dimensions of these vectors.\n\n\n\nThe high-dimensional embeddings can be quite complex and computationally expensive to cluster directly. In addition, there is a point at which additional dimensions only add marginal information and slow down the process more than they add value. UMAP helps by reducing the dimensionality of the embeddings. It maps the data into a lower-dimensional space (typically 2D or 3D for visualization purposes, but higher dimensions can be used for clustering), aiming to maintain the original data’s essential structure. Our data is usually of a lower dimension than that of the model. We therefore use a technique to reduce the dimension of the data. The Uniform Manifold Approximation and Projection (UMAP) technique is used for this.\nFor example, we can represent our words in a two-dimensional space based on our embeddings. This would look something like this:\n\nThe UMAP algorithm reduced the dimension of the data so that clustering algorithms can more efficiently identify clusters. Umap has many parameters that we can tweak to improve the results. The main parameters that we can play with are the following: (for a full explanation on this topic see this webiste):\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nn_components\nThis specifies the number of dimensions to which the data should be reduced. For example, n_components = 5 means that UMAP will reduce the data to 5 dimensions. This is useful when you want to reduce dimensionality but retain more structure in multiple dimensions, which can be important for clustering algorithms that follow the dimensionality reduction.\n\n\nn_neighbors\nA larger value for n_neighbors encourages UMAP to take a broader view of the data structure, making it more focused on preserving the global data structure. Conversely, a smaller value makes UMAP focus more on local data structures.\n\n\nmin_dist\nA smaller value of min_dist allows points to cluster more closely, which is useful for capturing fine-grained patterns or clusters in the data. Conversely, a larger value spreads points further apart, which can help in seeing the broader structure even if it might obscure some of the finer details.\n\n\nmetric\nThe distance metric used to measure the similarity between points in the high-dimensional space (cosine, euclidean)\n\n\n\nIn the following figure we give an intuition for what these parameters do. The n_components defines the dimension of the space (2 in the figure). n_neighbors defines how far to search for points (represented by the circles). min_distance controls how closely we will search for points to reduce the dimension (green arrows), the higher the value, the further away we search, the broader the clusters and therefore, the topics.\n\n\nfrom umap import UMAP\numap_model = UMAP(n_neighbors = 30, n_components = 5, min_dist = 0.0, metric = \"cosine\", random_state = 42)\n\n\n\n\nOnce we have reduced the dimensions of the data, we need to cluster the documents together into topics. For this purpose the HDBSCAN algorithm is used.\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nmin_cluster_size\nthe minimum size of clusters that HDBSCAN will consider valid. If min_cluster_size = 7, we need at least 7 documents in a cluster to consider this to be a cluster\n\n\nmetric\nThis is the distance metric used to measure the similarity or distance between points in the feature space\n\n\ncluster_selection_method\nThis parameter determines how clusters are selected from the hierarchical tree generated during the clustering process. The ‘eom’ method focuses on finding clusters that persist over a significant range of cluster stability (i.e., clusters that have a lot of “excess of mass”). This tends to result in selecting larger, more prominent clusters and is generally more robust to noise. The alternative is ‘leaf’, which would consider all possible clusters at the leaves of the tree, often resulting in a larger number of smaller clusters.\n\n\nprediction_data\nWhen set to True, this parameter instructs HDBSCAN to generate additional data about the cluster hierarchy and the soft clustering assignments, which can be used for predicting the cluster memberships of new, unseen data. This is particularly useful if you intend to use the clustering model as part of a production pipeline where new documents need to be automatically assigned to the existing clusters/topics identified during the initial clustering.\n\n\n\n\nfrom hdbscan import HDBSCAN\nhdbscan_model = HDBSCAN(min_cluster_size = 7, metric = \"euclidean\", cluster_selection_method = 'eom', prediction_data = True)\n\n\n\n\nNow that we have the clusters of documents, all that’s left to do is identify the words that best represent the clusters. This is done by a representation model, basically this means that we apply different text mining techniques to the text of the documents to only show the most salient terms.\nIn this step we chose different elements, mainly:\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nstop_words\nWe don’t want any stopwords reperesenting our topics, we remove the stopwords for this reason. sklearn has build-in dictionaries per language. We set this option to “english” to remove english stop words\n\n\nmin_df\nStands for “Minimum document frequency”. It represents the minimum number of documents in which the term should appear. When a float is given, it will be interpreted as the proportion of documents\n\n\nngram_range\nHow many tokens a word can have (1 = “fuel”, 2 = “fuel cell”, 3 = “fuel cell system”). We provide this information as a range. If we want tokens of size 2 exclusively, we provide (2,2), if we want words between 1 and 3 tokens, we provide (1,3)\n\n\nMaximalMarginalRelevance\nThe goal of MMR is to select documents or pieces of information that are both relevant to a query and diverse from each other, thereby reducing redundancy in the results presented to a user. λ is a parameter (ranging from 0 to 1) that controls the balance between relevance and diversity. A higher λ gives more weight to relevance, while a lower λ emphasizes diversity.\n\n\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect_model = CountVectorizer(stop_words = \"english\", min_df = 2, ngram_range = (1,1)) \nfrom bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\nimport spacy\nkeybert_model = KeyBERTInspired()\nmmr_model = MaximalMarginalRelevance(diversity = 0.3)\n\nUsing this we create the representation model:\n\nrepresentation_model = {\n    \"keyBERT\": keybert_model,\n    \"MMR\": mmr_model,\n #  \"POS\": pos_model\n}\n\nThe final Bert model is the the group of all the provided models:\n\n# regroup all the models into one\ntopic_model = BERTopic(\n    embedding_model = embed_model,\n    umap_model = umap_model,\n    hdbscan_model = hdbscan_model,\n    vectorizer_model = vect_model,\n    representation_model = representation_model,\n    top_n_words = 20, # how many words to include in the representation\n    verbose = True, # this options ensures that the function returns some info while running\n    calculate_probabilities = True\n)\n\nBy default, each document is assigned to one topic. However, base on the text analysis of the topics, each document can get a probability distribution for all the topics. If you prefer to have probabilities for each document to be assigned to each topic, add the calculate_probabilities = True option to the model.\nYou are now ready to run the model!\nThe code for this is the following:\n\ntopics, probs = topic_model.fit_transform(data)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (detailed)"
    ]
  },
  {
    "objectID": "Topic_modelling_bert_practise.html#step-1-loading-the-data",
    "href": "Topic_modelling_bert_practise.html#step-1-loading-the-data",
    "title": "Create the model (detailed)",
    "section": "",
    "text": "data_full = pd.read_csv(\"entr_eco_publications.csv\", sep = \";\")\n\nThe input data for bertopic needs to have a specific format: it need to be a list. We therefore transform our dataframe into a list. In addition, we only need the column that has the text, in our case the “abstract” column.\n\n# We first select only the column with the text:\ndata = data_full['Abstract']\n# We transform the dataframe into a list:\ndata = data.apply(lambda row: ' '.join(row.values.astype(str)),axis=1).tolist() \n\nNow that the data is in the right format we start with the setting of the parameters of the model.",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (detailed)"
    ]
  },
  {
    "objectID": "Topic_modelling_bert_practise.html#step-2-creating-the-embeddings",
    "href": "Topic_modelling_bert_practise.html#step-2-creating-the-embeddings",
    "title": "Create the model (detailed)",
    "section": "",
    "text": "First up are the embeddings we want to use for the data. Embeddings in text mining are representations of words or phrases in a numerical form. These numerical representations capture the semantic meaning of the words or phrases, allowing machines to understand and process text more effectively. Think of embeddings as a way to convert words into vectors of numbers, where similar words are represented by similar vectors. This enables algorithms to analyze, compare, and make predictions based on the meaning of the text rather than just the raw text itself. There are multiple ways to do this, some embeddings have specific purposes (models for patents for instance), others are more general, some focus on one language, others on multiple. In the following image some words are positioned in space. The closer the words, the more proximity between them. What we download are basically the coordinates of the words in this space and replace each word with it’s coordinates. Doing this allows us to take into account the distance between the words rather than considering each word at the same distance from each other word.\n\nFor this tutorial we pick a small, low-weight embedding model to not waste too much time on computation. Other models are available, check Hugging Face for inspiration.\n\nfrom sentence_transformers import SentenceTransformer\nembed_model = SentenceTransformer(\"all-MiniLM-L6-V2\")\n\n\n\n\n\n\n\nImportant\n\n\n\nYou choice in this step resides in the model of embeddings that you pick.\n\n\nthe embed_model object contains the coordinates for the documents in an n-dimensional space. The model we used has X dimensions and therefore our document have X-dimensional vectors. Clustering data with this many dimensions presents many issues, we therefore now move on to reduce the dimensions of these vectors.",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (detailed)"
    ]
  },
  {
    "objectID": "Topic_modelling_bert_practise.html#step-3-dimension-reduction",
    "href": "Topic_modelling_bert_practise.html#step-3-dimension-reduction",
    "title": "Create the model (detailed)",
    "section": "",
    "text": "The high-dimensional embeddings can be quite complex and computationally expensive to cluster directly. In addition, there is a point at which additional dimensions only add marginal information and slow down the process more than they add value. UMAP helps by reducing the dimensionality of the embeddings. It maps the data into a lower-dimensional space (typically 2D or 3D for visualization purposes, but higher dimensions can be used for clustering), aiming to maintain the original data’s essential structure. Our data is usually of a lower dimension than that of the model. We therefore use a technique to reduce the dimension of the data. The Uniform Manifold Approximation and Projection (UMAP) technique is used for this.\nFor example, we can represent our words in a two-dimensional space based on our embeddings. This would look something like this:\n\nThe UMAP algorithm reduced the dimension of the data so that clustering algorithms can more efficiently identify clusters. Umap has many parameters that we can tweak to improve the results. The main parameters that we can play with are the following: (for a full explanation on this topic see this webiste):\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nn_components\nThis specifies the number of dimensions to which the data should be reduced. For example, n_components = 5 means that UMAP will reduce the data to 5 dimensions. This is useful when you want to reduce dimensionality but retain more structure in multiple dimensions, which can be important for clustering algorithms that follow the dimensionality reduction.\n\n\nn_neighbors\nA larger value for n_neighbors encourages UMAP to take a broader view of the data structure, making it more focused on preserving the global data structure. Conversely, a smaller value makes UMAP focus more on local data structures.\n\n\nmin_dist\nA smaller value of min_dist allows points to cluster more closely, which is useful for capturing fine-grained patterns or clusters in the data. Conversely, a larger value spreads points further apart, which can help in seeing the broader structure even if it might obscure some of the finer details.\n\n\nmetric\nThe distance metric used to measure the similarity between points in the high-dimensional space (cosine, euclidean)\n\n\n\nIn the following figure we give an intuition for what these parameters do. The n_components defines the dimension of the space (2 in the figure). n_neighbors defines how far to search for points (represented by the circles). min_distance controls how closely we will search for points to reduce the dimension (green arrows), the higher the value, the further away we search, the broader the clusters and therefore, the topics.\n\n\nfrom umap import UMAP\numap_model = UMAP(n_neighbors = 30, n_components = 5, min_dist = 0.0, metric = \"cosine\", random_state = 42)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (detailed)"
    ]
  },
  {
    "objectID": "Topic_modelling_bert_practise.html#step-4-clustering",
    "href": "Topic_modelling_bert_practise.html#step-4-clustering",
    "title": "Create the model (detailed)",
    "section": "",
    "text": "Once we have reduced the dimensions of the data, we need to cluster the documents together into topics. For this purpose the HDBSCAN algorithm is used.\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nmin_cluster_size\nthe minimum size of clusters that HDBSCAN will consider valid. If min_cluster_size = 7, we need at least 7 documents in a cluster to consider this to be a cluster\n\n\nmetric\nThis is the distance metric used to measure the similarity or distance between points in the feature space\n\n\ncluster_selection_method\nThis parameter determines how clusters are selected from the hierarchical tree generated during the clustering process. The ‘eom’ method focuses on finding clusters that persist over a significant range of cluster stability (i.e., clusters that have a lot of “excess of mass”). This tends to result in selecting larger, more prominent clusters and is generally more robust to noise. The alternative is ‘leaf’, which would consider all possible clusters at the leaves of the tree, often resulting in a larger number of smaller clusters.\n\n\nprediction_data\nWhen set to True, this parameter instructs HDBSCAN to generate additional data about the cluster hierarchy and the soft clustering assignments, which can be used for predicting the cluster memberships of new, unseen data. This is particularly useful if you intend to use the clustering model as part of a production pipeline where new documents need to be automatically assigned to the existing clusters/topics identified during the initial clustering.\n\n\n\n\nfrom hdbscan import HDBSCAN\nhdbscan_model = HDBSCAN(min_cluster_size = 7, metric = \"euclidean\", cluster_selection_method = 'eom', prediction_data = True)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (detailed)"
    ]
  },
  {
    "objectID": "Topic_modelling_bert_practise.html#step-5-representations",
    "href": "Topic_modelling_bert_practise.html#step-5-representations",
    "title": "Create the model (detailed)",
    "section": "",
    "text": "Now that we have the clusters of documents, all that’s left to do is identify the words that best represent the clusters. This is done by a representation model, basically this means that we apply different text mining techniques to the text of the documents to only show the most salient terms.\nIn this step we chose different elements, mainly:\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nstop_words\nWe don’t want any stopwords reperesenting our topics, we remove the stopwords for this reason. sklearn has build-in dictionaries per language. We set this option to “english” to remove english stop words\n\n\nmin_df\nStands for “Minimum document frequency”. It represents the minimum number of documents in which the term should appear. When a float is given, it will be interpreted as the proportion of documents\n\n\nngram_range\nHow many tokens a word can have (1 = “fuel”, 2 = “fuel cell”, 3 = “fuel cell system”). We provide this information as a range. If we want tokens of size 2 exclusively, we provide (2,2), if we want words between 1 and 3 tokens, we provide (1,3)\n\n\nMaximalMarginalRelevance\nThe goal of MMR is to select documents or pieces of information that are both relevant to a query and diverse from each other, thereby reducing redundancy in the results presented to a user. λ is a parameter (ranging from 0 to 1) that controls the balance between relevance and diversity. A higher λ gives more weight to relevance, while a lower λ emphasizes diversity.\n\n\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect_model = CountVectorizer(stop_words = \"english\", min_df = 2, ngram_range = (1,1)) \nfrom bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\nimport spacy\nkeybert_model = KeyBERTInspired()\nmmr_model = MaximalMarginalRelevance(diversity = 0.3)\n\nUsing this we create the representation model:\n\nrepresentation_model = {\n    \"keyBERT\": keybert_model,\n    \"MMR\": mmr_model,\n #  \"POS\": pos_model\n}\n\nThe final Bert model is the the group of all the provided models:\n\n# regroup all the models into one\ntopic_model = BERTopic(\n    embedding_model = embed_model,\n    umap_model = umap_model,\n    hdbscan_model = hdbscan_model,\n    vectorizer_model = vect_model,\n    representation_model = representation_model,\n    top_n_words = 20, # how many words to include in the representation\n    verbose = True, # this options ensures that the function returns some info while running\n    calculate_probabilities = True\n)\n\nBy default, each document is assigned to one topic. However, base on the text analysis of the topics, each document can get a probability distribution for all the topics. If you prefer to have probabilities for each document to be assigned to each topic, add the calculate_probabilities = True option to the model.\nYou are now ready to run the model!\nThe code for this is the following:\n\ntopics, probs = topic_model.fit_transform(data)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (detailed)"
    ]
  },
  {
    "objectID": "Text_networks.html",
    "href": "Text_networks.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "We’ll now turn our attention to methods for identifying which links to retain in order to create a network between terms/tokens in a corpus. To do this, we will present different types of indicators (direct and indirect) which either propose a textual network or a semantic network. A textual network shows the terms present in the corpus and links them with other terms present in the same unit of analysis (document, paragraph, sentence). This type of network shows us which words/tokens appear in close proximity. A semantic network identifies words that appear together with the same other words. The idea is that when two terms always appear with the same other terms, they have a close semantic meaning. In what follows, we’ll look at 6 indicators, 4 direct, 2 indirect. These indicators are automatically calculated in the free Cortext software. We’ll go into the details of how these indicators are calculated, to ensure a detailed understanding of their advantages and limitations, so that the analyst can choose the right indicator for the right use case. For those who don’t wish to deal with the technical side, the final section offers a summary table presenting the interpretation, advantages and limitations of each indicator.\nTo enable readers who wish to repeat the calculations to make sure they have mastered the formulas, we will take an example of 5 documents with short text to enable manual calculation and to illustrate things a little more simply. In the chapters on use cases, examples will be presented using larger corpora, but based on a basic understanding of the indicators as presented here.\nFor the remainder of this chapter, we’ll use the corpus shown in figure \\(\\ref{5_docs}\\). This corpus contains 5 documents of varying text length. In these texts, we’ve colored the words “tire” and “geometry”, as we’ll be using them in the examples to interpret the indicators.\nAll the indicators we present here are based in one way or another on , i.e. the number of times two terms appear together in the same unit of analysis. In what follows, we use the document as the unit of analysis, although it is possible to choose the chapter, paragraph or sentence as required.",
    "crumbs": [
      "Home",
      "Network Analysis",
      "Textual Networks"
    ]
  },
  {
    "objectID": "Text_networks.html#textual-networks",
    "href": "Text_networks.html#textual-networks",
    "title": "NetworkIsLife",
    "section": "",
    "text": "We’ll now turn our attention to methods for identifying which links to retain in order to create a network between terms/tokens in a corpus. To do this, we will present different types of indicators (direct and indirect) which either propose a textual network or a semantic network. A textual network shows the terms present in the corpus and links them with other terms present in the same unit of analysis (document, paragraph, sentence). This type of network shows us which words/tokens appear in close proximity. A semantic network identifies words that appear together with the same other words. The idea is that when two terms always appear with the same other terms, they have a close semantic meaning. In what follows, we’ll look at 6 indicators, 4 direct, 2 indirect. These indicators are automatically calculated in the free Cortext software. We’ll go into the details of how these indicators are calculated, to ensure a detailed understanding of their advantages and limitations, so that the analyst can choose the right indicator for the right use case. For those who don’t wish to deal with the technical side, the final section offers a summary table presenting the interpretation, advantages and limitations of each indicator.\nTo enable readers who wish to repeat the calculations to make sure they have mastered the formulas, we will take an example of 5 documents with short text to enable manual calculation and to illustrate things a little more simply. In the chapters on use cases, examples will be presented using larger corpora, but based on a basic understanding of the indicators as presented here.\nFor the remainder of this chapter, we’ll use the corpus shown in figure \\(\\ref{5_docs}\\). This corpus contains 5 documents of varying text length. In these texts, we’ve colored the words “tire” and “geometry”, as we’ll be using them in the examples to interpret the indicators.\nAll the indicators we present here are based in one way or another on , i.e. the number of times two terms appear together in the same unit of analysis. In what follows, we use the document as the unit of analysis, although it is possible to choose the chapter, paragraph or sentence as required.",
    "crumbs": [
      "Home",
      "Network Analysis",
      "Textual Networks"
    ]
  },
  {
    "objectID": "Text_networks.html#direct-indicators",
    "href": "Text_networks.html#direct-indicators",
    "title": "NetworkIsLife",
    "section": "Direct indicators",
    "text": "Direct indicators\n\nraw cooccurrences\nThe most basic technique for identifying co-occurrences is simply to count them. To do this, we produce a so-called co-occurrence matrix. This matrix contains, for all the words/tokens identified in the corpus, the number of times they appear in the same document with another term. In the case of our 5 documents, we have 63 words in all, and therefore a 63x63 matrix. Given the limited space on a page of this document, we’ll present a subset of this matrix. In the table \\(\\ref{co_occurrence_matrix}\\) we show the co-occurrences between 7 tokens of the 5 documents. In red, the number of co-occurrences between the term “geometry” and “tire”. According to the matrix, the terms are found together in 4 documents. This value is noted as \\(n(i,j) = n(tire, geometry) = 4\\). In the shaded cells we have the sum of the co-occurrences in the sample. This line is only there to clarify the calculation. In the sample (the submatrix shown here) tire has 13 co-occurrences with the 6 other terms in the sample, which corresponds to the sum in row or column. Overall in the corpus, the term “tire” has 105 co-occurrences and we have 1372 co-occurrences in all.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntire\nbody\ngeometry\nadjacent\nedge\naxial\nradius\nSomme exemple\nTotal corpus\n\n\n\n\ntire\n5\n2\n4\n1\n3\n1\n2\n13\n105\n\n\nbody\n2\n2\n2\n1\n1\n1\n0\n7\n53\n\n\ngeometry\n4\n2\n4\n1\n3\n1\n1\n12\n90\n\n\nadjacent\n1\n1\n1\n1\n1\n1\n0\n5\n39\n\n\nedge\n3\n1\n3\n1\n3\n1\n1\n10\n76\n\n\naxial\n1\n1\n1\n1\n1\n1\n0\n5\n39\n\n\nradius\n2\n0\n1\n0\n1\n0\n2\n4\n36\n\n\nSomme Exemple\n13\n7\n12\n5\n10\n5\n4\n56\n…\n\n\nTotal corpus\n105\n53\n90\n39\n76\n39\n36\n…\n1372\n\n\n\nTable 1: Extraction de la matrice des co-occurrences\nThe raw indicator transforms this matrix into a network with a weight on the links corresponding to the number of co-occurrences. In the \\(\\ref{nw_semantic_raw}\\) we represent this matrix as a network. On the left, all terms are present, while on the right, weak co-occurrences have been removed. By playing with the weights in this way, we can highlight the core of the subject. Here, the texts deal with tire geometry, which is at the heart of the network. More peripheral elements such as the tire body or radius are also visible.\n\nNetwork generated from the co-occurrence submatrix given in table \\(\\ref{co_occurrence_matrix}\\). The numbers on the links correspond to the matrix weights. The 2 between the terms “body” and “tire” indicates that these terms are present together in two documents.\nThese results are therefore relatively easy to interpret. The hypothesis is that the number of times two tokens appear together (in the same document, paragraph or sentence), gives information on the semantic sense between the tokens. This exercise can be replicated on the whole corpus, as in the network in figure \\(\\ref{nw_semantic_raw}\\). In this network, all identified tokens are represented by nodes. If a co-occurrence exists between two tokens, a link is visible; the thicker the link, the stronger the co-occurrence. As stated earlier in this chapter, the network is very dense, even if we can identify certain communities. Further processing is therefore required to analyze the structure of this type of network.\n\n\n\n\n\n\n\nRemark\n\n\n\nThe raw co-occurrences reflect the language elements of the corpus, highlighting mainly terms according to their frequency. This indicator is not able to highlight important terms with a lower frequency.\n\n\nThe limitations of this approach are, however, significant. Using raw counts runs the risk of highlighting mainly language-related elements, as well as the asymmetrical distribution of tokens in a given language. The fact that two terms often appear together does not imply that this association is of any interest for analysis.\nMore sophisticated indicators have therefore been developed to reduce the influence of co-occurrences, which are of little interest in highlighting co-occurrences that structure the corpus.\n\n\nThe Cramer indicator\nThis is an indicator developed by (Cramer and Nicewander 1979). It is an indicator that is automatically calculated in the Cortext Manager text analysis software.\nThis indicator aims to remove language bias. In a given language, certain words occur more often than others, creating high co-occurrences for these words, which are de facto highlighted to the detriment of words that have lower frequencies but may be of interest. To correct for these biases, raw co-occurrences will be compared with expected co-occurrences. In other words, if a term is frequent in the text, it will have more co-occurrences by definition. An infrequent term will have fewer co-occurrences. The number of co-occurrences we can expect can be estimated by a statistical calculation:\n\\[\\begin{equation}\ne(i,j) = \\frac{n(i) \\cdot n(j)}{n}\n\\end{equation}\\]\nIn this equation, \\(n(i)\\) corresponds to the number of co-occurrences of token \\(i\\) and \\(n\\) to the number of co-occurrences in the corpus as a whole. In our example, \\(n\\) amounts to summing the matrix and dividing by two (the matrix being symmetrical). \\(n(i)\\) means summing the row or column corresponding to \\(i\\). In extracting the matrix \\(n(tire) = 13\\), \\(n(geometry) = 12\\), \\(n=56\\). The calculation of this indicator only makes sense if we mobilize the values of the entire corpus, so we’ll use the bold values in the matrix:\n\\[\\begin{equation}\ne(tire,geometry) = \\frac{n(tire) \\cdot n(geometry)}{n} = \\frac{105 \\cdot 90}{2744} = \\frac{9450}{2744} = 3.44\n\\end{equation}\\]\nThe number of times we expect to see “tire” and “geometry” together in the same unit (document, paragraph, sentence) is 3.44. By calculating this value for the matrix endpoints, we obtain the following results:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntire\nbody\ngeometry\nadjacent\nedge\naxial\nradius\n\n\n\n\ntire\n4.02\n2.03\n3.44\n1.49\n2.91\n1.49\n1.38\n\n\nbody\n2.03\n1.02\n1.74\n0.75\n1.47\n0.75\n0.7\n\n\ngeometry\n3.44\n1.74\n2.95\n1.28\n2.49\n1.28\n1.18\n\n\nadjacent\n1.49\n0.75\n1.28\n0.55\n1.08\n0.55\n0.51\n\n\nedge\n2.91\n1.47\n2.49\n1.08\n2.1\n1.08\n1\n\n\naxial\n1.49\n0.75\n1.28\n0.55\n1.08\n0.55\n0.51\n\n\nradius\n1.38\n0.7\n1.18\n0.51\n1\n0.51\n0.47\n\n\n\nTable 1: Matrice des valeurs attendues pour les 5 documents du corpus;\nThe second step in the formula is then to compare this expected (or hoped-for) number with the observed number. The latter is the number of co-occurrences found in the co-occurrence matrix:\n\\[\\begin{eqnarray}\n\\frac{n(i,j) - e(i,j)}{e(i,j)} &=& \\frac{n(i,j)}{e(i,j)}-1\\\\\n\\frac{4}{3.44} - 1 &=& 0.16\n\\end{eqnarray}\\]\nThe result shows that the number of co-occurrences is higher than expected (result \\(&gt;0\\)). In fact, the number is 16% higher than expected. Conversely, the same calculation for the “pull” and “adjacent” terms gives a negative result: \\(-0.33\\), implying here that the number is 67% lower than expected. The set of values for the matrix is represented in table \\(\\ref{table_cramer}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntire\nbody\ngeometry\nadjacent\nedge\naxial\nradius\n\n\n\n\ntire\n-1\n-0.01\n0.16\n-0.33\n0.03\n-0.33\n0.45\n\n\nbody\n-0.01\n-1\n0.15\n0.33\n-0.32\n0.33\n-1\n\n\ngeometry\n0.16\n0.15\n-1\n-0.22\n0.2\n-0.22\n-0.15\n\n\nadjacent\n-0.33\n0.33\n-0.22\n-1\n-0.07\n0.8\n-1\n\n\nedge\n0.03\n-0.32\n0.2\n-0.07\n-1\n-0.07\n0\n\n\naxial\n-0.33\n0.33\n-0.22\n0.8\n-0.07\n-1\n-1\n\n\nradius\n0.45\n-1\n-0.15\n-1\n0\n-1\n-1\n\n\n\nTable 1: Matrice des valeurs de Cramer pour les 5 documents du corpus.\nWe then assume that we want to keep only those co-occurrences that deviate from the expected value, and especially those that exceed the expected value. Other co-occurrences will be discarded. This gives us an automatic filter that facilitates network processing. Figure \\(\\ref{Semantique_complet_cramer}\\) shows the network linked to the Cramer scores. When we compare this network with the one in figure \\(\\ref{nw_semantique_raw}\\) we see that the Cramer network is less dense and shows more clearly defined communities. We’ve retained all links with a value &gt; 0. In this network, a link between two terms indicates that these terms appear together more than expected. It’s a kind of deviation from the mean.\n\nNetwork connecting terms linked by a positive value according to the Cramer indicator. Colors correspond to community detection performed by modularity maximization. Note: no cleaning has been carried out on the words retained, as efficient cleaning normally removes words such as “has”, “is”, “of” etc.\n\n\n\n\n\n\nRemark\n\n\n\nThe Cramer indicator identifies deviations from randomness and is thus able to identify important terms with low frequencies. In addition, the value of the indicator can be interpreted.\n\n\n\n\n\\(\\chi^2\\)\nThe \\(\\chi^2\\) indicator is well known in statistics, where it is commonly used to identify whether a deviation from an expected value is statistically significant. In this sense, the indicator is close to Cramer’s. The difference is in the denominator, where the root function is applied.\n\\[\\begin{equation}\n\\chi^2(i,j) = \\frac{n(i,j) - e(i,j)}{{sqrt{e(i,j)}}\\\\}\n\\end{equation}\\]\nA value below what’s expected will give rise to negative values, values above what’s expected will give rise to positive values, close to 0 we’re in the expected zone. The results are in the \\(\\ref{Matrice_chi2}\\) table. The higher the absolute value, the more it deviates from what’s expected.\n\n“Adjacent” and “Axial” have the highest positive score, implying that these two terms are found together more often in the same document than expected, so their association should be retained.\n“Radius” and “body” have the lowest negative score, implying that their association is well below what’s expected. Although we won’t retain this link, it is nonetheless interesting for other applications (identification of white zones, for example).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntire\nbody\ngeometry\nadjacent\nedge\naxial\nradius\n\n\n\n\ntire\n0\n-0.02\n0.3\n-0.403\n0.054\n-0.403\n0.53\n\n\nbody\n-0.02\n0\n0.198\n0.284\n-0.386\n0.284\n-0.834\n\n\ngeometry\n0.3\n0.198\n0\n-0.247\n0.321\n-0.247\n-0.166\n\n\nadjacent\n-0.403\n0.284\n-0.247\n0\n-0.077\n0.599\n-0.715\n\n\nedge\n0.054\n-0.386\n0.321\n-0.077\n0\n-0.077\n0.003\n\n\naxial\n-0.403\n0.284\n-0.247\n0.599\n-0.077\n0\n-0.715\n\n\nradius\n0.53\n-0.834\n-0.166\n-0.715\n0.003\n-0.715\n0\n\n\n\nTable 1: Scores \\(\\chi^2\\) pour les 5 documents du corpus.\nThe values are close to Cramer’s, as shown in the matrix \\(\\ref{chi2_cramer}\\). The highest and lowest values are the same. The only difference we observe is a smaller gap between values, with higher values for positive values and lower values for negative values. This is simply due to a smaller denominator than in the Cramer case. Smoothing with the root function reduces the gap between values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntire\nbody\ngeometry\nadjacent\nedge\naxial\nradius\n\n\n\n\ntire\n0/-1\n-0.02/-0.01\n0.3/0.16\n-0.403/-0.33}\n0.054/0.03}\n-0.403/-0.33\n0.53/0.45\n\n\nbody\n-0.02/-0.01\n0/-1\n0.198/0.15\n0.284/0.33}\n-0.386/-0.32}\n0.284/0.33\n-0.834}/-1\n\n\ngeometry\n0.3/0.16\n0.198/0.15\n0/-1\n-0.247/-0.22\n0.321/0.2\n-0.247/-0.22\n-0.166/-0.15\n\n\nadjacent\n-0.403/-0.33\n0.284/0.33\n-0.247/-0.22\n0/-1\n-0.077/-0.07\n0.599}/0.8\n-0.715/-1\n\n\nedge\n0.054/0.03\n-0.386/-0.32\n0.321/0.2\n-0.077/-0.07\n0/-1\n-0.077/-0.07\n0.003/0\n\n\naxial\n-0.403/-0.33\n0.284/0.33\n-0.247/-0.22\n0.599/0.8\n-0.077/-0.07\n0/-1\n-0.715/-1\n\n\nradius\n0.53/0.45\n-0.834/-1\n-0.166/-0.15\n-0.715/-1\n0.003/0\n-0.715/-1\n0/-1\n\n\n\nTable 1: Matrice contenant en bleu les scores de Cramer, en noir les scores \\(\\chi^2\\). En rouge les valeurs du \\(\\chi^2\\) les plus élevées.\nSince the difference is mainly a size effect, the network is identical to the network based on the Cramer indicator. Figure \\(\\ref{nw_semantic_chi2}\\) shows the network with the identical structure, apart from a few nodes that have changed place. The real difference comes into play when we try to identify a threshold for retaining or not retaining a link. In the network shown in figure all links with a positive value are retained. We can decide to retain only links with a \\(\\chi^2\\) greater than a certain threshold.\n\nTextual network based on the \\(\\chi^2\\) indicator. The structure of this network is equivalent to that produced by the Cramer indicator. Only a few nodes have changed position, but this is due solely to the spatialization algorithm.\n\n\n\n\n\n\nRemark\n\n\n\nThe \\(chi^2\\) identifies deviations from randomness and is thus able to identify important terms with low frequencies. The denominator accentuates deviations between values compared to Cramer.\n\n\n\n\nMutual Information\nA final indicator is Mutual information, which is characterized by the following formula: \\\n\\[\\begin{equation}\nI_{i,j} = log(\\frac{n(i,j)}{e(i,j)})    \n\\end{equation}\\]\nMore than a deviation from an observed value (like \\(\\chi^2\\) or the Cramer indicator), Mutual Information identifies a multiplication. In other words, by how many times the observed number is greater than the expected number. The \\(log(\\cdot)\\) function applied here initially transforms a value less than 1 ( value less than expected) into a negative value, while smoothing out high values. Figure \\(\\ref{information_criterion}\\) illustrates this principle. A value \\(&gt; 1\\) implies that the observed value is higher than expected and will receive a positive value. The log function reduces high values and in so doing also reduces the gaps between values.\n\nApplying this indicator to our sample, we obtain the matrix in table \\(\\ref{nw_semantic_MI}\\). We notice in this matrix the presence of “-Inf” values corresponding to \\(-\\infty\\). This value emerges when we divide by \\(0\\), which is the case here for some values. We find negative values corresponding to values lower than expected, and positive values corresponding to values higher than expected. Given the nature of the formula, this indicator gives a negative value to the same values as \\(\\chi^2\\) and \\(Cramer\\). In a first approach, we’ll find the same network structure, since the same links (with negative values) will be removed from the network.\nFigure \\(\\ref{information_criterion}\\) shows the network based on the mutual information indicator. Once again, this network has the same structure as the previous two. The advantage of mutual information lies in the inclusion of the log function, which smoothes high values and limits the impact of high values.\n\n\n\n\n\n\nRemark\n\n\n\nThe Mutual Information indicator smooths out strong co-occurrences, thereby reducing the impact of generic terms.\n\n\n Textual network based on the “Mutual Information” indicator. Once again, we find the same structure as for the other two indicators.\n\n\nComparing the indicators\nIn the previous subsections, we showed how to calculate three indicators to identify links for a text network. We have seen that the networks extracted are identical even if the weights on the links change. To better understand the difference between these indicators and which indicator to choose in which case, let’s take a closer look at these three indicators and what makes them different. First, let’s focus on the formulas to identify what the real difference is in terms of values. To do this, we’ll make the following calculation. We’d like to know by how much we need to multiply one indicator to obtain another. For example, for \\(\\chi^2\\) and Cramer, by how much must we multiply \\(\\chi^2\\) to obtain Cramer’s score:\n\\[\\begin{equation}\n\\chi^2 \\cdot factor = Cramer    \n\\end{equation}\\]\nBy doing this exercise for the three indicators, we obtain the results in the matrix \\(\\ref{indicators_comparison_table}\\). In this matrix, we can read that we need to multiply Cramer by \\(\\sqrt{e}\\) to obtain \\(\\chi^2\\), and multiply \\(\\chi^2\\) by \\(\\frac{1}{\\sqrt{e}}\\) to obtain Cramer.\n\n\n\n\n\n\n\n\n\n\nIM\n\\(\\chi^2\\)\nCramer\n\n\n\n\nIM\n1\n\\(\\frac{n-e}{\\log\\left(\\frac{n}{e}\\right)\\sqrt{e}}\\)\n\\(\\frac{n-e}{\\log\\left(\\frac{n}{e}\\right)e}\\)\n\n\n\\(\\chi^2\\)\n\\(\\log\\left(\\frac{n}{e}\\right) \\cdot \\frac{\\sqrt{e}}{n-e}\\)\n1\n\\(\\frac{1}{\\sqrt{e}}\\)\n\n\nCramer\n\\(\\log\\left(\\frac{n}{e}\\right) \\cdot \\frac{e}{n-e}\\)\n\\(\\sqrt{e}\\)\n1\n\n\n\nTable 1: Cette matrice regroupe les facteurs multiplicateurs pour passer d’un indicateur à un autre. Le sens de lecture est : il faut multiplier Cramer par \\(\\sqrt{e}\\) pour obtenir \\(\\chi^2\\).\nAll the factors of this matrix are positive. \\(log(\\frac{n-e}{log(\\frac{n}{e})e}\\) is positive because if we have \\((n-e) &lt; 0\\) then \\(log(\\frac{n}{e}) &lt; 0\\) the result is positive. This shows that there is no sign change between the different indicators. By removing the links with negative weights, we remove the same links with these three indicators.\nNext, let’s look at the level effect associated with the different indicators. Even if we keep the same sign, the scores differ because of the multipliers in the matrix.\nLet’s start by comparing \\(\\chi^2\\) and Cramer. The only difference between these two indicators is the numerator. In both formulas, the numerator is \\(n(i,j)-e(i,j)\\), so the indicators seek to measure a deviation from an expected value. This value is then divided by a different value depending on the indicator. In figure \\(\\ref{compare_chi_cramer}\\) these two numerators are compared. The diagonal line represents the Cramer denominator, the curve the \\(\\chi^2\\) denominator. The difference between the two functions shows that the higher \\(e(i,j)\\) is, the greater the difference between the two denominators. This implies that the higher the expected value for a co-occurrence, the more Cramer penalizes the score (\\(e(i,j)\\) being in the denominator, so we divide by a higher and higher value). In the case of \\(\\chi^2\\), this value is much lower, so the scores are higher. So for the same value of \\(e(i,j)\\), Cramer penalizes more than the \\(\\chi^2\\), and this penalty is all the stronger as \\(e(i,j)\\) increases.\n\nWe can see from this first part of the analysis that Cramer’s indicator reduces high expected values ( terms with high co-occurrences) to lower values in a more pronounced way than \\(\\chi^2\\). A more important point is the marginal effect for each function. The marginal effect is constant for Cramer (\\(\\frac{\\delta e(i,j)}{\\delta e} = 1\\)), so when we increase \\(e\\) by one unit, we increase the denominator by one unit too. In the case of \\(\\chi^2\\) we have a decreasing marginal effect (\\(\\frac{\\delta\"\\sqrt{e(i,j)}}{\\delta \"e} = -\\frac{1}{4}e^{3/4}\\)). This implies that for each increase in \\(e\\), \\(\\sqrt{e(i,j)}\\) increases, but the increase is smaller and smaller. This means that the difference between the \\(\\chi^2\\) values becomes smaller and smaller, while the Cramer values remain constant. In an analysis, \\(\\chi^2\\) therefore brings fewer distinctions on strong expected values, whereas Cramer remains more “distinguishing” &lt;- find a word here, it’s clearly not that, but I can’t find it.",
    "crumbs": [
      "Home",
      "Network Analysis",
      "Textual Networks"
    ]
  },
  {
    "objectID": "Text_networks.html#indirect-indicators",
    "href": "Text_networks.html#indirect-indicators",
    "title": "NetworkIsLife",
    "section": "Indirect indicators",
    "text": "Indirect indicators\nA second family of indicators are indirect indicators, meaning that what is taken into account are not direct co-occurrences between two words, but rather words that appear together with a third term. In other words, two terms are close if we often find them together with other terms. The weight we’re going to put on the link between two terms is therefore no longer a transformation of the number of co-occurrences, but a function of the number of times the co-occurrences are with another term. This type of indicator therefore compares all co-occurrences of a term with all co-occurrences of another term, and only increases if a third term makes the link. As shown in figure , it’s indirect links that will increase this indicator, not direct links. The links between “geometry” and “body” don’t play a role, but the fact that “tire” and “body” appear together with the term “geometry” indicates a certain proximity between the terms. This is what interests us in this type of indicator. For each word, we’ll compare and sum these indirect links to measure the proximity between two terms. In this way, we’re closer to what we might call a semantic link, since we’re taking into account all the words that appear in proximity to another term. As for the co-occurrence matrices we’ve used so far, we’re going to use them line by line rather than cell by cell.\n\nIn what follows, we present two indirect indicators: cosine and distributional.\n\nCosine\nThe first indicator we’ll be looking at here is cosine, a well-known and widely used indicator in scientometrics (Eck and Waltman 2009) and economics (Jaffe 1986), (Engelsman and Raan 1991), (Breschi, Lissoni, and Malerba 2003). In a later chapter, we’ll show how we use this same indicator to understand technological proximity based on technological classifications. In this chapter, we focus on the application to textual analysis. Let’s go back to the co-occurrence matrix, which we recall here for fluency:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntire\nbody\ngeometry\nadjacent\nedge\naxial\nradius\nSomme exemple\nTotal corpus\n\n\n\n\ntire\n5\n2\n4\n1\n3\n1\n2\n13\n105\n\n\nbody\n2\n2\n2\n1\n1\n1\n0\n7\n53\n\n\ngeometry\n4\n2\n4\n1\n3\n1\n1\n12\n90\n\n\nadjacent\n1\n1\n1\n1\n1\n1\n0\n5\n39\n\n\nedge\n3\n1\n3\n1\n3\n1\n1\n10\n76\n\n\naxial\n1\n1\n1\n1\n1\n1\n0\n5\n39\n\n\nradius\n2\n0\n1\n0\n1\n0\n2\n4\n36\n\n\nSomme Exemple\n13\n7\n12\n5\n10\n5\n4\n56\n\\(\\hdots\\)\n\n\nTotal corpus\n105\n53\n90\n39\n76\n39\n36\n\\(\\hdots\\)\n1372\n\n\n\nTable 2: Extraction de la matrice des co-occurrences\nDirect measurements compared observed co-occurrences with expected co-occurrences, so we were interested only in the individual cells of each matrix. The idea behind an indirect measure is to say that two terms are close because they appear together with other terms. In this sense, we’re more interested in semantic proximity: two terms that are surrounded by the same terms have a high probability of having the same meaning. To put this idea into practice, we need to compare all indirect co-occurrences between two terms. In other words, we need to identify with which other words each word co-occurs. If we want to know whether the word “tire” is close to the word “body”, we’ll have to check whether both are linked with “geometry”, “adjacent”, “edge”, “axial”, “radius” and so on for all the terms in the corpus. The reasoning and calculation will therefore mobilize vectors to facilitate the calculation of indicators. In what follows, we’ll move straight on to explaining the calculation. For a more intuitive explanation of this indicator, you can read the chapter on technological proximity, which explains the construction of the indicator step by step and explains the intuition behind each step.\nThe formula we’ll use here is\n\n\\[\\begin{equation}\n    cos(\\theta) = \\frac{\\sum_{k \\neq i,j}^{} n(i,k) \\cdot n(j, k)}{\\sqrt{\\sum_{k \\neq i,j}^{} n(i,k)^2}\\cdot \\sqrt{\\sum_{k \\neq i,j}^{} n(j,k)^2}}\n\\end{equation}\\]\nThe denominator of this indicator counts the number of times two terms appear together with a third term. \\(n(i,k)\\) represents the frequency with which terms \\(i\\) and \\(j\\) appear together and \\(n(j,k)\\) the frequency for terms \\(j\\) and \\(k\\). Multiplication implies that if there is no direct link between \\(i\\) (or \\(j\\)) and \\(k\\), the product will be zero, so the value of the numerator does not increase. Note that even if a direct link exists ( \\(n(i,j \\neq 0\\)) this does not increase the numerator.\nLet’s take the example of the terms “tire” and “body” (but let’s stay within the framework of the restricted matrix so as not to fill entire pages with lines of calculation):\n\\[\\begin{equation}\n\\begin{split}\n\\sum_{k=1}^{k} cooc_{ik} \\cdot cooc_{jk} & = \\sum_{k=1}^{5} cooc_{tire,k} \\cdot cooc_{body, k}\\\\\n& = (Cooc_{tire - geometry} \\cdot Cooc_{body - geometry})\\\\\n& + (Cooc_{tire - adjacent} \\cdot Cooc_{body - adjacent})\\\\\n& + (Cooc_{tire - edge} \\cdot Cooc_{body - edge}) \\\\\n& + (Cooc_{tire - axial} \\cdot Cooc_{body - axial}) \\\\\n& + (Cooc_{tire - radius} \\cdot Cooc_{body - radius}) \\\\\n& = (4 \\cdot 2) + (1 \\cdot 1) + (3 \\cdot 1) + (1 \\cdot 1) + (2 \\cdot 0)\\\\& = 13\n\\end{split}\n\\end{equation}\\]\nThere is no observed link between “radius” and “body”, so the product is zero. It becomes clear here that manual calculation is relatively time-consuming, especially if we have to do this for a corpus with thousands of words.\nTo calculate the final proximity between two CIB codes, divide by the norm of the two codes. Following on from our example :\n\\[\\begin{equation}\n    \\begin{split}\n        cos(\\theta) & = \\frac{\\sum_{k=1}^{5} cooc_{tire,k} \\cdot cooc_{body, k}}{\\sqrt{\\sum_{k=1}^{5} cooc_{tire,k}^2}\\cdot \\sqrt{\\sum_{k=1}^{5} cooc_{body, k}^2}}\\\\       \n                    & = \\frac{1}{\\sqrt{\\sum_{k=1}^{5} cooc_{tire,k}^2}\\cdot \\sqrt{\\sum_{k=1}^{5} cooc_{body, k}^2}}\\\\\n                    & =  \\frac{1}{\\sqrt{4^2 + 1^2 + 3^2 + 1^2 + 2^2}\\cdot \\sqrt{2^2+1^2+1^2+1^2+0^2}}\\\\\n                    & = \\frac{13}{\\sqrt{31}\\cdot \\sqrt{7}}\\\\\n                    & = 0.88\n    \\end{split}\n    \\label{proxi_nominateur2}\n\\end{equation}\\]\nThere’s a point of caution here: in the classic cosine measure, the sum excludes only the \\(i\\) term and keeps \\(j\\). A comparison with a software program may give a slightly different result following this modification of the formula: \\(\\sqrt{\\sum_{k \\neq i,\\textcolor{red}{j}} cooc(k,j)^2}\\).\\\\}\nThe values for the submatrix we’ve been following since the beginning can be found in the table \\(\\ref{cosinus_matrix}\\). We note here the symmetry of the matrix, as with the other indicators. The higher the score, the closer the two terms are. In the case of indicator \\(\\in [0,1]\\), we have perfect proximity between the terms “axial” and “adjacent”. This implies that the two terms appear together with exactly the same terms throughout the corpus. In no case does this imply that the terms appear together, as the score does not allow us to infer on this question. If a term appears with no other terms, the score will be \\(0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntire\nbody\ngeometry\nadjacent\nedge\naxial\nradius\n\n\n\n\ntire\n0\n0.882\n0.975\n0.857\n0.979\n0.857\n0.889\n\n\nbody\n0.882\n0\n0.929\n0.949\n0.966\n0.949\n0.862\n\n\ngeometry\n0.975\n0.929\n0\n0.898\n0.983\n0.898\n0.884\n\n\nadjacent\n0.857\n0.949\n0.898\n0\n0.873\n1\n0.73\n\n\nedge\n0.979\n0.966\n0.983\n0.873\n0\n0.873\n0.878\n\n\naxial\n0.857\n0.949\n0.898\n1\n0.873\n0\n0.73\n\n\nradius\n0.889\n0.862\n0.884\n0.73\n0.878\n0.73\n0\n\n\n\nTable 3: Table avec les scores cosinus\nIn figure \\(\\ref{cosinus_matrix}\\) we represent the network based on these scores for the 5 documents in our test corpus. The structure of this network is different from that of the direct indicators. We don’t have any negative scores, so we have many more links, resulting in a much denser network. To take advantage of this type of network, we need to filter the scores to keep only the terms with a high score.\n\nIn figure \\(\\ref{illu_liens_subset}\\) we show two examples, one where we’ve kept only links with a score &gt; 0.75 and one where we’ve kept links with a score &gt; 0.875. In our example we have words that are very close together, so the distinction is not obvious, but as we increase the filter we see that two components start to stand out, indicating that these contain terms that are not related to the rest of the corpus, or at least to a lesser extent.\n\n\n\nDistributional\nThe cosine indicator is based on raw co-occurrences between terms. Like cosine, the “Distributional” indicator aims to identify semantic meaning between terms. Instead of relying on raw counts, “Distributional” bases its calculation on the “Mutual Information” indicator. The indicator was proposed by and takes the following formula:\n\\[\\begin{equation}\n    \\frac{\\sum_{k\\neq i,j, I(i,k) &gt; 0}min(I(i,k),I(j,k)))}{\\sum_{k\\neq i,j, I(i,k) &gt; 0} I(i,k)}\n\\end{equation}\\]\nAs with cosine, this indicator targets indirect links. We thus find calculations based on \\(I(i,k)\\) and \\(I(j,k)\\), where \\(k\\) is the term in common. When two terms have a third in common, we take the minimum between the weights. The indicator is then normalized by the sum of the weights (in this case, mutual information).\nTo illustrate the calculation, we calculate the score for the terms “tire” and “geometry” in figure \\(\\ref{illu_lien_indirecte}\\). In the figure, the scores correspond to the mutual information scores.\n\nIllustration of the “distributional” indicator principle. The weights on the links correspond to the “Mutual information” indicator of the \\(\\ref{nw_semantic_MI}\\) matrix. Here, we seek to calculate the value for the terms “tire” and “geometry”. The network shows that the two terms have two others in common: “body” and “edge”.\nThe numerator is calculated by summing the minimum values for each triptych, keeping only the positive values. As a reminder, a negative value in mutual information implies a co-occurrence above the expected, to identify a semantic meaning we only wish to keep the co-occurrences which are more important than expected. In our example we therefore obtain:\n\\[\\begin{eqnarray}\nDistr(tire, geometry) &= \\frac{min(0.014) + min(0.031, 0.185)}{0.031} = 5.516\\\\\nDistr(geometry, tire) &= \\frac{min(0.14) + min(0.031, 0.185)}{0.14 + 0.185} = 0.5261\n\\end{eqnarray}\\]\nNote that because of the formulation of the denominator, the scores are asymmetric, implying that the matrix containing the values will be asymmetric as well. The matrix is presented in the table \\(\\ref{distrib_matrix}\\). The higher the score, the stronger the proximity. A score of 0 implies that the two terms do not appear together with a third term. As the score increases, the number of triads increases and the IM scores become more similar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntire\nbody\ngeometry\nadjacent\nedge\naxial\nradius\n\n\n\n\ntire\n0\n0.93\n1\n0\n0.29\n0\n0.1\n\n\nbody\n1\n0\n0\n1\n1\n1\n0\n\n\ngeometry\n0.17\n0\n0\n1\n0.21\n1\n0.46\n\n\nadjacent\n0\n0.48\n0.49\n0\n0\n1\n0\n\n\nedge\n0.81\n0.76\n1\n0\n0\n0\n1\n\n\naxial\n0\n0.48\n0.49\n1\n0\n0\n0\n\n\nradius\n1\n0\n0.41\n0\n0.08\n0\n0\n\n\n\nTable 1: Distributional scores for 7 identified terms. We note that this indicator is the only one to be asymmetrical. This means that the network we can create from this indicator will be directed, given that the weight of the link \\(a \\rightarrow b \\neq b \\rightarrow a\\).\nIn figure \\(\\ref{distr_illustration_nw}\\) we illustrate the network resulting from this indicator. As with the cosine-based network, it’s useful to use thresholds to highlight the underlying structure. In the example we have filtered out links below 0.7 and 0.94. The more we filter, the more the network structure breaks down and we see components emerge. In our example, the aim of which was to be able to calculate indicators by hand for educational purposes, we have very few documents and therefore a very dense network. With filtering at 0.94 we removed only 52% of the links. In a larger corpus, filtering should have a greater impact.\n\n\n\nConclusions and recap of the interpretations\nWe have presented a number of indicators available in the free Cortext software. There are, of course, many more indicators available for producing textual and semantic networks, but we’re focusing here on a solution that’s easy to access and free of charge, so as to spread the practice as widely as possible.\nIn the table \\(\\ref{tableau_recap_indicateurs}\\) we summarize all the indicators processed, their interpretation and their advantages and limitations.\n\nThis table summarizes the use of all the indicators presented. The nature of the indicator depends on the type of link analyzed: direct, meaning that we’re looking at terms that appear together; indirect, meaning that we’re looking at terms that appear together with a third term. We make a distinction here between textual and semantic, as the interpretation of the results is not the same.\nThe points to remember when dealing with a textual or semantic network are, firstly, to identify whether the indicator is based on direct or indirect measurement and, secondly, to deduce whether we are dealing with a textual or semantic analysis. A textual analysis shows the terms in the corpus, but nothing more. A semantic analysis highlights terms by their meaning. In this chapter (and in cortext), this is approached by means of indirect indicators, which include in the calculation a comparison between all the terms in the corpus.\nThe aim here is not to specify that one indicator is better than another, as this will depend on the use case. The most important thing is to understand how these indicators are calculated or, failing that, to be aware of their limitations, as presented in this table, in order to make a coherent choice of indicator for the chosen use case.",
    "crumbs": [
      "Home",
      "Network Analysis",
      "Textual Networks"
    ]
  },
  {
    "objectID": "Topic_modelling_bertopic_nodetails.html",
    "href": "Topic_modelling_bertopic_nodetails.html",
    "title": "Create the model (TL;DR version)",
    "section": "",
    "text": "We start by loading required packages\n\nimport pandas as pd\nfrom bertopic import BERTopic\nfrom sentence_transformers import SentenceTransformer\nfrom umap import UMAP\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\nimport spacy\nfrom hdbscan import HDBSCAN\n\n\n\n\ndata = pd.read_csv(\"entr_eco_publications.csv\", sep = \";\")\n\nThe input data for bertopic needs to have a specific format: it need to be a list. We therefore transform our dataframe into a list. In addition, we only need the column that has the text, in our case the “abstract” column.\n\n# We first select only the column with the text:\ndata = data['abstract']\n# We transform the dataframe into a list:\ndata = data.apply(lambda row: ' '.join(row.values.astype(str)),axis=1).tolist() \n\n\n\n\n\n# Step 1: create the embeddings\nembed_model = SentenceTransformer(\"all-MiniLM-L6-V2\")\n# Step 2: create the dimension reduction model\numap_model = UMAP(n_neighbors = 30, n_components = 5, min_dist = 0.0, metric = \"cosine\", random_state = 42)\n# Step 3: create the clustering model\nhdbscan_model = HDBSCAN(min_cluster_size = 7, metric = \"euclidean\", cluster_selection_method = 'eom', prediction_data = True)\n# Step 4: create the representation model\nvect_model = CountVectorizer(stop_words = \"english\", min_df = 2, ngram_range = (1,1)) \nkeybert_model = KeyBERTInspired()\nmmr_model = MaximalMarginalRelevance(diversity = 0.3)\nrepresentation_model = {\n    \"keyBERT\": keybert_model,\n    \"MMR\": mmr_model,\n #  \"POS\": pos_model\n}\n\nNow we regroup all the models into one which will be our topic model:\n\n# regroup all the models into one\ntopic_model = BERTopic(\n    embedding_model = embed_model,\n    umap_model = umap_model,\n    hdbscan_model = hdbscan_model,\n    vectorizer_model = vect_model,\n    representation_model = representation_model,\n    top_n_words = 20, # how many words to include in the representation\n    verbose = True # this options ensures that the function returns some info while running\n)\n\n\n\n\nWe can now use this model to extract the topics from the data, we do this with the following command:\n\ntopics, probs = topic_model.fit_transform(data)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (TL;DR version)"
    ]
  },
  {
    "objectID": "Topic_modelling_bertopic_nodetails.html#step-1-loading-the-data",
    "href": "Topic_modelling_bertopic_nodetails.html#step-1-loading-the-data",
    "title": "Create the model (TL;DR version)",
    "section": "",
    "text": "data = pd.read_csv(\"entr_eco_publications.csv\", sep = \";\")\n\nThe input data for bertopic needs to have a specific format: it need to be a list. We therefore transform our dataframe into a list. In addition, we only need the column that has the text, in our case the “abstract” column.\n\n# We first select only the column with the text:\ndata = data['abstract']\n# We transform the dataframe into a list:\ndata = data.apply(lambda row: ' '.join(row.values.astype(str)),axis=1).tolist()",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (TL;DR version)"
    ]
  },
  {
    "objectID": "Topic_modelling_bertopic_nodetails.html#step-2-build-the-topic-model",
    "href": "Topic_modelling_bertopic_nodetails.html#step-2-build-the-topic-model",
    "title": "Create the model (TL;DR version)",
    "section": "",
    "text": "# Step 1: create the embeddings\nembed_model = SentenceTransformer(\"all-MiniLM-L6-V2\")\n# Step 2: create the dimension reduction model\numap_model = UMAP(n_neighbors = 30, n_components = 5, min_dist = 0.0, metric = \"cosine\", random_state = 42)\n# Step 3: create the clustering model\nhdbscan_model = HDBSCAN(min_cluster_size = 7, metric = \"euclidean\", cluster_selection_method = 'eom', prediction_data = True)\n# Step 4: create the representation model\nvect_model = CountVectorizer(stop_words = \"english\", min_df = 2, ngram_range = (1,1)) \nkeybert_model = KeyBERTInspired()\nmmr_model = MaximalMarginalRelevance(diversity = 0.3)\nrepresentation_model = {\n    \"keyBERT\": keybert_model,\n    \"MMR\": mmr_model,\n #  \"POS\": pos_model\n}\n\nNow we regroup all the models into one which will be our topic model:\n\n# regroup all the models into one\ntopic_model = BERTopic(\n    embedding_model = embed_model,\n    umap_model = umap_model,\n    hdbscan_model = hdbscan_model,\n    vectorizer_model = vect_model,\n    representation_model = representation_model,\n    top_n_words = 20, # how many words to include in the representation\n    verbose = True # this options ensures that the function returns some info while running\n)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (TL;DR version)"
    ]
  },
  {
    "objectID": "Topic_modelling_bertopic_nodetails.html#step-3-run-the-topic-model",
    "href": "Topic_modelling_bertopic_nodetails.html#step-3-run-the-topic-model",
    "title": "Create the model (TL;DR version)",
    "section": "",
    "text": "We can now use this model to extract the topics from the data, we do this with the following command:\n\ntopics, probs = topic_model.fit_transform(data)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Create the model (TL;DR version)"
    ]
  },
  {
    "objectID": "2_scripts.html",
    "href": "2_scripts.html",
    "title": "",
    "section": "",
    "text": "#Useful scripts and where to find them"
  },
  {
    "objectID": "feature_extraction.html",
    "href": "feature_extraction.html",
    "title": "Feature Extraction",
    "section": "",
    "text": "Feature Extraction\n80% of the data at our disposal is textual and often unstructured (web, press, reports, internal notes). This data contains high value-added information for decision-making. Given the amount of information the text contains, it is important to know what is worth reading and what can be put aside (fayyad1996knowledge?). Thus, the text requires particular techniques which aim to extract the salient elements of the text in order to be able to transform the text into intelligence. The objective of this training is to introduce the basic notions associated with textual and semantic analysis and present some use cases linked to sustainability.\nThe learning goals for this chapter are:\n\nKnow the vocabulary associated with textual and semantic analysis\nKnow how to differentiate between textual and semantic analysis\nUnderstand and implement the fundamental steps involved in preparing the text before analysis\nUnderstand the importance of preparing the text for analysis\nUnderstand the importance of an ontology\nKnow some applied cases for the study of sustainability\n\nThe difficulty associated with textual analysis is the identification of terms with high informational value contained in a corpus of documents. The raw text contains both terms which have value for the analysis we wish to make of them and terms which do not provide any particular information. The first step in textual analysis is the identification of terms that have value for the question the analyst seeks to address. In itself, extraction is a complex task, but there is software that more or less automate these steps (Gargantext, Cortext, Vantagepoint). However, the informational value of terms changes depending on a question asked. For example, identifying the population’s feeling towards a technology does not use the same terms as an analysis of the technological positioning of players in a field. It is therefore important to be able to intervene in the identification of terms to be able to guide the analysis for maximum impact on decision-making.\n\n\n\n\n\n\nFigure 1: 5 Short texts\n\n\n\nLet’s take the example of the three texts in Figure 1 . Three claims from two patents are represented. For an analysis that focuses on technology, we are mainly interested in the terms “reduced cross-section”, “edge geometry”, “thickness of the main body geometry” etc. We are not interested in elements such as “according to”, “claims”, “also includes”. We are therefore looking for an efficient method to identify the technical terms present in these patent documents.\nIn other cases, such as press data, we could be interested in events (conferences, congresses, symposiums) and the actors present, we then look for information of a different nature, requiring different extraction methods.\nRegardless of the question or the nature of the data sources, there are methods and indicators that allow us to zoom in on the information that we consider to have informational value. These algorithms and methods are based on hypotheses in information science in order to scientifically justify the indicators. These indicators are purely statistical in nature and highlight terms which have value due to their occurrence or positioning in the text. In this document we detail these indicators by exposing both the theoretical hypotheses and the calculations. After an initial automated extraction, comes the work of the analyst to rework the extraction through manual work (which is not necessarily very long) to better answer the question asked. This step is complementary to automatic statistical analysis in order to group and delete irrelevant terms. In this document we will present some use cases to illustrate this point.\nIn summary, textual analysis is initially based on an automatic extraction of candidate terms, followed by a cleaning step by the analyst which aims to direct towards the question:\nThe objective of textual analysis is to highlight salient elements in a textual corpus and to propose an analysis and contextualization of these elements.\nTextual analysis can be done in different ways. The one we use here is called the Bag-of-Words approach, literally, bag of words. The idea of this approach is to identify salient terms in the text and carry out a statistical analysis of these terms. Unlike semantic analysis, the meaning of terms is not taken into account. The approach focuses on terms and the association of these terms.\nThe approach is based on different sequential steps that we will follow in this tutorial.\n\nText cleanup\nIn text analysis, linguistic complexity is a major challenge. Whether it is plurals, conjugations, synonyms or even homographs and other variations, there are problems that we must address in order to maximize the informational value of the elements that we can extract from the text. In table X. different problems that can be encountered in a corpus of text are grouped together. Some are generic (like synonyms and homographs) others are specific to domains (the question of chemical elements in particular).\n\n\n\n\n\n\n\n\n\nCase\nExample\nProblem\nSolution\n\n\n\n\nSynonym - Abbreviation\nRhino ~ Rhinoceros\nUnderestimation of the term\nStemming, Lemmatisation\n\n\nSynonym\nDwelling ~ House\nUnderestimation of the terms\nStemming, Lemmatisation\n\n\nHomographe\nBass (Fish) ~ Bass (Instrument)\nOverestimation of the term\nLemmatisation\n\n\nCompound terms\nFuel Cell\nOverestimation of fuel and cell, Underestimation of fuel cell\nC-value\n\n\nNumbers - Latin\n1, 25gr, C02\nRemoving numbers can result in deleting valuable information.\nDictionary, RegEx\n\n\nNumbers - Roman\nUniv. Bordeaux IV\nCan be confused with acronyms (IV). Can create items with empty values.\nDictionary\n\n\nConjugation\nEvolved ~ Evolves ~ Evolving\nUnderestimation of the term evolve.\nLemmatisation\n\n\nPlurals\nMill ~ Mills\nUnderestimation of the term Mill\nStemming, Lemmatisation\n\n\nChemistry\nC02\nUnderestimation of the term C02 if improperly cleaned\nRegEx, Dictionary\n\n\nAcronyms\naminophenyltrimethoxysilane ~ APhTMS\nUnderestimation of the term APhTMS\nDictionary\n\n\n\nThe problems are mainly linked to counting problems which are the keystone of textual analysis. The elements which are given in solution will be explained one by one in the following paragraphs. For now the key point we want to make is that it is important to understand why each of these identified cases poses a problem in order to have correct counts. For the majority of cases, the underlying problem is that not processing the text creates more tokens than necessary resulting in an undervaluation of terms. For example, if we keep tire and tyres in the text, we have tokens with proper counts. Logic would, however, dictate that the two terms have the same informational value and are therefore grouped with a single frequency. The same thing is true for synonyms, what interests us basically is to know the importance of the concept of accommodation in a corpus. Suppose we are interested in the importance of housing among young people. In the stories that we can collect we are therefore indifferent between house, housing or even habitat the value is the same. Choosing a reference term and replacing all the synonyms with this value therefore allows you to better appreciate how many times the question of housing appears in the corpus.\nThe following subsections present the different stages of text preparation that we generally encounter in word processing software and in research papers that use open-source algorithms.\n\n\nStopwords\nThe BOW approach is based on the identification of terms that have strong informational value for a given question. One of the first steps then consists of removing terms with little or no informational value. These terms are called stop words. Typically words such a, the, or, and, to are considered tool words. These terms have high frequencies in any corpus and therefore create noise compared to the terms we seek to highlight. This list can, however, be amended if a term is essential for a given question. Conversely, it is possible to add terms which have no value in a particular context or for a particular question. For example, if we analyze a corpus of press data, it is possible to highlight events (trade fairs, congresses, etc.) which do not provide information if we seek to analyze technology. We can then choose to remove these terms for this particular question. A list of tool words starts from a list of basic terms and is then amended by the analyst depending on the case study and the question being addressed.\nIn R this can done with the tm package. The tm package has a removeWords function that takes two arguments, a first is the text from which we want to remove words, the second argument is an object that contains the words to remove. This can either be a simple list of words (made with the c() function), or stopword lists supplied by different packages. At this stage we are mostly interested in removing stopwords which implies that we need to also specify the language of the text. Using the kind argument allows for the specification of the language en for english, nl for dutch, fr for french and so on. In the code below we remove english stopwords from text_object which has the format of a character string.\nLet’s use as an example the abstract of a patent on water desalination:\n\ntext_object = \"Provided is a separation membrane for seawater desalination\nand a method for manufacturing the same, and more particularly, a separation\nmembrane for seawater desalination with excellent water permeability and salt\nrejection and a method for manufacturing the same. If the separation membrane\nfor seawater desalination and the method for manufacturing the same according\nto the present disclosure are applied, it is possible to provide a separation\nmembrane for seawater desalination with excellent water permeability and salt\nrejection. Therefore, it is possible to provide a separation membrane for\nseawater desalination with improved performance in comparison to an existing\nseparation membrane for seawater desalination. As a result, water resources\nmay be widely utilized.\"\n\nWe remove the stopwords from this text:\n\nlibrary(tm)\n  text_object_nostopwords = removeWords(text_object, stopwords(kind = \"en\")) \n\n\n\"Provided   separation membrane  seawater desalination   method \nmanufacturing  ,   particularly,  separation membrane  seawater\ndesalination  excellent water permeability  salt rejection   method \nmanufacturing  . If  separation membrane  seawater desalination \nmethod  manufacturing   according   present disclosure  applied,   possible  provide  separation membrane  seawater desalination  excellent\nwater permeability  salt rejection. Therefore,   possible  provide \nseparation membrane  seawater desalination  improved performance \ncomparison   existing separation membrane  seawater desalination. \nAs  result, water resources may  widely utilized.\"\n\n[1] \"Provided   separation membrane  seawater desalination   method \\nmanufacturing  ,   particularly,  separation membrane  seawater\\ndesalination  excellent water permeability  salt rejection   method \\nmanufacturing  . If  separation membrane  seawater desalination \\nmethod  manufacturing   according   present disclosure  applied,   possible  provide  separation membrane  seawater desalination  excellent\\nwater permeability  salt rejection. Therefore,   possible  provide \\nseparation membrane  seawater desalination  improved performance \\ncomparison   existing separation membrane  seawater desalination. \\nAs  result, water resources may  widely utilized.\"\n\n\n\n\nStemming and Lemmatization\nIt is common, even natural, to find inflections of the same term in a corpus of texts. The presence of a term and its plural (desalinator, desalinators), abbreviations (pneu, pneumatics), conjugations (run, ran, running) or terms with a close semantic meaning (desalinator, desalination) are common occurrences. These inflections, however, pose a problem in term frequency counts. In general, we consider that the terms desalinator, desalination and desalinators have the same informational value and are therefore synonymous. Retaining multiple inflections in the text results in a frequency calculation for each individual term resulting in a lower overall importance of each term. We would like to have only one count, for a term that we consider to be the reference term. There are two approaches to doing this, stemming and lemmatization. Stemming approaches this issue by reducing each word to its stem. The stem that results from this process is not always a word and can be difficult to understand out of context. Lemmatization has a different approach and used dictionary definitions to replace terms by a main form. Figure 2 gives an example for different inflections which are reduced to a main form, in this case: desalinate).\n\n\n\n\n\n\n\n\nflowchart LR\n  C(Desalinate)\n  D[Desalinates] --&gt; C\n  E[Desalinating] --&gt; C\n  F[Desalinated] --&gt; C\n  G[Desalinator] --&gt; C\n  H[Desalination] --&gt; C\n\n\n\n\nFigure 2: Example of lemmatisation, where variations are replaced by “desalinate”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  O(Desalin)\n  J[Desalinates] --&gt; O\n  K[Desalinating] --&gt; O\n  L[Desalinated] --&gt; O\n  M[Desalinator] --&gt; O\n  N[Desalination] --&gt; O\n\n\n\n\nFigure 3: Example of Stemming, where variations are reduced to their stem “desalin”\n\n\n\n\n\n\n\nThere are practical advantages to using lemmatization since the main form remains readable, while with stemming this is more complicated. In fine, it’s up to the analyst to decide which approach is best for both the question at hand and the data chosen. In the following table some advantages and disadvantages are shown:\n\n\n\n\n\n\n\n\nAspect\nLemmatization\nStemming\n\n\n\n\nAccuracy\nBetter accuracy, considers context\nFaster, computationally less expensive\n\n\nReadability\nImproved, real words\nSimpler, heuristic rules\n\n\nContext Preservation\nConsiders word meaning, preserves context\nMay lead to over-stemming, loss of specificity\n\n\nComputational Complexity\nMore computationally expensive\nLess computationally expensive\n\n\nResource Intensive\nRequires linguistic resources\nMinimal resources required\n\n\n\n\nStemming and Lemmatization in R\nFor the implementation of lemmatization we will use the textstem package. Lemmatization is done in two steps, first a dictionnary is created based on the text. Basically this means that all terms in the provided text are identified and for these terms lemmas are identified. In a second step this dictionary is then applied to the text. The main reason for this two-step approach is to reduce computation time since we don’t have to search through words that are not in the text.\n\nlibrary(textstem)\n# Some variations on a word as an example:\nExample_text &lt;-  c(\"Desalinates\", \"Desalinating\", \"Desalinated\", \"Desalinator\", \"Desalination\")\nExample_text &lt;- tolower(Example_text) # remove the capital letters (required)\n# we make a dictionary from the text\nMy_dico &lt;-  make_lemma_dictionary(Example_text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\nlemmatized_text &lt;- lemmatize_strings(Example_text, dictionary = My_dico)\nlemmatized_text\n\n[1] \"desalinate\"  \"desalinate\"  \"desalinate\"  \"desalinator\" \"desalinate\" \n\n\nThe function has changed the words in the vector to their dictionary reference form. Not that “desalinator” has not been changed. This is because the word has no reference form in the underlying dictionary. We can address this issue by fist making our own dictionary and applying it before using the more generic ones. Actually, let’s go a step further. Suppose we are aiming for an analysis that aims at technologies. In this case all the conjugated forms of the word are actually relating to the “desalinator” as a technology. We could decide to replace all these words with “desalinator”. Lets’ see how we could do this:\n\nlibrary(quanteda)\n# first we create a dictionary that contains the variations of the word and the reference form:\nMy_dico = data.frame(term = c(\"Desalination\", \"Desalinates\", \"Desalinating\", \"Desalinated\"), lemma =c(\"Desalinator\", \"Desalinator\", \"Desalinator\", \"Desalinator\"), stringsAsFactors = FALSE)\n\n# we extract the tokens\nExtracted_tokens &lt;- tokens(text_object, remove_punct = TRUE)\n# then we use our new dictionnary to replace the tokens \nLemmatized_tokens &lt;- tokens_replace(Extracted_tokens,\n                             pattern = My_dico$term,\n                             replacement = My_dico$lemma,\n                             case_insensitive = TRUE, \n                             valuetype = \"fixed\")\n# the results is a list, we want our character string back so we unlist...\nLemmatized_tokens = unlist(Lemmatized_tokens)\n# ... and recombine the words\ncat(Lemmatized_tokens)\n\nIn certain cases we would want to keep the library from replacing specific terms, we can do this by explicitly excluding them from the set\n\nlibrary(lexicon)\n# we are looking to implement our own cleaning in this scenario, we have a particular vision of what we want to do, so we want to make sure changes is text follow our own definitions and not those of a dictionary which would not reflect our aim.\n# excluding specific words: \nmy_lex &lt;- lexicon::hash_lemmas[!token == \"Desalinator\", ]\ndf_lemmatized &lt;- lemmatize_strings(text_object, dictionary = my_lex)\ndf_lemmatized\n\nStemming is a more straightforward approach, it reduces the word to their stems with specific algorithms. Different algorithms have different approaches used to define the stem of a word.\n\n# we use the tm package and the stemDocument function\nExample_text &lt;-  c(\"Desalinates\", \"Desalinating\", \"Desalinated\", \"Desalinator\", \"Desalination\")\ntext_object_nostopwords &lt;- tm::stemDocument(Example_text)\ntext_object_nostopwords\n\n[1] \"Desalin\" \"Desalin\" \"Desalin\" \"Desalin\" \"Desalin\"\n\n\n\n\n\nThe dictionary\nIn many case there are word that we want to remove from the text because they appear in a high frequency but do not contain any value of the analysis. We can create a vector of words that we want to remove:\n\nMy_dictionnary &lt;- c(\"project\", \"invention\", \"aim\", \"goal\", \"paper\", \"water desalination\")\n\nWe can then remove these words using the removeWords function from the tm package.\n\ntext &lt;- tm::removeWords(tmp, My_dictionnary)\n\nThis approach breaks the structure of a sentence and should be used carefully. Later on we will search for specific sentence structures.\n\n\nThe Regex language\nIn many cases, text cleaning can require us to search and replace, remove or extract specific patterns in the text. For example in twitter data, we want to extract the # that are used in the tweets, when working with geo data we might want to extract postal codes which have specific patterns. For chemical solutions we might want to search specifically for µg, mg or numbers, in other cases the text contains errors or formatting that is creating noise in the text. All this and more can be done with a specific language called RegEx which stands for Regular Expressions. Using this language we can search for very specific patterns in the text, or indeed in any chain of characters. For example if we have the task of extracting postcodes from a character chain that contains a full address, we need to search for a chain that contains four numbers followed by two letters (at least in the Netherlands):\n\naddress &lt;- \"Straatweg 42, 9942AA, dorpstad\"\n\nWe can search for this pattern using the following regex:\n\n\"\\d[0-9]{3}[A-Z]{2}\"\n\n\nearches for digits, we specify that we search for any number between 0 and 9 by using the brackets.\n{3} indicates that we want to repeat this 3 times after the first, in other words we search for 4 numbers between 0 and 9\n[A-Z] searches for capital letters between A and Z. {2} indicates that we want 2\n\nWe can use this type of expression directly in most functions in R. However we often need to adjust some of the “\\” in the text. In the example below we need to add a “\\”:\n\nlibrary(stringr)\nstr_extract(\"Straatweg 42, 9942AA, dorpstad\", \"\\\\d[0-9]{3}[A-Z]{2}\")\n\n[1] \"9942AA\"\n\n\nThe combination of these elements will hence search for 4 numbers followed by 2 capital letters. Given that RegEx is a language in itself, it goes beyond the scope of this document to go into details. Know that it exists and can help with the identification of specific patterns in data. For more information see this website for testing and this website to get you started.\n\n\nPart-of-Speech Tagging\nOnce we have harmonized the text with the different cleaning steps, we are going to enter more into the details of the identification of terms with informational value. In textual data we will often see that one term alone has less informational value than when it’s combined with another: - Windmill energy - Fuel cell - Water desalination system\nWindmill is of less interest than Windmill energy. This means we need to find a way to identify these terms (also called n-grams) and extract them. The general understanding in the field of natural language processing and text mining is that n-grams (including bigrams, trigrams, etc.) can capture more complex patterns and relationships in text compared to monograms (single words). This is because n-grams consider sequences of words, providing more context and capturing dependencies between adjacent words.\nIf we take a look at the text we were initially working with we can identify mutiple multi-terms of interest:\n“Provided is a separation membrane for seawater desalination and a method for manufacturing the same, and more particularly, a separation membrane for seawater desalination with excellent water permeability and salt rejection and a method for manufacturing the same. If the separation membrane for seawater desalination and the method for manufacturing the same according to the present disclosure are applied, it is possible to provide a separation membrane for seawater desalination with excellent water permeability and salt rejection. Therefore, it is possible to provide a separation membrane for seawater desalination with improved performance in comparison to an existing separation membrane for seawater desalination. As a result, water resources may be widely utilized.”\nLet’s start with the analysis of this text. Start with downloading a language model that we will use for the tagging of the words in the text. The language can be changed according to the language of the text you are trying to analyse.\n\n#install.packages(\"udpipe\")\nlibrary(udpipe)\nud_model &lt;- udpipe::udpipe_download_model(language = \"english\")\nud_model &lt;- udpipe::udpipe_load_model(ud_model)\n\nx &lt;- udpipe_annotate(ud_model, x = text_object, doc_id = 1)\nx &lt;- as.data.frame(x)\nx\n\n\n\n\n\n\n\nFigure 4: Output of the Tagging process\n\n\n\nThe output of this script gives a dataframe in which each word (token) is identified, its lemma is supplied and then upos, xpos and feats are given:\n\nupos: “universal part-of-speech” -&gt; tags\nxpos: more detailed, language specific elements\n\n\n\n\nUPOS\nXPOS\nPOS\n\n\n\n\nNOUN\nNN\nNoun\n\n\nVERB\nVB\nVerb\n\n\nADJ\nJJ\nAdjective\n\n\nADV\nRB\nAdverb\n\n\nPRON\nPRP\nPersonal Pronoun\n\n\nADP\nIN\nAdposition\n\n\nCONJ\nCC\nConjunction\n\n\nNUM\nCD\nNumeral\n\n\n\n\nfeats: The feats field in the annotation output represents morphological features associated with each token in the processed text. Morphological features can include various linguistic information related to the morphology of a word, such as gender, number, case, tense, aspect, and more. These features are language-specific and can provide detailed information about the grammatical and morphological properties of words in a text.\n\nNow let’s do the same for a text that’s less about technologies but more about social issues with climate change:\n\ntext2 = \"Climate anxiety, an emotional response to the growing threat of climate change, \nhas become a prevalent concern in today's society. As global temperatures rise and extreme\nweather events escalate, individuals grapple with a sense of helplessness, fear, and \nimpending environmental doom. The constant barrage of alarming news, coupled with the \nrealization of the ecological crisis, fuels this anxiety. It manifests as eco-grief, \neco-anxiety, or climate despair, impacting mental health. Coping strategies \ninvolve activism, eco-conscious lifestyle changes, and fostering resilience. \nAddressing climate anxiety necessitates global cooperation, sustainable \npolicies, and a collective commitment to mitigating the impacts of climate change.\"\n\n\nx &lt;- udpipe_annotate(ud_model, x = text2, doc_id = 1)\nx &lt;- as.data.frame(x)\nx\n\n This example shows more clearly the effects of lemmatization on the text and all the different elements that are identified by the tagging system, punctuation included. Treating the text in this way allows us to identify specific phrases within the text that can be of value for an analysis. When we focus on the analysis of technologies, we are mainly interested in the nouns which would extract the technological terms from the text. If we want to identify social aspects, or emotions we can focus on nouns, verbs and adjectives. According to the question we are trying to answer, we can search for different elements within the text itself. The advantage of Part-Of-Speech tagging is that we can chose these patterns ourselves. These patterns are referred to as “word phrases”. Using the keywords_phrases function from the udpipe package we can search them directly.\n\n(A|N)*: This part represents a sequence of zero or more occurrences of either an adjective (A) or a noun (N).\nN: This represents a single occurrence of a noun.\n(P+D*(A|N)*N)*: This part represents a sequence that includes a preposition (P) followed by zero or more determiners (D) and then followed by the same pattern as described earlier for adjectives and nouns.\n\n\nx$phrase_tag &lt;- as_phrasemachine(x$upos, type = \"upos\")\nstats &lt;- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \n                            pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                            is_regex = TRUE, detailed = TRUE)\n\n\n\n\nResult of the phrases extraction\n\n\nThe results of the extraction show the different patterns containing adjectives, prepositions, determiners and nouns. The terms that are identified in this way are less random than extracting all possible combinations of sequential tokens. Of course the type of pattern can be adjusted in relation to a specific question or specific data.\nWe now have a long list of terms we can use which should have at least some informational value. But not all the terms have the same value. emotional response and response do not convey the same level of information. We hence need a method to determine which terms we would want to keep. This is especially the case for nested terms such as emotional response which contains two monograms. How can we automatically define that it’s the combination of both terms that has value rather than each term individually?\n\nC-value\nC value = \\(log_2|a|f(a) - \\frac{1}{P(T_a)}\\sum_{b \\in T_a}f(b)\\)\nIt’s this problem that the c-value indicator proposed by (frantzi2000automatic?) seeks to address. The logic is to reduce the value of a term according to the number of times it is nested within another term. The hypothesis is that if a term only appears as part of a larger term, it has no value. For example, in Table 1, we compute the c-values for the term “Fuel Cell Battery Stack”. This term has 3 nested terms (fuel cell battery, Fuel Cell, Fuel). The term \\(Fuel\\) appears 14 times in the text, of these 14 occurrences 12 are attributed to the broader term Fuel Cell. This reduces significantly the importance of the term \\(Fuel\\) in the text. Applying the formula from the beginning of this section we can compute the values for each term we get:\n\n\n\nTable 1: C Value computation illustration\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nFrequency\nn-grams\n\\(P_t\\)\n\\(\\sum f(b)\\)\nc-value\n\n\n\n\nFuel\n14\n1\n3\n12 + 10 + 7 = 29\n0\n\n\nFuel Cell\n12\n2\n2\n10 + 7 = 17\n3.5\n\n\nFuel Cell Battery\n10\n3\n1\n7\n4.75\n\n\nFuel Cell Battery Stack\n7\n4\n0\n0\n14\n\n\n\n\n\n\nThe results provide a high value for Fuel Cell Battery Stack, while Fuel Cell Battery and Fuel Cell have a positive but low value. Whether or not they will be kept in the final dataset depends on the values for other terms from other documents.\nThe c-value helps us select words based on a specific hypothesis, which related to the nestedness of terms. Another option is to count the frequencies of the terms and aim to assess when a frequency is high enough for us to consider a term important. For this we have multiple indicators which we will present now. Note however, that the c-values can be combined with the following indicators to identify terms more precisely.\n\n\n\nTerm classification\n\nOccurences\nThe first indicator of a term’s importance is simply the number of times it appears in a document. The assumption is that the frequency of occurrence of the term correlates with its importance in the corpus.\nThis frequency can be calculated in two ways: firstly, we can consider absolute frequency, i.e. the number of times the term is encountered in the corpus. This approach implies that if the term occurs three times in the same document, we increase the frequency by three. This method will therefore give the same score to a term that appears 10 times in a single document as to a term that appears once in 10 documents.\nThe second approach consists in calculating frequencies at document level, so that even if a term appears 10 times in a document, its frequency only increases by 1.\nBoth approaches have their advantages and disadvantages. Counting at the sentence level brings out terms that are important in a subset of documents, whereas the document-level approach will bring out mainly generic terms (terms present in all documents).\nTo conduct an in-depth analysis, these indicators have limitations. A problem of generality. We can consider a term to be generic when it appears in a significant number of documents (jones1972statistical?). The use of frequency-based classification can therefore lead to the highlighting of elements of little interest. For example, when studying cars, it’s not necessarily useful to highlight , and . More precise terms might contain more information.\n\nTaking into account the absolute frequency of terms can help to highlight more specific terms. However, this approach is influenced by document size. In a corpus with heterogeneous text lengths, this largely influences the counts and can bias the identification work.\nThe limitations of raw counts in texts has led to the development of indicators that aim to avoid generic terms while highlighting more specific terms that represent at least part of the corpus. In the case of technological analyses, this means avoiding generic technological terms while retaining important terms for technological sub-segments.\n\n\n\n\\(\\chi^2\\) or Pearson residual\nA first idea to overcome these biases is to compare the frequency of a term with its expected frequency. In other words, given the size of the vocabulary and the number of documents, how often should a term be found in the corpus? The logic being that the longer the texts or the larger the corpus, the greater the probability of encountering a term. If the frequency exceeds the expected frequency, we could consider that the term is important for the corpus. This indicator is the \\(\\chi^2\\), commonly used in statistics to identify whether a deviation from an expected value is statistically significant. The formula is fairly self-explanatory:\n\\(\\chi^2(i,j) = \\frac{n(i,j) - e(i,j)}{\\sqrt{e(i,j)}}\\)\nHere, \\(n(i,j)\\) corresponds to the observed frequency of the term, while \\(e(i,j)\\) is the expected frequency, taking into account text length and number of documents. The nominator directly calculates the difference between the two values, and then normalizes. A value below what is expected will give rise to negative values, values above what is expected will give rise to positive values, close to 0 we are in the expected (generic) zone. In our corpus, the specificity score highlights as the most specific term. The higher value of certain terms is due to their presence in fewer documents (and therefore have lower co-occurrences). Using this indicator, it’s possible to remove generic terms that recur in a large majority of documents, to leave more room for more specific terms that may be of greater interest to us.\n\n\nTF-IDF / GF-IDF\nOne of the simplest indicators for classifying terms is known as or (). The aim of this indicator is to provide a measure of term specificity. The hypothesis on which the indicator is based is that a term should be considered more specific if it appears in only a few documents relative to the size of the corpus. Thus, a term with a frequency of 10 in the corpus, but present in only 2 documents, will be considered more specific than a term present in 10 documents.\nFormally, we express this idea as:\n\\[\\begin{equation}\n    \\frac{gf_t}{df_t}\n\\end{equation}\\]\nWith \\(gf_t\\) the overall frequency of the term \\(t\\), i.e. the number of times the term appears in the corpus. \\(df_t\\) the frequency in terms of documents, i.e. the number of documents containing the term \\(t\\). The element we need to decide on specificity is a comparison with the size of the corpus. 10 documents may not seem much, but if the corpus contains only 15 documents, 10 is a lot. So we’ll adjust the ratio in the previous equation by another ratio. What we’re interested in is a measure of the importance of the corpus containing the term in the overall corpus. We’ll proceed as follows:\n\\[\\begin{equation}\n    log(\\frac{N}{|d \\in D : t \\in d|})\n\\end{equation}\\]\n\\(N\\) is the total number of documents in the corpus. The numerator is the number of documents in the corpus containing the term \\(t\\). The ratio therefore measures the fraction of documents containing the term.\n\\[\\begin{equation}\n    gf.idf = \\frac{\\frac{gf_t}{df_t}}{log(\\frac{N}{|d \\in D : t \\in d|})}\n\\end{equation}\\]\nIn the nominator, \\(gf_t\\) corresponds to the frequency of appearance of the term \\(t\\) in the corpus. \\(df_t\\) corresponds to the number of documents in which \\(t\\) is present. The nominator therefore measures the average frequency of the term when present in a document.\nThis value is then adjusted in relation to the number of documents in the corpus. In the denominator, \\(N\\) corresponds to the number of documents in the corpus and \\(|d \\in D : t\\in d|\\) corresponds to the fraction of documents in the corpus containing the term \\(t\\). The measure therefore takes into account the importance of the term in the corpus as a whole, while integrating its importance in the documents themselves. The higher this value, the more important the term is in the corpus.",
    "crumbs": [
      "Home",
      "Text Mining",
      "Feature Extraction"
    ]
  },
  {
    "objectID": "Intro_programming_Databasemanip.html",
    "href": "Intro_programming_Databasemanip.html",
    "title": "Database Manipulation",
    "section": "",
    "text": "In this chapter we look into database manipulation with R. We will use a combination of base-R functions and functions from the tidyverse. From this chapter you are expected to know how to:\n\nSubset a dataframe, matrix or dictionnary (specific to Python)\nFilter observations\ngroup observations\nMissing data treatment\nData transformations\nText data operations\n\nRegex\ngsub()\nsubstr()\n\n\n\n\n\n\nWhen working with data, we face different data object types. In practice we will mainly use the following formats:\n\nA scalar is just a numerical value.\n\nscalar_example &lt;- 4\n\nWhen we combine multiple of these values, we can create a vector. The c() function is used to combine different values into a vector:\n\nvector_example &lt;- as.vector(c(4,2,3,1,6))\nprint(vector_example)\n\n[1] 4 2 3 1 6\n\n\nA matrix, just as the mathematical object, is an aggregation of vectors.\n\nmatrix_example = matrix(c(1,2,3,4,5,6,7,8,9,10,11,12), nrow = 3, ncol = 4, byrow = T)\nprint(matrix_example)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\nA dataframe is the more versatile than a matrix since it can contain different types of data. It also has column names that we can refer to when manipulating the object, and can have row names.\n\ndataframe_example &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  Age = c(25, 30, 22, 28, 24),\n  Gender = c(\"Female\", \"Male\", \"Male\", \"Male\", \"Female\"),\n  Subscribed = c(TRUE, FALSE, TRUE, TRUE, FALSE)\n)\nprint(dataframe_example)\n\n     Name Age Gender Subscribed\n1   Alice  25 Female       TRUE\n2     Bob  30   Male      FALSE\n3 Charlie  22   Male       TRUE\n4   David  28   Male       TRUE\n5     Eve  24 Female      FALSE\n\n\n\n\n\n\nThe selection of rows and columns can be achieved in two ways in base-R. Either by selecting the name of the column using the $ operator. Or by referencing the column number using [,].\nThe brackets have as first argument the row number(s) and as second argument the column umber(s). For example if you want to select the first row you would use [1,]. When no argument is supplied (as is the case here for the columns) then all columns are selected. For this reason [1,] gives the first row for all the columns. We can go further and search for the first row and only the first two columns, then we would write [1,c(1,2)].\n\n# select the column by its name: \"Name\"\ndataframe_example$Name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eve\"    \n\n# select the column by its index:\ndataframe_example[1]\n\n     Name\n1   Alice\n2     Bob\n3 Charlie\n4   David\n5     Eve\n\n# select the column by its index:\ndataframe_example[,1]\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eve\"    \n\n\nThe $ operator only works when the column has a header. Matrices do not have a header and therefor cannot be subsetted with the $ operator, only the [,] method works.\n\n# select the column by its index:\nmatrix_example[,1]\n\n[1] 1 5 9\n\n# select a row by its index:\nmatrix_example[1,]\n\n[1] 1 2 3 4\n\n# select multiple rows by their index:\nmatrix_example[c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    9   10   11   12\n\n# select multiple columns by their index:\nmatrix_example[,c(2,4)]\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    6    8\n[3,]   10   12\n\n\nUsing the tidyverse, we can accomplish similar tasks, often in a more efficient manner (in terms of computation time).\n\n\n\n\n\n\nPay Attention\n\n\n\nThe functions we expect you to know here are select() and slice().\n\n\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# Selecting one column\nselected_columns &lt;- data %&gt;% select(Pat_num)\nprint(selected_columns)\n\n       Pat_num\n1  WO200214562\n2 WO2023738962\n3 EP2023778962\n4 FR2019272698\n5  FR201922671\n\n# Selecting multiple columns\nselected_columns &lt;- data %&gt;% select(Pat_num, Year)\nprint(selected_columns)\n\n       Pat_num Year\n1  WO200214562 2002\n2 WO2023738962 2023\n3 EP2023778962 2023\n4 FR2019272698 2019\n5  FR201922671 2019\n\n# Selecting rows\nSelected_rows &lt;- data %&gt;% slice(1:3)\nprint(Selected_rows)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n\n\n\n\nWe often need to subset datasets to suit specific needs. This means that we want to extract rows and columns from a dataset based on specific conditions. For example, we might want to extract all papers published in a specific year, from authors from a specific university. We need to filter according to specific conditions in the data. In base R we can do this by combining the logic operators (which we discussed in another chapter), and the function subset(). The subset() function requires two arguments, the first is the dataframe you want to subset, the second is the condition used to filter:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# Suppose we want to extract all patents filed in 2023:\npats_2023 &lt;- subset(data, data$Year == 2023)\nprint(pats_2023)\n\n       Pat_num Year Domain\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n# Suppose we want to extract all patent NOT filed in 2023:\nsubset = subset(data, !(Year == 2023))\nprint(subset)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n4 FR2019272698 2019   C08K\n5  FR201922671 2019   C08K\n\n# We can also combine multiple conditions\npats_2023 &lt;- subset(data, Year == 2023 | Year == 2002 )\nprint(pats_2023)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n# We can also combine multiple conditions\nsubset &lt;- subset(data, Domain != \"B60C\" & Year &gt;= 2019 )\nprint(subset)\n\n       Pat_num Year Domain\n3 EP2023778962 2023   B29D\n4 FR2019272698 2019   C08K\n5  FR201922671 2019   C08K\n\n\nThe same types of operations can be obtained with the tidyverse environment using the select() and filter() functions. Just as base-R, these functions require two arguments, a first with the dataframe, the second with the elements to select:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# extract all patents with domain B60C\nselected_columns &lt;- data %&gt;% filter(Domain == \"B60C\")\nprint(selected_columns)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n\n# extract the years of the patents with domain B60C\nsubset &lt;- data %&gt;% filter(Domain == \"B60C\") %&gt;% select(Year)\nprint(subset)\n\n  Year\n1 2002\n2 2023\n\n# extract the years of the patents NOT with domain B60C\nsubset &lt;- data %&gt;% filter(!(Domain == \"B60C\")) %&gt;% select(Year)\nprint(subset)\n\n  Year\n1 2023\n2 2019\n3 2019\n\n\n\n\n\nInstead of extracting particular rows and columns, we can also decide to drop rows and columns. In base-R this is achieved with the same functions as for selection, with the addition of a “-” to signify that we want to remove the column or row:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# remove the first column:\nmatrix_example[,-1]\n\n     [,1] [,2] [,3]\n[1,]    2    3    4\n[2,]    6    7    8\n[3,]   10   11   12\n\n# remove the first row:\nmatrix_example[-1,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    5    6    7    8\n[2,]    9   10   11   12\n\n# remove multiple rows by their index:\nmatrix_example[-c(1,3),]\n\n[1] 5 6 7 8\n\n# remove multiple columns by their index:\nmatrix_example[,-c(2,4)]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    5    7\n[3,]    9   11\n\n\nWith the use of the dplyr package:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n# Selecting one column\nselected_columns &lt;- data %&gt;% select(-Pat_num)\nprint(selected_columns)\n\n  Year Domain\n1 2002   B60C\n2 2023   B60C\n3 2023   B29D\n4 2019   C08K\n5 2019   C08K\n\n# Selecting multiple columns\nselected_columns &lt;- data %&gt;% select(-Pat_num, -Year)\nprint(selected_columns)\n\n  Domain\n1   B60C\n2   B60C\n3   B29D\n4   C08K\n5   C08K\n\n# Equivalent form of the previous operation:\nselected_columns &lt;- data %&gt;% select(-c(Pat_num, Year))\nprint(selected_columns)\n\n  Domain\n1   B60C\n2   B60C\n3   B29D\n4   C08K\n5   C08K\n\n\n\n\n\nWith real data we often need to regroup observations by year, organisation, region, or any other entity. For example if we have a set of scientific publications and we want to know how many publications we have per author per year, we need to regroup the observations in both those dimensions. In R we will do this using the tidyverse which will build upon the dplyr package. Mainly we will focus on three functions, group_by, summarise and reframe.\ngroup_by focuses on grouping observations according to a specific value. We can the compute values based on the created groups. For example, if we have a database with researchers and their publications, if we want to know how many publications each of them has, we would first have to create a subset per researcher, count how many publications he/she has, store this information, then move to the next one and so on. group_by allows us to create these subsets and summarise allows us to compute on these sets:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# want to know in which year the first patent in a domain has been filed, and when the last year of filing was.\n# we will group by domain, and then compute the min and max of the years to find the correct dates. \ndata %&gt;% group_by(Domain) %&gt;% summarise(\"first_year\" = min(Year), \"last_year\" = max(Year))\n\n# A tibble: 3 × 3\n  Domain first_year last_year\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 B29D         2023      2023\n2 B60C         2002      2023\n3 C08K         2019      2019\n\n# The arguments \"first_year\" and \"last_year\" will be the names of the columns we are creating in the resulting dataframe.\n\nWe now have a dataframe that has one row for each Domain, with the first and last year as second and third columns. summarise can contain any function, ranging from sum, mean to paste. If you want to simply count the number of occurrences within a group, the n() function will compute this:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# want to know in which year the first patent in a domain has been filed, and when the last year of filing was.\n# we will group by domain, and then compute the min and max of the years to find the correct dates. \ndata %&gt;% group_by(Domain) %&gt;% summarise(\"frequency\" = n())\n\n# A tibble: 3 × 2\n  Domain frequency\n  &lt;chr&gt;      &lt;int&gt;\n1 B29D           1\n2 B60C           2\n3 C08K           2\n\n\n\n\n\n\n\n\nJust as in R, Python allows for the extraction of columns both by their name and by their index. Locations in dataframes are accessed by the use of [[]], the first argument between brackets will refer to the rows, the second to the columns. If you want to extract all rows for a given column (or simply put, you want to extract a specific column), you would write [[“column_name”]].\n\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 28, 24],\n    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n}\n\ndf = pd.DataFrame(data)\n\n# Extracting one Column in Python\nselected_columns = df[['Name']]\nprint(selected_columns)\n\n      Name\n0    Alice\n1      Bob\n2  Charlie\n3    David\n4      Eve\n\n# Extracting multiple Columns in Python\nselected_columns = df[['Name', 'Age']]\nprint(selected_columns)\n\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   22\n3    David   28\n4      Eve   24\n\n\nUsing the index of the columns is also a possibility. For this case we will need to use the .iloc function. .iloc is used when we are addressing a dataframe by the index of the rows and columns, i.e by the number corresponding to the location in the dataframe. We need to provide two arguments for this function, the rows we want to select and the columns we want to select. On the contrary of the previous example, we need to specify rows when we use .iloc. If we want to extract all rows for a given column, we can do so by using :.\nIf we want to use a combination of both names and indexes, we need to use the .loc function which does not expect numbers as arguments.\n\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 28, 24],\n    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n}\n\ndf = pd.DataFrame(data)\n\n# Extracting Columns by Index in Python\n# We use \":\" in the first argument to specify that we want all rows. We then specify the columns we want (here 0 and 1).\nselected_columns = df.iloc[:,[0, 1]]\nprint(selected_columns)\n\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   22\n3    David   28\n4      Eve   24\n\n# If we do not specify which rows we want, Python will\n# interpret the numbers as being rows and not columns.\nfiltered_rows = df.iloc[[0, 4]]\nprint(filtered_rows)\n\n    Name  Age  Gender\n0  Alice   25  Female\n4    Eve   24  Female\n\n# A combination of both is also possible:\nrow3 = df.loc[0, \"Name\"]\nprint(row3)\n\nAlice\n\n\n\n\n\nWe will use filtering on a pandas dataframe. As discussed in the chapter about basic operations, the logic operators will change compared to the base logic operators. Mainly: we will use & for and, | for or and ~ for not.\n\nimport pandas as pd\n# create a small dataframe\ndata = {\n  'Pat_num': [\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"],\n  'Year': [2002, 2023, 2023, 2019, 2019],\n  'Domain': [\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\"]\n}\ndf = pd.DataFrame(data)\n\n# subset the dataframe to extract only patents from 2023\nsubset = df[df['Year'] == 2023]\nprint(subset)\n\n        Pat_num  Year Domain\n1  WO2023738962  2023   B60C\n2  EP2023778962  2023   B29D\n\n# subset the dataframe to extract patent in domain \"B60C\"\n# with year &gt;= 2019\nsubset = df[(df['Year'] &gt;= 2019) & (df['Domain'] == \"B60C\")]\nprint(subset)\n\n        Pat_num  Year Domain\n1  WO2023738962  2023   B60C\n\n# subset the dataframe to extract patent NOT in domain \"B60C\"\nsubset = df[~(df['Domain'] == \"B60C\")]\nprint(subset)\n\n        Pat_num  Year Domain\n2  EP2023778962  2023   B29D\n3  FR2019272698  2019   C08K\n4   FR201922671  2019   C08K\n\n\n\n\n\nWith real data we often need to regroup observations by year, organisation, region, or any other entity. For example if we have a set of scientific publications and we want to know how many publications we have per author per year, we need to regroup the observations in both those dimensions. In Python we will do this using the pandas package. Mainly we will focus on four functions, groupby, agg, count and reset_index.\ngroupby focuses on grouping observations according to a specific value. We can the compute values based on the created groups. For example, if we have a database with researchers and their publications, if we want to know how many publications each of them has, we would first have to create a subset per researcher, count how many publications he/she has, store this information, then move to the next one and so on. groupby allows us to create these subsets and agg allows us to compute on these sets:\n\nimport pandas as pd\ndata = {\n  'Pat_num': [\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"],\n  'Year': [2002, 2023, 2023, 2019, 2019],\n  'Domain': [\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\"]\n}\ndf = pd.DataFrame(data)\n\ngrouped_set = df.groupby('Domain')['Year'].agg([min, max]).reset_index()\nprint(grouped_set)\n\n  Domain   min   max\n0   B29D  2023  2023\n1   B60C  2002  2023\n2   C08K  2019  2019\n\ngrouped_set = df.groupby('Domain').count().reset_index()\nprint(grouped_set)\n\n  Domain  Pat_num  Year\n0   B29D        1     1\n1   B60C        2     2\n2   C08K        2     2",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Database Manipulation"
    ]
  },
  {
    "objectID": "Intro_programming_Databasemanip.html#data-manipulation-with-r",
    "href": "Intro_programming_Databasemanip.html#data-manipulation-with-r",
    "title": "Database Manipulation",
    "section": "",
    "text": "When working with data, we face different data object types. In practice we will mainly use the following formats:\n\nA scalar is just a numerical value.\n\nscalar_example &lt;- 4\n\nWhen we combine multiple of these values, we can create a vector. The c() function is used to combine different values into a vector:\n\nvector_example &lt;- as.vector(c(4,2,3,1,6))\nprint(vector_example)\n\n[1] 4 2 3 1 6\n\n\nA matrix, just as the mathematical object, is an aggregation of vectors.\n\nmatrix_example = matrix(c(1,2,3,4,5,6,7,8,9,10,11,12), nrow = 3, ncol = 4, byrow = T)\nprint(matrix_example)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\nA dataframe is the more versatile than a matrix since it can contain different types of data. It also has column names that we can refer to when manipulating the object, and can have row names.\n\ndataframe_example &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  Age = c(25, 30, 22, 28, 24),\n  Gender = c(\"Female\", \"Male\", \"Male\", \"Male\", \"Female\"),\n  Subscribed = c(TRUE, FALSE, TRUE, TRUE, FALSE)\n)\nprint(dataframe_example)\n\n     Name Age Gender Subscribed\n1   Alice  25 Female       TRUE\n2     Bob  30   Male      FALSE\n3 Charlie  22   Male       TRUE\n4   David  28   Male       TRUE\n5     Eve  24 Female      FALSE\n\n\n\n\n\n\nThe selection of rows and columns can be achieved in two ways in base-R. Either by selecting the name of the column using the $ operator. Or by referencing the column number using [,].\nThe brackets have as first argument the row number(s) and as second argument the column umber(s). For example if you want to select the first row you would use [1,]. When no argument is supplied (as is the case here for the columns) then all columns are selected. For this reason [1,] gives the first row for all the columns. We can go further and search for the first row and only the first two columns, then we would write [1,c(1,2)].\n\n# select the column by its name: \"Name\"\ndataframe_example$Name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eve\"    \n\n# select the column by its index:\ndataframe_example[1]\n\n     Name\n1   Alice\n2     Bob\n3 Charlie\n4   David\n5     Eve\n\n# select the column by its index:\ndataframe_example[,1]\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eve\"    \n\n\nThe $ operator only works when the column has a header. Matrices do not have a header and therefor cannot be subsetted with the $ operator, only the [,] method works.\n\n# select the column by its index:\nmatrix_example[,1]\n\n[1] 1 5 9\n\n# select a row by its index:\nmatrix_example[1,]\n\n[1] 1 2 3 4\n\n# select multiple rows by their index:\nmatrix_example[c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    9   10   11   12\n\n# select multiple columns by their index:\nmatrix_example[,c(2,4)]\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    6    8\n[3,]   10   12\n\n\nUsing the tidyverse, we can accomplish similar tasks, often in a more efficient manner (in terms of computation time).\n\n\n\n\n\n\nPay Attention\n\n\n\nThe functions we expect you to know here are select() and slice().\n\n\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# Selecting one column\nselected_columns &lt;- data %&gt;% select(Pat_num)\nprint(selected_columns)\n\n       Pat_num\n1  WO200214562\n2 WO2023738962\n3 EP2023778962\n4 FR2019272698\n5  FR201922671\n\n# Selecting multiple columns\nselected_columns &lt;- data %&gt;% select(Pat_num, Year)\nprint(selected_columns)\n\n       Pat_num Year\n1  WO200214562 2002\n2 WO2023738962 2023\n3 EP2023778962 2023\n4 FR2019272698 2019\n5  FR201922671 2019\n\n# Selecting rows\nSelected_rows &lt;- data %&gt;% slice(1:3)\nprint(Selected_rows)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n\n\n\n\nWe often need to subset datasets to suit specific needs. This means that we want to extract rows and columns from a dataset based on specific conditions. For example, we might want to extract all papers published in a specific year, from authors from a specific university. We need to filter according to specific conditions in the data. In base R we can do this by combining the logic operators (which we discussed in another chapter), and the function subset(). The subset() function requires two arguments, the first is the dataframe you want to subset, the second is the condition used to filter:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# Suppose we want to extract all patents filed in 2023:\npats_2023 &lt;- subset(data, data$Year == 2023)\nprint(pats_2023)\n\n       Pat_num Year Domain\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n# Suppose we want to extract all patent NOT filed in 2023:\nsubset = subset(data, !(Year == 2023))\nprint(subset)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n4 FR2019272698 2019   C08K\n5  FR201922671 2019   C08K\n\n# We can also combine multiple conditions\npats_2023 &lt;- subset(data, Year == 2023 | Year == 2002 )\nprint(pats_2023)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n3 EP2023778962 2023   B29D\n\n# We can also combine multiple conditions\nsubset &lt;- subset(data, Domain != \"B60C\" & Year &gt;= 2019 )\nprint(subset)\n\n       Pat_num Year Domain\n3 EP2023778962 2023   B29D\n4 FR2019272698 2019   C08K\n5  FR201922671 2019   C08K\n\n\nThe same types of operations can be obtained with the tidyverse environment using the select() and filter() functions. Just as base-R, these functions require two arguments, a first with the dataframe, the second with the elements to select:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# extract all patents with domain B60C\nselected_columns &lt;- data %&gt;% filter(Domain == \"B60C\")\nprint(selected_columns)\n\n       Pat_num Year Domain\n1  WO200214562 2002   B60C\n2 WO2023738962 2023   B60C\n\n# extract the years of the patents with domain B60C\nsubset &lt;- data %&gt;% filter(Domain == \"B60C\") %&gt;% select(Year)\nprint(subset)\n\n  Year\n1 2002\n2 2023\n\n# extract the years of the patents NOT with domain B60C\nsubset &lt;- data %&gt;% filter(!(Domain == \"B60C\")) %&gt;% select(Year)\nprint(subset)\n\n  Year\n1 2023\n2 2019\n3 2019\n\n\n\n\n\nInstead of extracting particular rows and columns, we can also decide to drop rows and columns. In base-R this is achieved with the same functions as for selection, with the addition of a “-” to signify that we want to remove the column or row:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# remove the first column:\nmatrix_example[,-1]\n\n     [,1] [,2] [,3]\n[1,]    2    3    4\n[2,]    6    7    8\n[3,]   10   11   12\n\n# remove the first row:\nmatrix_example[-1,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    5    6    7    8\n[2,]    9   10   11   12\n\n# remove multiple rows by their index:\nmatrix_example[-c(1,3),]\n\n[1] 5 6 7 8\n\n# remove multiple columns by their index:\nmatrix_example[,-c(2,4)]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    5    7\n[3,]    9   11\n\n\nWith the use of the dplyr package:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n# Selecting one column\nselected_columns &lt;- data %&gt;% select(-Pat_num)\nprint(selected_columns)\n\n  Year Domain\n1 2002   B60C\n2 2023   B60C\n3 2023   B29D\n4 2019   C08K\n5 2019   C08K\n\n# Selecting multiple columns\nselected_columns &lt;- data %&gt;% select(-Pat_num, -Year)\nprint(selected_columns)\n\n  Domain\n1   B60C\n2   B60C\n3   B29D\n4   C08K\n5   C08K\n\n# Equivalent form of the previous operation:\nselected_columns &lt;- data %&gt;% select(-c(Pat_num, Year))\nprint(selected_columns)\n\n  Domain\n1   B60C\n2   B60C\n3   B29D\n4   C08K\n5   C08K\n\n\n\n\n\nWith real data we often need to regroup observations by year, organisation, region, or any other entity. For example if we have a set of scientific publications and we want to know how many publications we have per author per year, we need to regroup the observations in both those dimensions. In R we will do this using the tidyverse which will build upon the dplyr package. Mainly we will focus on three functions, group_by, summarise and reframe.\ngroup_by focuses on grouping observations according to a specific value. We can the compute values based on the created groups. For example, if we have a database with researchers and their publications, if we want to know how many publications each of them has, we would first have to create a subset per researcher, count how many publications he/she has, store this information, then move to the next one and so on. group_by allows us to create these subsets and summarise allows us to compute on these sets:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# want to know in which year the first patent in a domain has been filed, and when the last year of filing was.\n# we will group by domain, and then compute the min and max of the years to find the correct dates. \ndata %&gt;% group_by(Domain) %&gt;% summarise(\"first_year\" = min(Year), \"last_year\" = max(Year))\n\n# A tibble: 3 × 3\n  Domain first_year last_year\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 B29D         2023      2023\n2 B60C         2002      2023\n3 C08K         2019      2019\n\n# The arguments \"first_year\" and \"last_year\" will be the names of the columns we are creating in the resulting dataframe.\n\nWe now have a dataframe that has one row for each Domain, with the first and last year as second and third columns. summarise can contain any function, ranging from sum, mean to paste. If you want to simply count the number of occurrences within a group, the n() function will compute this:\n\ndata &lt;- data.frame(\n  Pat_num = c(\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"),\n  Year = c(2002, 2023, 2023, 2019, 2019),\n  Domain = c(\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\")\n)\n\n# want to know in which year the first patent in a domain has been filed, and when the last year of filing was.\n# we will group by domain, and then compute the min and max of the years to find the correct dates. \ndata %&gt;% group_by(Domain) %&gt;% summarise(\"frequency\" = n())\n\n# A tibble: 3 × 2\n  Domain frequency\n  &lt;chr&gt;      &lt;int&gt;\n1 B29D           1\n2 B60C           2\n3 C08K           2",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Database Manipulation"
    ]
  },
  {
    "objectID": "Intro_programming_Databasemanip.html#data-manipulation-with-python",
    "href": "Intro_programming_Databasemanip.html#data-manipulation-with-python",
    "title": "Database Manipulation",
    "section": "",
    "text": "Just as in R, Python allows for the extraction of columns both by their name and by their index. Locations in dataframes are accessed by the use of [[]], the first argument between brackets will refer to the rows, the second to the columns. If you want to extract all rows for a given column (or simply put, you want to extract a specific column), you would write [[“column_name”]].\n\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 28, 24],\n    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n}\n\ndf = pd.DataFrame(data)\n\n# Extracting one Column in Python\nselected_columns = df[['Name']]\nprint(selected_columns)\n\n      Name\n0    Alice\n1      Bob\n2  Charlie\n3    David\n4      Eve\n\n# Extracting multiple Columns in Python\nselected_columns = df[['Name', 'Age']]\nprint(selected_columns)\n\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   22\n3    David   28\n4      Eve   24\n\n\nUsing the index of the columns is also a possibility. For this case we will need to use the .iloc function. .iloc is used when we are addressing a dataframe by the index of the rows and columns, i.e by the number corresponding to the location in the dataframe. We need to provide two arguments for this function, the rows we want to select and the columns we want to select. On the contrary of the previous example, we need to specify rows when we use .iloc. If we want to extract all rows for a given column, we can do so by using :.\nIf we want to use a combination of both names and indexes, we need to use the .loc function which does not expect numbers as arguments.\n\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 28, 24],\n    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n}\n\ndf = pd.DataFrame(data)\n\n# Extracting Columns by Index in Python\n# We use \":\" in the first argument to specify that we want all rows. We then specify the columns we want (here 0 and 1).\nselected_columns = df.iloc[:,[0, 1]]\nprint(selected_columns)\n\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   22\n3    David   28\n4      Eve   24\n\n# If we do not specify which rows we want, Python will\n# interpret the numbers as being rows and not columns.\nfiltered_rows = df.iloc[[0, 4]]\nprint(filtered_rows)\n\n    Name  Age  Gender\n0  Alice   25  Female\n4    Eve   24  Female\n\n# A combination of both is also possible:\nrow3 = df.loc[0, \"Name\"]\nprint(row3)\n\nAlice\n\n\n\n\n\nWe will use filtering on a pandas dataframe. As discussed in the chapter about basic operations, the logic operators will change compared to the base logic operators. Mainly: we will use & for and, | for or and ~ for not.\n\nimport pandas as pd\n# create a small dataframe\ndata = {\n  'Pat_num': [\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"],\n  'Year': [2002, 2023, 2023, 2019, 2019],\n  'Domain': [\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\"]\n}\ndf = pd.DataFrame(data)\n\n# subset the dataframe to extract only patents from 2023\nsubset = df[df['Year'] == 2023]\nprint(subset)\n\n        Pat_num  Year Domain\n1  WO2023738962  2023   B60C\n2  EP2023778962  2023   B29D\n\n# subset the dataframe to extract patent in domain \"B60C\"\n# with year &gt;= 2019\nsubset = df[(df['Year'] &gt;= 2019) & (df['Domain'] == \"B60C\")]\nprint(subset)\n\n        Pat_num  Year Domain\n1  WO2023738962  2023   B60C\n\n# subset the dataframe to extract patent NOT in domain \"B60C\"\nsubset = df[~(df['Domain'] == \"B60C\")]\nprint(subset)\n\n        Pat_num  Year Domain\n2  EP2023778962  2023   B29D\n3  FR2019272698  2019   C08K\n4   FR201922671  2019   C08K\n\n\n\n\n\nWith real data we often need to regroup observations by year, organisation, region, or any other entity. For example if we have a set of scientific publications and we want to know how many publications we have per author per year, we need to regroup the observations in both those dimensions. In Python we will do this using the pandas package. Mainly we will focus on four functions, groupby, agg, count and reset_index.\ngroupby focuses on grouping observations according to a specific value. We can the compute values based on the created groups. For example, if we have a database with researchers and their publications, if we want to know how many publications each of them has, we would first have to create a subset per researcher, count how many publications he/she has, store this information, then move to the next one and so on. groupby allows us to create these subsets and agg allows us to compute on these sets:\n\nimport pandas as pd\ndata = {\n  'Pat_num': [\"WO200214562\", \"WO2023738962\", \"EP2023778962\", \"FR2019272698\", \"FR201922671\"],\n  'Year': [2002, 2023, 2023, 2019, 2019],\n  'Domain': [\"B60C\", \"B60C\", \"B29D\", \"C08K\", \"C08K\"]\n}\ndf = pd.DataFrame(data)\n\ngrouped_set = df.groupby('Domain')['Year'].agg([min, max]).reset_index()\nprint(grouped_set)\n\n  Domain   min   max\n0   B29D  2023  2023\n1   B60C  2002  2023\n2   C08K  2019  2019\n\ngrouped_set = df.groupby('Domain').count().reset_index()\nprint(grouped_set)\n\n  Domain  Pat_num  Year\n0   B29D        1     1\n1   B60C        2     2\n2   C08K        2     2",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Database Manipulation"
    ]
  },
  {
    "objectID": "Intro_programming_basic_operations.html",
    "href": "Intro_programming_basic_operations.html",
    "title": "Basic Operations",
    "section": "",
    "text": "Basic Operations\nThis chapter shows how to use basic operations in both R and Python. We will focus on operations that we encounter in a data analysis setting: arithmetic, comparisons, logic.\nYou are expected to be able to use all the operations described in this chapter.\n\nBasic Mathematical operations\nIn both R and Python, basic mathematical operations can be written directly in the\n\nArithmetic operators\n\n\n\n\n# In R\n1 + 1\n\n[1] 2\n\n4 * 2\n\n[1] 8\n\n4 / 2\n\n[1] 2\n\n\n\n\n\n\n\n# In python\n1 + 1\n\n2\n\n4 * 2\n\n8\n\n4 / 2\n\n2.0\n\n\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nFor more complex mathematical operators we might need some functions that are not provided in the base version of the software we use. For instance, the ln() function and exp() function are not available in base python. We therefore need to load a package which contains these functions, this will be the maths package in Python. For R, both the ln() and exp() functions are part of the base package, we therefor do not need to load an additional package, we can use these functions right away.\n\n\n\n\n\n\n# Base R\nlog(12)\n\n[1] 2.484907\n\nexp(12)\n\n[1] 162754.8\n\nsqrt(12) # square root\n\n[1] 3.464102\n\n\n\n\n\n\n\nimport math\nmath.log(12)\n\n2.4849066497880004\n\nmath.exp(12)\n\n162754.79141900392\n\nmath.sqrt(12)\n\n3.4641016151377544\n\n\n\n\n\n\nComparison Operators\nComparison operations function in the same way in both languages. The only difference we note is in the assignment of the values to a variable in which it is recommended in R to use &lt;-.\n\n\n\n\n\n\nWarning\n\n\n\nEven though = works for simple operations in R, you might run into trouble with more complex code. For a detailed explanation see this post. We recommend you to always use the &lt;- operator when programming in R.\n\n\n\n\n\n\n# R\nx &lt;- 5\ny &lt;- 5\n# Equal to\nx == y\n\n[1] TRUE\n\n# Not equal to\nx != y\n\n[1] FALSE\n\n# Greater than\nx &gt; y\n\n[1] FALSE\n\n# Less than\nx &lt; y\n\n[1] FALSE\n\n# Greater than or equal to\nx &gt;= y\n\n[1] TRUE\n\n# Less than or equal to\nx &lt;= y\n\n[1] TRUE\n\n\n\n\n\n\n\n# Python\nx = 5\ny = 5\n# Equal to\nx == y\n\nTrue\n\n# Not equal to\nx != y\n\nFalse\n\n# Greater than\nx &gt; y\n\nFalse\n\n# Less than\nx &lt; y\n\nFalse\n\n# Greater than or equal to\nx &gt;= y\n\nTrue\n\n# Less than or equal to\nx &lt;= y\n\nTrue\n\n\n\n\n\n\nLogic Operators\nLogical operators are commonly used in data analysis, especially when sub-setting datasets. For example when we want to extract documents that are from the year 2000 which have the term “sustainability” and the term “climate change” but not the term “fossil fuel”. Combining these operators is important, and so is understanding how they work.\n\n\n\n\n\n\nPay Attention\n\n\n\nSome differences between R and Python become apparent here. In R, TRUE and FALSE must be written in all caps to be recognised as the logical operator. In Python, True and False must start with a capitalized letter. or, and, not should also be written exactly in this manner.If these operators are written differently, they will be recognized as objects.\n\n\n\n\n\n\nx &lt;- 4\ny &lt;- 8\n# Equal (==)\nx == y\n\n[1] FALSE\n\n# And (&)\nx == 4 & y == 8\n\n[1] TRUE\n\n# Or (|)\nx == 4 | y == 8\n\n[1] TRUE\n\n# Not (!)\n!y\n\n[1] FALSE\n\n# Combine\nz &lt;- \"plop\"\nx == 4 & (y == 8 | z == \"plop\")\n\n[1] TRUE\n\n\n\n\n\n\n\nx = 4\ny = 8\n# Equal (==)\nx == y\n\nFalse\n\n# And \nx == 4 and y == 8\n\nTrue\n\n# Or\nx == 4 or y == 8\n\nTrue\n\n# Not\nnot x\n\nFalse\n\n# Combine\nz = \"plop\"\nx == 4 and (y == 8 or z == \"plop\")\n\nTrue\n\n\n\n\nIn data analysis, we usually use operators to subset data. This means that we compare a variable to a value to check if it fits our criteria. For example, if we have a column that contains a year, and we only want observations with the year 2003, we will search for year == 2003. In this setting the R operators we just described will be the same. It is possible that these operators varie when different packages are used in python. For instance, in the context of the pandas package, and becomes &, or becomes |, not becomes ~. We will adress these variations in the database manipulation chapter.",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Basic Operations"
    ]
  },
  {
    "objectID": "Topic_modelling_lda.html",
    "href": "Topic_modelling_lda.html",
    "title": "Topic Modelling",
    "section": "",
    "text": "We will use two data types for this tutorial:\n\nLexisNexis from which we have extracted articles about climate anxiety\nLens.org from which we have extracted patents about water desalination\n\nThe aims are:\n\nLearn how to perform topic modelling using R\nLearn how to prepare the data for topic modelling\nLearn how to adjust text preparation to optimize your results for a given question\nDecide on the optimal number of topics in your data\nConnect the topics to other variables in your dataset (players, time, inventors,…)\n\nThe datasets can be found on blackboard. If, howerver, you prefer working on your own data/topic we will first start with a quick brief on how to export data from LexisNexis and import this data into R.\n\n\n\n\nEven though exporting data from LexisUni is quite straightforward, if we want to export in a format that can be directly read by R and used for textmining, we need to tick the right boxes. We will go through those steps now:\nStart with building a query in the advances query section. Make sure you check the AND and OR operators. \nBelow the text elements you can adapt the data range if required.\n\nThen click on search to launch the search process. Once the results are shown, make sure to click the switch to remove Group Duplicates (this should be on “On”). At this point adjust any other filter. Be particulary cautious with the language. Many language models are trained for one language so remove any document not in that language if you use a mono-lingual model.\n Once you’re happy with the dataset, go to the “download button”:  A window will appear, this is were you need to be sure to click the same boxes as in the following screenshots. Ensure that the format is Word (docx) and that the documents are grouped into one.\n\nThe second screen should look like this: (these should be the basic options):\n\nYou can then click on “download”.\nDo this as many times are required to download all the articles (100 per 100). Creating an account with your uu email will make this process slightly more streamlined and faster.\nPut all the files into one folder. We will then use the LexisNexisTools package to import and format this data to make it directly usable.\nFirst we need to search for all the documents, using the list.files() function we create a list of all the .docx documents in the “LN” folder. Note that the LN folder is created by me to store the files, replace LN with the name of the folder in which you store the files. If the files are stored directly at the root of the current WD, use ““.\n\nmy_files &lt;- list.files(pattern = \".docx\", path = \"LN\", full.names = TRUE, recursive = TRUE, ignore.case = TRUE)\n\nAt this stage the data is not yet imported, we merely created a list with paths to the files. We will now load those files into R with the lnt_read() function. This will load the data into a specific object with a specific format that we cannot use directly. We therefore transform this object with the lnt_convent() function. We specify “to =”data.frame” so that the result is a dataframe that we can use.\n\nlibrary(LexisNexisTools)\ndat &lt;- lnt_read(my_files) #Object of class 'LNT output'\nLN_dataframe = lnt_convert(dat, to = \"data.frame\")\n\nWe now have the data loaded in R and ready for use. We will perform two extra actions before we continue to ensure that we don’t run into trouble later on. We start with the removal of identical articles (even though the switch is on, there are still some left…) and we remove the articles that are too short for the analysis. We use the nchar() function to count the number of characters in each article and remove those with less than 200 characters.\n\nLN_dataframe = unique(LN_dataframe)\nLN_dataframe$length = nchar(LN_dataframe$Article)\nLN_dataframe = subset(LN_dataframe, LN_dataframe$length &gt;= 200)\n\nThe dataset is now ready for use.\n\n# loading our custom functions\nlibrary(tm)\nlibrary(udpipe)\nlibrary(tidyverse)\nlibrary(textstem)\nclean_text_lemma &lt;- function(text){\n  #text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with \"\".\n  text &lt;- gsub(\"\\\\.\", \"\", text)\n  text &lt;- gsub(\"\\\\;\", \"\", text)\n  text &lt;- gsub(\"\\\\-\", \"\", text)\n  text &lt;- gsub(\"\\\\+\", \"plus\", text)\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  # here we apply lemmatization instead of stemming:\n  lemma_dico &lt;-  make_lemma_dictionary(text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\n  text &lt;- lemmatize_strings(text, dictionary = lemma_dico)\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n  text &lt;- gsub(\"  \", \" \", text)\n}\nc_value = function(words){\n# we need to compute the number of higher order terms and the frequency of these terms\n# we initiate two empty columns in which we can store this information\nwords$terms_containing_term &lt;- 0\nwords$Sum_freq_higher_terms &lt;- 0\n# We make sure the data has a dataframe format\nwords = as.data.frame(words)\n# we now loop over all the words to check how often they are nested\nfor(i in 1:dim(words)[1]){\n  # first we check in which term the term is nested\n  # if the term is part of another string it will return TRUE, FALSE otherwise\n  # The str_detect() function searches for a pattern in a second string and returns\n  # True if the pattern is part of the string\n  words$tmp = stringr::str_detect(words$keyword, words[i,1])\n  \n  # We then only keep the words that contain the word we are searching for\n  tmp = subset(words, words$keyword != words[i,1] & words$tmp == TRUE)\n  # The number of strings in which the pattern is nested is then simply the \n  # dimension of the dataframe we just found\n  words[i,4] &lt;- dim(tmp)[1]\n  # the sum of the frequencies is simply the sum of the individual frequencies\n  words[i,5] &lt;- sum(tmp$freq)\n}\n# now compute the c-value\n# we first adda column that will contain this value\nwords$C_value = 0\n# then we check if there are nested terms or not and apply the formula accordingly\nwords$C_value = case_when(\n    words$terms_containing_term == 0 ~ log2(words$ngram) * words$freq,\n    #keyword | n_gram | freq dataset | terms count terms | sum_freq_high\n    words$terms_containing_term != 0 ~ (log2(words$ngram) * (words$freq - (1/words$terms_containing_term) * words$Sum_freq_higher_terms))\n)\n# to make this work with the other functions we remove the \"tmp\" column...\nwords = words[,-6]\n#... and we reorganise the columns so that we do not have to adjust the other functions\nwords = words[,c(1,3,2,4,5,6)]\nreturn(words)\n}\nterm_extraction = function(Text_data, max_gram, min_freq){\n  # we need three elements of importance for the function to run\n  # the data, the max ngram and the minimum frequency\n  Text_df = as.data.frame(Text_data[1,])\n  x &lt;- udpipe_annotate(ud_model, x = as.character(Text_df[1,2]), doc_id = Text_df[1,1])\n  x &lt;- as.data.frame(x)\n  \n  stats &lt;- keywords_rake(x = x, term = \"lemma\", group = \"doc_id\", \n                         relevant = x$upos %in% c(\"NOUN\", \"ADJ\"))\n  stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword))\n  \n  \n  x$phrase_tag &lt;- as_phrasemachine(x$upos, type = \"upos\")\n  stats &lt;- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \n                            pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                            is_regex = TRUE, detailed = FALSE)\n  \n  stats = subset(stats, stats$ngram &gt;= 1 & stats$ngram &lt;= max_gram)\n  stats = subset(stats, stats$freq &gt;= min_freq)\n  return(stats)\n}\n# specify the dictionary\nmy_dico = c(\"a method\", \"patent\")\n# load the language model\nlibrary(udpipe)\nud_model &lt;- udpipe::udpipe_download_model(language = \"english\")\n#save(ud_model, file = \"ud_model.rdata\")\nud_model &lt;- udpipe::udpipe_load_model(ud_model)\n\n\n\n\n\nWe start by prepping the text as we did in the first tutorial. We use a loop since it is likely that errors occur in this process that will push us to adjust parameters. The use of a loop makes it easier to pause and continue where we left of. Once the code is stabilised you can use the map() function from tidyverse to make this more efficient and run faster. Or even use multi-threading with the foreach package.\nColumn 11 contains the text, we start by lemmatizing the text.\n\nlibrary(textstem)\nLN_dataframe = LN_dataframe[-c(382,431, 464, 1087),]\nfor(i in 1:dim(LN_dataframe)[1]){\n  LN_dataframe[i,11] &lt;-clean_text_lemma(LN_dataframe[i,11])\n  # now er extract the terms\n  tmp &lt;- term_extraction(LN_dataframe[i,c(1,11)], max_gram = 4, min_freq =  1)\n  # and compute the c_value\n  tmp &lt;- c_value(tmp)\n  # we remove words with low values\n  tmp &lt;- subset(tmp, tmp$C_value &gt; 0)\n  # we combine the tokens\n  tmp$keyword &lt;- apply(as.matrix(tmp$keyword), 1, gsub, pattern = \" \", replacement = \"_\")\n  # and we put the text back into the document\n  LN_dataframe[i,11] &lt;- paste(tmp$keyword, collapse = \" \")\n}\n# we save the result in a .rdata file in case we make a mistake later\nsave(LN_dataframe, file = \"LN_dataframe_POS_lemmcleaning.rdata\")\n\nNow that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the DocumentTermMatrix() function from the tm package. This function has one argument which is a dataframe with the text of the corpus.\n\nlibrary(tm)\n# create the document-Term-matrix\ndtm &lt;- DocumentTermMatrix(LN_dataframe$Article)\n\nBased on this matrix we will now try to define how many topics we need to extract. For this we use the FindTopicsNumber and FindTopicsNumber_plot from the ldatuning package.\nThe FindTopicsNumber functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The seq() function creates a vector which starts at the from arguments, stops at the to argument. The size of the steps is set by the by argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, …) we would write seq(from = 2, to = 60, by = 5).\nWe then specify the metrics we want to compute, we have discussed these in the lecture.\n\n\n\n\n\n\nWarning\n\n\n\nFor an unknown reason, the “Griffiths2004” function does not work on mac OSX. It should work for windows users.\n\n\nThe mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algo is stuck, still running or finished.\n\nlibrary(ldatuning)\ntopic_num &lt;- FindTopicsNumber(\n  dtm,\n  topics = seq(from = 2, to = 60, by = 5),\n  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  mc.cores = 8L,\n  verbose = TRUE\n)\nFindTopicsNumber_plot(topic_num)\n\nNow that we have an idea of how many topics we need, so let’s extract the topics. We will use the LDA function from the topicmodels package.\n\nlibrary(topicmodels)\nlibrary(tidytext)\n# perform topic modelling\ntopics_model &lt;- LDA(dtm, k = 7)\n# get the betas\nbetas &lt;- tidytext::tidy(topics_model, matrix = \"beta\")\n# subset the betas for results\nbetas2&lt;- subset(betas, betas$topic %in% c(2,3,4,5,7))\n\nWe now have the topics and the betas for each topic-term couple. We only keep the highest values for each topic:\n\nap_top_terms &lt;- betas2 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nAnd now we visualise some of the results:\n\nlibrary(ggplot2)\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\nAdjust the parameters of the previous text cleaning functions. Adjust the dictionnary, maybe switch the noun phrases option ((A|N)*N(P+D*(A|N)*N)*) to an equivalent that includes actions.\n\n\n\nNow try the same logic on patent data. Check the results step by step. Adjust the dictionary etc. to suit patent data. What do you look for in the case of patent data, how is this different from news data?",
    "crumbs": [
      "Home",
      "Text Mining",
      "Topic Modelling"
    ]
  },
  {
    "objectID": "Topic_modelling_lda.html#exporting-and-importing-lexisuni-data",
    "href": "Topic_modelling_lda.html#exporting-and-importing-lexisuni-data",
    "title": "Topic Modelling",
    "section": "",
    "text": "Even though exporting data from LexisUni is quite straightforward, if we want to export in a format that can be directly read by R and used for textmining, we need to tick the right boxes. We will go through those steps now:\nStart with building a query in the advances query section. Make sure you check the AND and OR operators. \nBelow the text elements you can adapt the data range if required.\n\nThen click on search to launch the search process. Once the results are shown, make sure to click the switch to remove Group Duplicates (this should be on “On”). At this point adjust any other filter. Be particulary cautious with the language. Many language models are trained for one language so remove any document not in that language if you use a mono-lingual model.\n Once you’re happy with the dataset, go to the “download button”:  A window will appear, this is were you need to be sure to click the same boxes as in the following screenshots. Ensure that the format is Word (docx) and that the documents are grouped into one.\n\nThe second screen should look like this: (these should be the basic options):\n\nYou can then click on “download”.\nDo this as many times are required to download all the articles (100 per 100). Creating an account with your uu email will make this process slightly more streamlined and faster.\nPut all the files into one folder. We will then use the LexisNexisTools package to import and format this data to make it directly usable.\nFirst we need to search for all the documents, using the list.files() function we create a list of all the .docx documents in the “LN” folder. Note that the LN folder is created by me to store the files, replace LN with the name of the folder in which you store the files. If the files are stored directly at the root of the current WD, use ““.\n\nmy_files &lt;- list.files(pattern = \".docx\", path = \"LN\", full.names = TRUE, recursive = TRUE, ignore.case = TRUE)\n\nAt this stage the data is not yet imported, we merely created a list with paths to the files. We will now load those files into R with the lnt_read() function. This will load the data into a specific object with a specific format that we cannot use directly. We therefore transform this object with the lnt_convent() function. We specify “to =”data.frame” so that the result is a dataframe that we can use.\n\nlibrary(LexisNexisTools)\ndat &lt;- lnt_read(my_files) #Object of class 'LNT output'\nLN_dataframe = lnt_convert(dat, to = \"data.frame\")\n\nWe now have the data loaded in R and ready for use. We will perform two extra actions before we continue to ensure that we don’t run into trouble later on. We start with the removal of identical articles (even though the switch is on, there are still some left…) and we remove the articles that are too short for the analysis. We use the nchar() function to count the number of characters in each article and remove those with less than 200 characters.\n\nLN_dataframe = unique(LN_dataframe)\nLN_dataframe$length = nchar(LN_dataframe$Article)\nLN_dataframe = subset(LN_dataframe, LN_dataframe$length &gt;= 200)\n\nThe dataset is now ready for use.\n\n# loading our custom functions\nlibrary(tm)\nlibrary(udpipe)\nlibrary(tidyverse)\nlibrary(textstem)\nclean_text_lemma &lt;- function(text){\n  #text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with \"\".\n  text &lt;- gsub(\"\\\\.\", \"\", text)\n  text &lt;- gsub(\"\\\\;\", \"\", text)\n  text &lt;- gsub(\"\\\\-\", \"\", text)\n  text &lt;- gsub(\"\\\\+\", \"plus\", text)\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  # here we apply lemmatization instead of stemming:\n  lemma_dico &lt;-  make_lemma_dictionary(text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\n  text &lt;- lemmatize_strings(text, dictionary = lemma_dico)\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n  text &lt;- gsub(\"  \", \" \", text)\n}\nc_value = function(words){\n# we need to compute the number of higher order terms and the frequency of these terms\n# we initiate two empty columns in which we can store this information\nwords$terms_containing_term &lt;- 0\nwords$Sum_freq_higher_terms &lt;- 0\n# We make sure the data has a dataframe format\nwords = as.data.frame(words)\n# we now loop over all the words to check how often they are nested\nfor(i in 1:dim(words)[1]){\n  # first we check in which term the term is nested\n  # if the term is part of another string it will return TRUE, FALSE otherwise\n  # The str_detect() function searches for a pattern in a second string and returns\n  # True if the pattern is part of the string\n  words$tmp = stringr::str_detect(words$keyword, words[i,1])\n  \n  # We then only keep the words that contain the word we are searching for\n  tmp = subset(words, words$keyword != words[i,1] & words$tmp == TRUE)\n  # The number of strings in which the pattern is nested is then simply the \n  # dimension of the dataframe we just found\n  words[i,4] &lt;- dim(tmp)[1]\n  # the sum of the frequencies is simply the sum of the individual frequencies\n  words[i,5] &lt;- sum(tmp$freq)\n}\n# now compute the c-value\n# we first adda column that will contain this value\nwords$C_value = 0\n# then we check if there are nested terms or not and apply the formula accordingly\nwords$C_value = case_when(\n    words$terms_containing_term == 0 ~ log2(words$ngram) * words$freq,\n    #keyword | n_gram | freq dataset | terms count terms | sum_freq_high\n    words$terms_containing_term != 0 ~ (log2(words$ngram) * (words$freq - (1/words$terms_containing_term) * words$Sum_freq_higher_terms))\n)\n# to make this work with the other functions we remove the \"tmp\" column...\nwords = words[,-6]\n#... and we reorganise the columns so that we do not have to adjust the other functions\nwords = words[,c(1,3,2,4,5,6)]\nreturn(words)\n}\nterm_extraction = function(Text_data, max_gram, min_freq){\n  # we need three elements of importance for the function to run\n  # the data, the max ngram and the minimum frequency\n  Text_df = as.data.frame(Text_data[1,])\n  x &lt;- udpipe_annotate(ud_model, x = as.character(Text_df[1,2]), doc_id = Text_df[1,1])\n  x &lt;- as.data.frame(x)\n  \n  stats &lt;- keywords_rake(x = x, term = \"lemma\", group = \"doc_id\", \n                         relevant = x$upos %in% c(\"NOUN\", \"ADJ\"))\n  stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword))\n  \n  \n  x$phrase_tag &lt;- as_phrasemachine(x$upos, type = \"upos\")\n  stats &lt;- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \n                            pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                            is_regex = TRUE, detailed = FALSE)\n  \n  stats = subset(stats, stats$ngram &gt;= 1 & stats$ngram &lt;= max_gram)\n  stats = subset(stats, stats$freq &gt;= min_freq)\n  return(stats)\n}\n# specify the dictionary\nmy_dico = c(\"a method\", \"patent\")\n# load the language model\nlibrary(udpipe)\nud_model &lt;- udpipe::udpipe_download_model(language = \"english\")\n#save(ud_model, file = \"ud_model.rdata\")\nud_model &lt;- udpipe::udpipe_load_model(ud_model)",
    "crumbs": [
      "Home",
      "Text Mining",
      "Topic Modelling"
    ]
  },
  {
    "objectID": "Topic_modelling_lda.html#topic-modelling-on-news-data",
    "href": "Topic_modelling_lda.html#topic-modelling-on-news-data",
    "title": "Topic Modelling",
    "section": "",
    "text": "We start by prepping the text as we did in the first tutorial. We use a loop since it is likely that errors occur in this process that will push us to adjust parameters. The use of a loop makes it easier to pause and continue where we left of. Once the code is stabilised you can use the map() function from tidyverse to make this more efficient and run faster. Or even use multi-threading with the foreach package.\nColumn 11 contains the text, we start by lemmatizing the text.\n\nlibrary(textstem)\nLN_dataframe = LN_dataframe[-c(382,431, 464, 1087),]\nfor(i in 1:dim(LN_dataframe)[1]){\n  LN_dataframe[i,11] &lt;-clean_text_lemma(LN_dataframe[i,11])\n  # now er extract the terms\n  tmp &lt;- term_extraction(LN_dataframe[i,c(1,11)], max_gram = 4, min_freq =  1)\n  # and compute the c_value\n  tmp &lt;- c_value(tmp)\n  # we remove words with low values\n  tmp &lt;- subset(tmp, tmp$C_value &gt; 0)\n  # we combine the tokens\n  tmp$keyword &lt;- apply(as.matrix(tmp$keyword), 1, gsub, pattern = \" \", replacement = \"_\")\n  # and we put the text back into the document\n  LN_dataframe[i,11] &lt;- paste(tmp$keyword, collapse = \" \")\n}\n# we save the result in a .rdata file in case we make a mistake later\nsave(LN_dataframe, file = \"LN_dataframe_POS_lemmcleaning.rdata\")\n\nNow that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the DocumentTermMatrix() function from the tm package. This function has one argument which is a dataframe with the text of the corpus.\n\nlibrary(tm)\n# create the document-Term-matrix\ndtm &lt;- DocumentTermMatrix(LN_dataframe$Article)\n\nBased on this matrix we will now try to define how many topics we need to extract. For this we use the FindTopicsNumber and FindTopicsNumber_plot from the ldatuning package.\nThe FindTopicsNumber functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The seq() function creates a vector which starts at the from arguments, stops at the to argument. The size of the steps is set by the by argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, …) we would write seq(from = 2, to = 60, by = 5).\nWe then specify the metrics we want to compute, we have discussed these in the lecture.\n\n\n\n\n\n\nWarning\n\n\n\nFor an unknown reason, the “Griffiths2004” function does not work on mac OSX. It should work for windows users.\n\n\nThe mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algo is stuck, still running or finished.\n\nlibrary(ldatuning)\ntopic_num &lt;- FindTopicsNumber(\n  dtm,\n  topics = seq(from = 2, to = 60, by = 5),\n  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  mc.cores = 8L,\n  verbose = TRUE\n)\nFindTopicsNumber_plot(topic_num)\n\nNow that we have an idea of how many topics we need, so let’s extract the topics. We will use the LDA function from the topicmodels package.\n\nlibrary(topicmodels)\nlibrary(tidytext)\n# perform topic modelling\ntopics_model &lt;- LDA(dtm, k = 7)\n# get the betas\nbetas &lt;- tidytext::tidy(topics_model, matrix = \"beta\")\n# subset the betas for results\nbetas2&lt;- subset(betas, betas$topic %in% c(2,3,4,5,7))\n\nWe now have the topics and the betas for each topic-term couple. We only keep the highest values for each topic:\n\nap_top_terms &lt;- betas2 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nAnd now we visualise some of the results:\n\nlibrary(ggplot2)\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\nAdjust the parameters of the previous text cleaning functions. Adjust the dictionnary, maybe switch the noun phrases option ((A|N)*N(P+D*(A|N)*N)*) to an equivalent that includes actions.\n\n\n\nNow try the same logic on patent data. Check the results step by step. Adjust the dictionary etc. to suit patent data. What do you look for in the case of patent data, how is this different from news data?",
    "crumbs": [
      "Home",
      "Text Mining",
      "Topic Modelling"
    ]
  },
  {
    "objectID": "Intro_programming_loops_ifelse.html",
    "href": "Intro_programming_loops_ifelse.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to create loops and program with if-else statements. These chapters only contain elements that we expect you to know at the end of the course. More detailled books and references exist. For more detailes explanations on specific points, please check: R for Data Science.\nLoops are a useful tool for programming. It allows us to automatically repeat operations. For example if we want to compute indicators for each year in a dataset, we can automatically subset per year, compute indicators, store the results and move on to the next year.\n\n\n\n\n\n\nWarning\n\n\n\nThe biggest difference between R and Python resides in the fact that Python starts at 0 while R starts at 1. You can see that in the results below. range(5) gives 0,1,2,3,4 in python.",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Loops, if-else and Mapping"
    ]
  },
  {
    "objectID": "Intro_programming_loops_ifelse.html#loops-if-else-and-mapping",
    "href": "Intro_programming_loops_ifelse.html#loops-if-else-and-mapping",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to create loops and program with if-else statements. These chapters only contain elements that we expect you to know at the end of the course. More detailled books and references exist. For more detailes explanations on specific points, please check: R for Data Science.\nLoops are a useful tool for programming. It allows us to automatically repeat operations. For example if we want to compute indicators for each year in a dataset, we can automatically subset per year, compute indicators, store the results and move on to the next year.\n\n\n\n\n\n\nWarning\n\n\n\nThe biggest difference between R and Python resides in the fact that Python starts at 0 while R starts at 1. You can see that in the results below. range(5) gives 0,1,2,3,4 in python.",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Loops, if-else and Mapping"
    ]
  },
  {
    "objectID": "Intro_programming_loops_ifelse.html#making-loops",
    "href": "Intro_programming_loops_ifelse.html#making-loops",
    "title": "NetworkIsLife",
    "section": "Making loops",
    "text": "Making loops\n\nFor Loops\n\n\n\nA loop in R starts with the for operator. This is followed by a condition that determines how the loop runs (“the loop runs for…”). We start by defining a variable that will take the different values in the loop. Suppose we want to print the value 1 to 5. This requires a loop that takes a variables that starts at 1, and increases by one with each iteration. The in operator is used to define the values the variables will take.\nIn the following code we will show different ways to loop:\n\n# range of numbers\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n# items in a vector\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\")\nfor (fruit in fruits) {\n  print(fruit)\n}\n\n[1] \"apple\"\n[1] \"banana\"\n[1] \"cherry\"\n[1] \"date\"\n\n# Loop with an index\nlanguages &lt;- c(\"Python\", \"Java\", \"C++\", \"Ruby\")\nfor (i in 1:length(languages)) {\n  cat(\"Language\", i, \"is\", languages[i], \"\\n\")\n}\n\nLanguage 1 is Python \nLanguage 2 is Java \nLanguage 3 is C++ \nLanguage 4 is Ruby \n\n# custom step\nfor (number in seq(1, 10, by = 2)) {\n  cat(\"Odd number:\", number, \"\\n\")\n}\n\nOdd number: 1 \nOdd number: 3 \nOdd number: 5 \nOdd number: 7 \nOdd number: 9 \n\n# Nested loops\nfor (i in 1:3) {\n  for (j in 1:2) {\n    cat(\"i =\", i, \", j =\", j, \"\\n\")\n  }}\n\ni = 1 , j = 1 \ni = 1 , j = 2 \ni = 2 , j = 1 \ni = 2 , j = 2 \ni = 3 , j = 1 \ni = 3 , j = 2 \n\n# break statement\n# it is possible to stop the loop given a certain condition\nnumbers &lt;- c(3, 7, 1, 9, 4, 2)\nfor (num in numbers) {\n  if (num == 9) {\n    cat(\"Found 9. Exiting the loop.\\n\")\n    break\n  }\n  cat(\"Processing\", num, \"\\n\")\n}\n\nProcessing 3 \nProcessing 7 \nProcessing 1 \nFound 9. Exiting the loop.\n\n\n\n\n\n\nA loop in Python starts with the for operator. This is followed by a condition that determines how the loop runs (“the loop runs for…”). We start by defining a variable that will take the different values in the loop. Suppose we want to print the value 0 to 4. This requires a loop that takes a variables that starts at 0, and increases by one with each iteration. The in operator is used to define the values the variables will take.\nIn the following code we will show different ways to loop:\n\n# a range of numbers\n\nfor i in range(5):\n    print(\"Iteration\", i)\n\nIteration 0\nIteration 1\nIteration 2\nIteration 3\nIteration 4\n\n# items in a list\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\nfor fruit in fruits:\n    print(\"I like\", fruit)\n\nI like apple\nI like banana\nI like cherry\nI like date\n\n# with an index\nlanguages = [\"Python\", \"Java\", \"C++\", \"Ruby\"]\n\nfor i, language in enumerate(languages):\n    print(\"Language\", i + 1, \"is\", language)\n\nLanguage 1 is Python\nLanguage 2 is Java\nLanguage 3 is C++\nLanguage 4 is Ruby\n\n# with a custom step\n\nfor number in range(1, 11, 2):\n    print(\"Odd number:\", number)\n\nOdd number: 1\nOdd number: 3\nOdd number: 5\nOdd number: 7\nOdd number: 9\n\n# Nested loops\n\nfor i in range(3):\n    for j in range(2):\n        print(\"i =\", i, \", j =\", j)\n\ni = 0 , j = 0\ni = 0 , j = 1\ni = 1 , j = 0\ni = 1 , j = 1\ni = 2 , j = 0\ni = 2 , j = 1\n\n# a break statement\nnumbers = [3, 7, 1, 9, 4, 2]\n\n\nfor num in numbers:\n    if num == 9:\n        print(\"Found 9. Exiting the loop.\")\n        break\n    print(\"Processing\", num)\n\nProcessing 3\nProcessing 7\nProcessing 1\nFound 9. Exiting the loop.\n\n\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nWhile R, in general, does not care about the indentation of your code, Python does. If the code is not properly indented, the script will not run.\n\n\n\n\nWhile loops\nWhile loops continue to loop as long as a specific condition is satisfied. The therefore differ from the for loops which have a specified stopping point. The danger with these loops is that they can theoretically run forever if the conditions is always verified. The basic logic of these loops is while followed by a condition and then the code to execute while this condition is verified:\n\n\n\n\n# Example 1: Simple while loop\ncount &lt;- 1\nwhile (count &lt;= 5) {\n  cat(\"Iteration\", count, \"\\n\")\n  count &lt;- count + 1}\n\nIteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \nIteration 5 \n\n# Example 3: Loop with a condition and next statement (equivalent to continue in Python)\ni &lt;- 0\nwhile (i &lt; 10) {\n  i &lt;- i + 1\n  if (i %% 2 == 0) {\n    next }  # Skip even numbers\n  cat(i, \"\\n\")}\n\n1 \n3 \n5 \n7 \n9 \n\n# Example 4: Nested while loops\nrow &lt;- 1\nwhile (row &lt;= 3) {\n  col &lt;- 1\n  while (col &lt;= 3) {\n    cat(\"Row\", row, \"Column\", col, \"\\n\")\n    col &lt;- col + 1 }\n  row &lt;- row + 1}\n\nRow 1 Column 1 \nRow 1 Column 2 \nRow 1 Column 3 \nRow 2 Column 1 \nRow 2 Column 2 \nRow 2 Column 3 \nRow 3 Column 1 \nRow 3 Column 2 \nRow 3 Column 3 \n\n\n\n\n\n\n\n# Example 1: Simple while loop\ncount = 1\nwhile count &lt;= 5:\n    print(\"Iteration\", count)\n    count += 1\n\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nIteration 5\n\n# Example 3: Loop with a condition and continue statement\ni = 0\nwhile i &lt; 10:\n    i += 1\n    if i % 2 == 0:\n        continue  # Skip even numbers\n    print(i)\n\n1\n3\n5\n7\n9\n\n# Example 4: Nested while loops\nrow = 1\nwhile row &lt;= 3:\n    col = 1\n    while col &lt;= 3:\n        print(\"Row\", row, \"Column\", col)\n        col += 1\n    row += 1\n\nRow 1 Column 1\nRow 1 Column 2\nRow 1 Column 3\nRow 2 Column 1\nRow 2 Column 2\nRow 2 Column 3\nRow 3 Column 1\nRow 3 Column 2\nRow 3 Column 3",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Loops, if-else and Mapping"
    ]
  },
  {
    "objectID": "Intro_programming_loops_ifelse.html#conditional-statements-if-else-elif",
    "href": "Intro_programming_loops_ifelse.html#conditional-statements-if-else-elif",
    "title": "NetworkIsLife",
    "section": "Conditional statements (if, else, elif)",
    "text": "Conditional statements (if, else, elif)\n\n\n\n\n# Using if statement\nx &lt;- 10\nif (x &gt; 5) {\n  cat(\"x is greater than 5\\n\")}\n\nx is greater than 5\n\n# Using if-else statement\ny &lt;- 3\nif (y &gt; 5) {\n  cat(\"y is greater than 5\\n\")\n} else {\n  cat(\"y is not greater than 5\\n\")}\n\ny is not greater than 5\n\n# Using if-else if-else statement\nz &lt;- 7\nif (z &gt; 10) {\n  cat(\"z is greater than 10\\n\")\n} else if (z &gt; 5) {\n  cat(\"z is greater than 5 but not greater than 10\\n\")\n} else {\n  cat(\"z is not greater than 5\\n\")}\n\nz is greater than 5 but not greater than 10\n\n# Nested if statements\na &lt;- 15\nb &lt;- 6\nif (a &gt; 10) {\n  if (b &gt; 5) {\n    cat(\"Both a and b are greater than 10 and 5, respectively\\n\")\n  } else {\n    cat(\"a is greater than 10, but b is not greater than 5\\n\")}}\n\nBoth a and b are greater than 10 and 5, respectively\n\n# Using a conditional expression (ternary operator)\nage &lt;- 18\nstatus &lt;- if (age &gt;= 18) \"adult\" else \"minor\"\ncat(\"You are an\", status, \"\\n\")\n\nYou are an adult \n\n\n\n\n\n\n\n# Using if statement\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\n\nx is greater than 5\n\n# Using if-else statement\ny = 3\nif y &gt; 5:\n    print(\"y is greater than 5\")\nelse:\n    print(\"y is not greater than 5\")\n\ny is not greater than 5\n\n# Using if-elif-else statement\nz = 7\nif z &gt; 10:\n    print(\"z is greater than 10\")\nelif z &gt; 5:\n    print(\"z is greater than 5 but not greater than 10\")\nelse:\n    print(\"z is not greater than 5\")\n\nz is greater than 5 but not greater than 10\n\n# Nested if statements\na = 15\nb = 6\nif a &gt; 10:\n    if b &gt; 5:\n        print(\"Both a and b are greater than 10 and 5, respectively\")\n    else:\n        print(\"a is greater than 10, but b is not greater than 5\")\n\nBoth a and b are greater than 10 and 5, respectively\n\n# Using a conditional expression (ternary operator)\nage = 18\nstatus = \"adult\" if age &gt;= 18 else \"minor\"\nprint(\"You are an\", status)\n\nYou are an adult\n\n\n\n\n\nMapping functions\nWhen working with data, we often want to apply a function to all rows of a column, or even on a whole dataframe. For example if we want to remove the capital letters from text or round up numbers to a set number of digits. This could be achieved by looping over all the cells in the dataframe, but there are often more efficient ways of doing this. There are functions that allow us to apply another function to each cell of the column, or dataframe, that we select. This function takes care of the looping behind the scenes.\nIn R we have multiple options for this, some in base R, some from different packages. We will discuss the most notable functions here.\napply sapply mapply lapply\nmap\n\n\n\n\n# All these functions are \n# present in base R\n\n# Example 1: apply function\nmatrix_data &lt;- matrix(1:9, nrow = 3)\n\n# Apply a function to rows (axis 1)\nrow_sums &lt;- apply(matrix_data, 1, sum)\nprint(\"Row Sums:\")\n\n[1] \"Row Sums:\"\n\nprint(row_sums)\n\n[1] 12 15 18\n\n# Apply a function to columns (axis 2)\ncol_sums &lt;- apply(matrix_data, 2, sum)\nprint(\"Column Sums:\")\n\n[1] \"Column Sums:\"\n\nprint(col_sums)\n\n[1]  6 15 24\n\n# Example 2: lapply function\ndata_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\n\n# Apply a function to each element of the list\nsquared_list &lt;- lapply(data_list, function(x) x^2)\nprint(\"Squared List:\")\n\n[1] \"Squared List:\"\n\nprint(squared_list)\n\n$a\n[1] 1 4 9\n\n$b\n[1] 16 25 36\n\n$c\n[1] 49 64 81\n\n# Example 3: sapply function\n# Similar to lapply, but attempts to simplify the result\nsquared_vector &lt;- sapply(data_list, function(x) x^2)\nprint(\"Squared Vector:\")\n\n[1] \"Squared Vector:\"\n\nprint(squared_vector)\n\n     a  b  c\n[1,] 1 16 49\n[2,] 4 25 64\n[3,] 9 36 81\n\n# Example 4: tapply function\n# Used for applying a function by factors\nsales_data &lt;- data.frame(product = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n                         sales = c(100, 150, 120, 180, 90))\n\n# Apply a function to calculate total sales by product\ntotal_sales &lt;- tapply(sales_data$sales, sales_data$product, sum)\nprint(\"Total Sales by Product:\")\n\n[1] \"Total Sales by Product:\"\n\nprint(total_sales)\n\n  A   B \n310 330 \n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# Example 1: NumPy equivalent of apply\nmatrix_data = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n\n# Apply a function to rows (axis 1)\nrow_sums = np.apply_along_axis(np.sum, axis=1, arr=matrix_data)\nprint(\"Row Sums:\")\n\nRow Sums:\n\nprint(row_sums)\n\n[12 15 18]\n\n# Apply a function to columns (axis 0)\ncol_sums = np.apply_along_axis(np.sum, axis=0, arr=matrix_data)\nprint(\"Column Sums:\")\n\nColumn Sums:\n\nprint(col_sums)\n\n[ 6 15 24]\n\n# List \ndata_list = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\n\n# Apply a function to each element of the dictionary values\nsquared_list = {key: [x**2 for x in value] for key, value in data_list.items()}\nprint(\"Squared Dictionary:\")\n\nSquared Dictionary:\n\nprint(squared_list)\n\n{'a': [1, 4, 9], 'b': [16, 25, 36], 'c': [49, 64, 81]}\n\n# Example 3: pandas\ndata_dict = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\ndf = pd.DataFrame(data_dict)\n\n# Apply a function to each column (series) and return a DataFrame\nsquared_df = df.apply(lambda x: x**2)\nprint(squared_df)\n\n   a   b   c\n0  1  16  49\n1  4  25  64\n2  9  36  81\n\n# pandas \nsales_data = pd.DataFrame({'product': ['A', 'B', 'A', 'B', 'A'],\n                           'sales': [100, 150, 120, 180, 90]})\n\n# Apply a function to calculate total sales by product\ntotal_sales = sales_data.groupby('product')['sales'].sum()\nprint(\"Total Sales by Product:\")\n\nTotal Sales by Product:\n\nprint(total_sales)\n\nproduct\nA    310\nB    330\nName: sales, dtype: int64\n\n\n\n\n\nThe map() function in R\nIn R we also have the map() function, this function is part of the purrr package. map() is able to apply a function of your chosing to each element of a list, vector or other data strucutr. The basic syntax for the function is:\n\nmap(.x, .f, ...)\n\nThe .x argument is the data structure you wish to apply the function to. The .f argument will be the function you will be applying to the dataframe (for example, gsub(), as.integer(), tolower(), etc.)\nThe main advantage of map() is that it is a one format solution while the base R functions (apply, lappply, sapply and tapply) all have different syntaxes and behaviour. map() works on all and behaves identically on all datastructures.\nHere is an example of how to use this function:\n\nlibrary(purrr)\ntest_data = list(3,9,12,15,18)\n\n# we will apply the sqrt() function to these numbers\n\ntest_data_sqrt = map(test_data, ~ sqrt(.))\nprint(test_data_sqrt)\n\n[[1]]\n[1] 1.732051\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 3.464102\n\n[[4]]\n[1] 3.872983\n\n[[5]]\n[1] 4.242641\n\n\nIt also works for dataframes:\n\ntest_dataframe = data.frame(\"value\" = c(3,9,12,15,18))\ntest_data_sqrt = map(test_dataframe, ~ sqrt(.))\nprint(test_data_sqrt)\n\n$value\n[1] 1.732051 3.000000 3.464102 3.872983 4.242641\n\n\nIt also works for a matrix:\n\ntest_matrix = matrix(c(3,9,12,15,18), nrow = 5, ncol = 1)\ntest_data_sqrt = map(test_matrix, ~ sqrt(.))\nprint(test_data_sqrt)\n\n[[1]]\n[1] 1.732051\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 3.464102\n\n[[4]]\n[1] 3.872983\n\n[[5]]\n[1] 4.242641\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nIt is possible to use map() on a specific colomn in a dataframe. You will need to pay close attention to select this column correctly. Applying the function to a matrix will result in the matrix being reverted back to a list. For this reason we add the unlist() function so that the results can be returned in matrix format.\n\n\n\ntest_dataframe = data.frame(\"value\" = c(3,9,12,15,18), \"year\" = c(2000,2004,2018,2021,2025))\ntest_dataframe$value = map(test_dataframe$value, ~ sqrt(.))\nprint(test_dataframe)\n\n     value year\n1 1.732051 2000\n2        3 2004\n3 3.464102 2018\n4 3.872983 2021\n5 4.242641 2025\n\n\n\ntest_matrix = matrix(c(3,9,12,15,18,2000,2004,2018,2021,2025), nrow = 5, ncol = 2)\ntest_matrix[,1] = unlist(map(test_matrix[,1],  ~ sqrt(.)))\n# Using pipes we can also write an equivalent form:\n# test_matrix[,1] = test_matrix[,1] %&gt;% map( ~ sqrt(.)) %&gt;% unlist()\nprint(test_matrix)\n\n         [,1] [,2]\n[1,] 1.732051 2000\n[2,] 3.000000 2004\n[3,] 3.464102 2018\n[4,] 3.872983 2021\n[5,] 4.242641 2025",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Loops, if-else and Mapping"
    ]
  },
  {
    "objectID": "Intro_programming_functions.html",
    "href": "Intro_programming_functions.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to create a function\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Making functions"
    ]
  },
  {
    "objectID": "Intro_programming_functions.html#making-functions",
    "href": "Intro_programming_functions.html#making-functions",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to create a function\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Making functions"
    ]
  },
  {
    "objectID": "State_of_data_04062024.html",
    "href": "State_of_data_04062024.html",
    "title": "State of Data",
    "section": "",
    "text": "This document describes the current state of the available data in the ESMEE project. Given that we continuously add and improve upon the data we create this webpage, so that we can easily update the information and ensure we all have the latest information.\nIn short the datasources we currently have are:\n\n\n\n\n\n\n\n\n\n\nType\nSource\nTime frame\nRegionalised\nEcosystem Element\n\n\n\n\nPatents\nRegpat/Lens.org\n1800-Today\nYes - by hand\nNetworks, Knowledge, Talent, Leadership\n\n\nPublications\nLens.org\n1800-Today\nYes - by hand\nNetworks, Knowledge, Talent, Leadership\n\n\nEuropean Projects\nEU\nFp1-2024\nYes\nNetworks, Knowledge, Leadership\n\n\nKeep/Interreg\nEU\n\nYes\nNetworks, Knowledge, Leadership\n\n\nStartups\nCrunchbase\n2010-2024\nYes\nCulture, Finance\n\n\nSubsidies\nRVO\n\nby hand\nFormal Institutions\n\n\nNews\nNPO\n2010-2024\nby hand\nCulture, Leadership, Demand, Formal Institutions\n\n\nTrademarks/designs\nEU\n\nby hand\nCulture\n\n\nEco-label\nEU\n\nby hand\nCulture\n\n\n\n\n\nPatents are used as an indicator of technological innovation. Each patent family contains information on a specific invention that at the time of submission was new to the state of the art. When all patent information is aggregated into a database we have a valuable source of knowledge on technological innovation.\nSummarizing the data available in patents and all the research questions we can answer with patent is quite complicated. A limited answer to this question would be: Patents allow us to measure and understand who developed (inventors/assignees) which technologies (classifications, text), where (Address of inventors and assignees) and based on what knowledge (citations).\nThere are many complexities and limitations to this data which we will not describe here (there is just too much to address).\n\n\nFor the measurement of technological innovation, we use two patent databases. The first is REGPAT which is provided by the OECD and the patents are already regionlised. The second is lens.org\nHow are regions attributed to a patent? There are two ways to attribute a region to a patent document. First we can look at the address of the patent applicant. If the applicant has supplied an address on the patent we can use it to attach a region to the patent. The second method is to use the address of the inventor. In scientific research the address of the inventor is usually used to regionalize a patent. The reasoning behind this is that we want to approach the region in which the knowledge is located. The inventors usually live close to the place where they work and hence where the knowledge is created. Companies on the other hand can file patent on behalf of subsidiaries in other countries. Actually, there are many fiscal incentives for companies to have other structures file and manage the patents for them. Depending on the level of detail supplied this is more or less easy. In RegPat this work has been done for us. Each patent document has a nuts3 code attached to it. Note: the RegPat database only has patents that went through the EPO, WIPO, or JPO. A patent filed by a Dutch Company at the DPO or even GPO/FPO will not be present in this database. Other than the restrictions per office, there are not limits to the geographical origin of the inventors or the applicants.\nCaution: A patent can be filed in different offices at the same time, there is a difference between a patent family and a patent document. One cannot simply count the number of patents in a region, we need to take into account that we are over evaluating the number of patents in the region if we do not regroup them at the family level.\nWe have the following data distribution of the patents:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompanies_new_per_year = regpat %&gt;% group_by(app_name_harmonised) %&gt;% summarise(\"first\"= min(year), \"last\"= max(year))\ncompanies_new_per_year = companies_new_per_year %&gt;% group_by(first) %&gt;% summarise(\"freq\" = n())\ncouleur = \"#4CAF50\"\np = ggplot(companies_new_per_year, aes(x = first, y = freq)) + geom_bar(stat = \"identity\", fill = couleur) +\n  xlab(\"\") + ylab(\"\") + theme( \n    text = element_text(size=10),\n    plot.title = element_text(hjust = 0.5),\n    panel.background = element_rect(fill = \"transparent\"), # bg of the panel\n    plot.background = element_rect(fill = \"transparent\", color = NA), # bg of the plot\n    legend.background = element_rect(fill = \"transparent\"), # get rid of legend bg\n    legend.box.background = element_rect(fill = \"transparent\")) +\n  scale_y_continuous(breaks = seq(0, max(companies_new_per_year$freq), by = 100)) +\n  scale_x_discrete(breaks = seq(2010, max(companies_new_per_year$first), by = 1))\n\nNew companies with patents per year:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(p)\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# missing values with the naniar package\nlibrary(naniar)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatents contain a variety of variables that we can use for the analysis of innovation and entrepreneurship. Summarizing the data available in patents and all the research questions we can answer with patent is quite complicates. A limited answer to this question would be: Patents allow us to measure and understand who developed (inventors/assignees) which technologies (classifications, text), where (Address of inventors and assignees) and based on what knowledge (citations).\nAll this data sounds great, but patents are a complex data source and not all information is perfectly available. In the following table we summarise the different fields of interest in patent data.\n\n\n\n\n\n\n\n\nCategory\nField\nDescription\n\n\n\n\nDates\nPriority\n\n\n\n\nApplication\n\n\n\n\nPublication\n\n\n\nApplicants\nApp_name\n\n\n\nAssignees\n\n\n\n\nOwners\n\n\n\n\nInventors\n\n\n\n\nClassifications\nIPC\nInternational Patent Classification\n\n\n\nCPC\nCooperative Patent Classification\n\n\n\nUSC\n\n\n\nIdentifiers\nApp_nbr\nApplication number\n\n\n\nAppln_id\nInternal patent number\n\n\n\nPerson_id\nInternal applicant number\n\n\n\nPub_nbr\nPublication number\n\n\n\nPct_nbr\nWo application? Number\n\n\n\nInternat_appln_nr\n\n\n\nLocalisation\nAddress\nFull address as written by the assignee (or the inventor)\n\n\n\nCity\nCity of the assignee (or inventor)\n\n\n\nPostal_code\nPostcode of the assignee (or the inventor)\n\n\n\nReg_code\nNUTS-3\n\n\n\nCtry_code\nIso 2 country code\n\n\n\nReg_share\nShare to attribute to each region on the patent. When multiple assignees are on the patent, and they come from different regions, we only assign a fraction of the patent to the region. 2 regions = 0.5, 3 regions = 0.333 etc.\n\n\n\nApp-share\nWhen multiple assignees on the patent, we attribute a fraction of the count to the assignee. When there are two assignees we only count the patent as 0.5 for the assignee.",
    "crumbs": [
      "Home",
      "State of Data",
      "State of Data"
    ]
  },
  {
    "objectID": "State_of_data_04062024.html#patents",
    "href": "State_of_data_04062024.html#patents",
    "title": "State of Data",
    "section": "",
    "text": "Patents are used as an indicator of technological innovation. Each patent family contains information on a specific invention that at the time of submission was new to the state of the art. When all patent information is aggregated into a database we have a valuable source of knowledge on technological innovation.\nSummarizing the data available in patents and all the research questions we can answer with patent is quite complicated. A limited answer to this question would be: Patents allow us to measure and understand who developed (inventors/assignees) which technologies (classifications, text), where (Address of inventors and assignees) and based on what knowledge (citations).\nThere are many complexities and limitations to this data which we will not describe here (there is just too much to address).\n\n\nFor the measurement of technological innovation, we use two patent databases. The first is REGPAT which is provided by the OECD and the patents are already regionlised. The second is lens.org\nHow are regions attributed to a patent? There are two ways to attribute a region to a patent document. First we can look at the address of the patent applicant. If the applicant has supplied an address on the patent we can use it to attach a region to the patent. The second method is to use the address of the inventor. In scientific research the address of the inventor is usually used to regionalize a patent. The reasoning behind this is that we want to approach the region in which the knowledge is located. The inventors usually live close to the place where they work and hence where the knowledge is created. Companies on the other hand can file patent on behalf of subsidiaries in other countries. Actually, there are many fiscal incentives for companies to have other structures file and manage the patents for them. Depending on the level of detail supplied this is more or less easy. In RegPat this work has been done for us. Each patent document has a nuts3 code attached to it. Note: the RegPat database only has patents that went through the EPO, WIPO, or JPO. A patent filed by a Dutch Company at the DPO or even GPO/FPO will not be present in this database. Other than the restrictions per office, there are not limits to the geographical origin of the inventors or the applicants.\nCaution: A patent can be filed in different offices at the same time, there is a difference between a patent family and a patent document. One cannot simply count the number of patents in a region, we need to take into account that we are over evaluating the number of patents in the region if we do not regroup them at the family level.\nWe have the following data distribution of the patents:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompanies_new_per_year = regpat %&gt;% group_by(app_name_harmonised) %&gt;% summarise(\"first\"= min(year), \"last\"= max(year))\ncompanies_new_per_year = companies_new_per_year %&gt;% group_by(first) %&gt;% summarise(\"freq\" = n())\ncouleur = \"#4CAF50\"\np = ggplot(companies_new_per_year, aes(x = first, y = freq)) + geom_bar(stat = \"identity\", fill = couleur) +\n  xlab(\"\") + ylab(\"\") + theme( \n    text = element_text(size=10),\n    plot.title = element_text(hjust = 0.5),\n    panel.background = element_rect(fill = \"transparent\"), # bg of the panel\n    plot.background = element_rect(fill = \"transparent\", color = NA), # bg of the plot\n    legend.background = element_rect(fill = \"transparent\"), # get rid of legend bg\n    legend.box.background = element_rect(fill = \"transparent\")) +\n  scale_y_continuous(breaks = seq(0, max(companies_new_per_year$freq), by = 100)) +\n  scale_x_discrete(breaks = seq(2010, max(companies_new_per_year$first), by = 1))\n\nNew companies with patents per year:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(p)\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# missing values with the naniar package\nlibrary(naniar)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatents contain a variety of variables that we can use for the analysis of innovation and entrepreneurship. Summarizing the data available in patents and all the research questions we can answer with patent is quite complicates. A limited answer to this question would be: Patents allow us to measure and understand who developed (inventors/assignees) which technologies (classifications, text), where (Address of inventors and assignees) and based on what knowledge (citations).\nAll this data sounds great, but patents are a complex data source and not all information is perfectly available. In the following table we summarise the different fields of interest in patent data.\n\n\n\n\n\n\n\n\nCategory\nField\nDescription\n\n\n\n\nDates\nPriority\n\n\n\n\nApplication\n\n\n\n\nPublication\n\n\n\nApplicants\nApp_name\n\n\n\nAssignees\n\n\n\n\nOwners\n\n\n\n\nInventors\n\n\n\n\nClassifications\nIPC\nInternational Patent Classification\n\n\n\nCPC\nCooperative Patent Classification\n\n\n\nUSC\n\n\n\nIdentifiers\nApp_nbr\nApplication number\n\n\n\nAppln_id\nInternal patent number\n\n\n\nPerson_id\nInternal applicant number\n\n\n\nPub_nbr\nPublication number\n\n\n\nPct_nbr\nWo application? Number\n\n\n\nInternat_appln_nr\n\n\n\nLocalisation\nAddress\nFull address as written by the assignee (or the inventor)\n\n\n\nCity\nCity of the assignee (or inventor)\n\n\n\nPostal_code\nPostcode of the assignee (or the inventor)\n\n\n\nReg_code\nNUTS-3\n\n\n\nCtry_code\nIso 2 country code\n\n\n\nReg_share\nShare to attribute to each region on the patent. When multiple assignees are on the patent, and they come from different regions, we only assign a fraction of the patent to the region. 2 regions = 0.5, 3 regions = 0.333 etc.\n\n\n\nApp-share\nWhen multiple assignees on the patent, we attribute a fraction of the count to the assignee. When there are two assignees we only count the patent as 0.5 for the assignee.",
    "crumbs": [
      "Home",
      "State of Data",
      "State of Data"
    ]
  },
  {
    "objectID": "Topic_modelling_bertopic.html",
    "href": "Topic_modelling_bertopic.html",
    "title": "Installing Bertopic",
    "section": "",
    "text": "Installing Bertopic\n\nCreating a virtual environment with anaconda\nImagine that you have a toolbox where you keep all your tools. Each tool has a specific purpose: a hammer for nails, a screwdriver for screws, etc. Now, think of your computer as a large workshop where you can undertake various projects (like building software or analyzing data). For each project, you might need a different set of tools. Some projects might require a hammer and a saw, while others might need a screwdriver and a wrench.\nIn the world of programming, especially when working with Python, the “tools” are the various libraries and packages that you use to write your programs (like NumPy for mathematical operations, Pandas for data manipulation, etc.). Just like in a real workshop, using the right tools can make your work much easier.\nHowever, there’s a catch. Sometimes, different projects require different versions of the same tool. Maybe one project needs a hammer with a wooden handle, but another requires one with a fiberglass handle. If you tried to use the wrong version, it might not work as well or could even mess up your project.\nThis is where Anaconda environments come in. An Anaconda environment is like having a separate, smaller toolbox for each project. You can put exactly the tools (libraries and packages) you need for a project into its toolbox, including the right versions. This way, when you work on that project, you know you have all the right tools handy, and they won’t interfere with the tools needed for other projects. It keeps everything organized and ensures that your projects run smoothly without conflicts between different versions of your tools.\nIn technical terms, an Anaconda environment is a virtual space on your computer where you can install specific versions of libraries and packages needed for a particular project. This isolation prevents conflicts and makes it easier to manage dependencies, ensuring that you can reproduce your work on another machine without issues.\nLet’s create an anaconda environment for topic modelling with bertopic. Start by downloading anaconda from this website:\nhttps://www.anaconda.com/download/success\nOnce it’s done downloading install and launch the program. You should be greeted by a window that looks like this:\n\nNow we start by creating the environment. Click on “Environments” on the left and then on the “create” button on the bottom:\n\nIn this window, all the existing environments are listed. As you can see in this image I already have a multitude of environments. Each environment was created for specific projects or tools I wanted to test without risking creating conflicts with other projects. On the right there is a list with packages that allows you to check what is installed in each environment.\nOnce you click on the “create” button, you will be prompted to provide some information. First you will have to fill in a name, this is the name that will appear in the list. Second you will need to pick a version of python to use. Any version between 3.9.19 and 3.12.2 should work (I have not tested the previous versions). We will proceed with version 3.12.2.\n\nAfter a while, the window should load and a green play button should appear next to the enviroment you just created. For the purpose of this tutorial I’ve called the environment “test”, it now has the play button next to it meaning it’s now the active environment. The packages on the right are the packages installed by default in the environment.\n\nClick on the home button on the top left to go back to the home screen:\n\nYou now have the option to install different interfaces to interact with python. For the puropose of this tutorial we will use jupyter. Click the install button below the logo to install.\n\nAfter a while the window should reload and you should now have a “launch” button below jupyter. Jupyter allows you to program in python in your internet browser of choice. This will work even if you’re not connected to the internet.\n\n\nSetting up Jupyter\nIt’s now time to install the packages we need to work with BERTopic. In anaconda you will have the choice between different code editors. For the purpose of this tutorial we will use Jupyter. Jupyter is a code editor that opens in your favorite internet browser (it does not require an internet connection to function). To start, click on the “Launch” button below Jupyter as shown below:\n\nYour navigator should appear with the following page open. On this page we are going to create a new notebook. A notebook is equivalent to a word document, but for programming. You will be able to save all your code in this file.\n\nOnce you have clicked on “Notebook” you will be prompted to select a Kernel. Different versions of python exist and different applications may require different versions of python. For our purpose 3 works. For other packages you might want to use this might be different. Always check the version required for the packages you are going to use.\n\nOnce you’ve selected this (or if you have already done this in the past) you will have the option to create directly a new notebook with this kernel:\n\nThis will open your first notebook. The first action we will take is change the name of the notebook. This can be done by clicking on “untitled1” next to the Jupyter logo on the top left of the page as shown below. You will then be prompted to provide a new name. Let’s give it the name “Bertopic_Tutorial”.\n\nSome basics on the strucute. Jupyter functions with blocks that you can execute one by one. This makes trouble shooting easier. The numbers next to the blocks show the order in which you ran the blocks. If there is no number the block has not yet been run, if there is a star (*) then the block is currently running. Blocks can be easily added and removed as you develop your code. Note that adding comments can be done by starting a line with a #.\n\n\n\nInstalling microsoft visual C++ build tools\nDownload and install C++ visual build tools:\nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/\n\n\nInstalling Bertopic\nTo use Bertopic we need a certain number of packages. We will first install the basic requirements. The installation of packages in Python is done with the pip command. Note that since we use anaconda, in a specific environment, the packages will be installed in this environment and hence will not be accessible from other environments. This has as a consequence that we sometimes have to install the same package multiple times in different environments.\nTo install bertopic in our environment use pip as shown below, between brackets you should see the * appear indicating that the code is running, and slowly you will see the different steps appearing underneath the code block:\n\nNow do the same for other packages:\n\npip install ipywidgets\npip install pandas\npip install spacy\n\nOnce all the packages are installed we reload the kernel so that everything will be available for use. Click on the circular arrow in the menu:\n\nWe should now be good to go!",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Installing Bertopic"
    ]
  },
  {
    "objectID": "DAFS_install_R.html",
    "href": "DAFS_install_R.html",
    "title": "Installing all the software",
    "section": "",
    "text": "For this course we use a combination of R and Rstudio. If we think of this as a car, then R is the engine, and rstudio is the carrosserie that makes many things easier and more comfortable. For Rstudio to work, we first need to install R.\n\n\n\nFor mac users: \nFor windows users, first click on “install R for the first time”\n\nThen download the latest version:\n\n\n\n\nHere you can download R studio. The download button should adapt directly to your operating system\n\nRun the installers and you should be good to go. You only need to launch Rstudio. You never need to launch R.\n\n\n\nThe interface has 4 panels, each with a purpose:\n\nOnce we start programming, we create variable and objects that are stored:\n\nWhen working on a project, we like to keep everything in the same place, your data, your scripts, R. This can be done with an Rproj file:\n This is how you create an rproj. Once you have created this, everytime you quit en relaunch R, all your data and scripts will be loaded.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Installing all the software"
    ]
  },
  {
    "objectID": "DAFS_install_R.html#go-to-httpsmirrors.evoluso.comcran",
    "href": "DAFS_install_R.html#go-to-httpsmirrors.evoluso.comcran",
    "title": "Installing all the software",
    "section": "",
    "text": "For mac users: \nFor windows users, first click on “install R for the first time”\n\nThen download the latest version:",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Installing all the software"
    ]
  },
  {
    "objectID": "DAFS_install_R.html#go-to-httpsposit.codownloadrstudio-desktop",
    "href": "DAFS_install_R.html#go-to-httpsposit.codownloadrstudio-desktop",
    "title": "Installing all the software",
    "section": "",
    "text": "Here you can download R studio. The download button should adapt directly to your operating system\n\nRun the installers and you should be good to go. You only need to launch Rstudio. You never need to launch R.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Installing all the software"
    ]
  },
  {
    "objectID": "DAFS_install_R.html#some-basics-on-rstudio",
    "href": "DAFS_install_R.html#some-basics-on-rstudio",
    "title": "Installing all the software",
    "section": "",
    "text": "The interface has 4 panels, each with a purpose:\n\nOnce we start programming, we create variable and objects that are stored:\n\nWhen working on a project, we like to keep everything in the same place, your data, your scripts, R. This can be done with an Rproj file:\n This is how you create an rproj. Once you have created this, everytime you quit en relaunch R, all your data and scripts will be loaded.",
    "crumbs": [
      "Home",
      "DAFS Programming module",
      "Installing all the software"
    ]
  },
  {
    "objectID": "Inno_Rproject.html",
    "href": "Inno_Rproject.html",
    "title": "R: Projects, Scripts and the console",
    "section": "",
    "text": "R: Projects, Scripts and the console\nCreating a project in R is essential for organizing and managing your work efficiently, especially when dealing with complex data analysis. A project in R allows you to keep all your files, scripts, data, and outputs in a single, well-structured directory. This structure ensures that your code is portable, reproducible, and easier to share with others. It also helps in maintaining a clear workflow by automatically setting the working directory, which reduces errors related to file paths. Ultimately, using an R project enhances collaboration, simplifies version control, and keeps your work environment tidy and focused.\nAn Rproject is created in a folder in which you can keep all the files related to your project. This allows you to keep all your material in the same place and makes importing and exporting data from and to R, easier."
  },
  {
    "objectID": "SeCo_in_Europe.html",
    "href": "SeCo_in_Europe.html",
    "title": "Semicondcutors in Europe",
    "section": "",
    "text": "Semicondcutors in Europe"
  },
  {
    "objectID": "Intro_programming_visualisation.html",
    "href": "Intro_programming_visualisation.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to visualize data\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data visualisation"
    ]
  },
  {
    "objectID": "Intro_programming_visualisation.html#data-visualisation",
    "href": "Intro_programming_visualisation.html#data-visualisation",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This chapter discusses how to visualize data\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data visualisation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html",
    "href": "Intro_programming_DataImportAndPrep.html",
    "title": "Importing in R",
    "section": "",
    "text": "How to import different dataformats in python and R.",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#data-import-and-preparation",
    "href": "Intro_programming_DataImportAndPrep.html#data-import-and-preparation",
    "title": "Importing in R",
    "section": "",
    "text": "How to import different dataformats in python and R.",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#txt-csv-tsv",
    "href": "Intro_programming_DataImportAndPrep.html#txt-csv-tsv",
    "title": "Importing in R",
    "section": "TXT, CSV, TSV",
    "text": "TXT, CSV, TSV\nFor the import of data from txt, csv or tsv formats we will use the read_delim() function from the readr package. The main difference between these data formats resides in the separator of the data. For txt data, this is often undefined and needs to be specified by the user. The main arguments of the read_delim() are the file and the separator. The separator is the character string that defines where a line should be cut. For example, if our raw data looks like this:\nYear;Class;Value 2002;B60C;42 2003;B29K;21 2009;C08K;12\nThen we see that each column is separated by a “;”. By splitting the data whenever there is a “;” we create the following dataframe:\n\n\n\nYear\nClass\nValue\n\n\n\n\n2002\nB60C\n42\n\n\n2003\nB29K\n21\n\n\n2009\nC08K\n12\n\n\n\nSeperators that we commonly find in data are the semicolon: “;”, the comma: “,”, the vertical bar (or pipe) “|”, the space **” “, and tabs which are coded with**”\\t”.\n\n\n\n\n\n\nPay Attention\n\n\n\nEven though csv stands for Comma Separated Values, this does not mean that the separator is always a comma, often enough it is in fact a semicolon. Always check your data to make sure you have the right one.\n\n\n\nlibrary(readr)\n# csv and various other separators\ndata &lt;- read_delim(file, sep = \",\")\n\ndata &lt;- read_delim(file, sep = \"|\")\n\ndata &lt;- read_delim(file, sep = \";\")\n\n# txt (space separated values)\ndata &lt;- read_delim(file, sep = \" \")\n\n# tsv (tab separated values)\ndata &lt;- read_delim(file, sep = \"\\t\")",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#xls-xlsx",
    "href": "Intro_programming_DataImportAndPrep.html#xls-xlsx",
    "title": "Importing in R",
    "section": "XLS, XLSX",
    "text": "XLS, XLSX\nExcel files can also easily be importanted into R with the the readxl package.\n\nlibrary(readxl)\nread_excel(\"sample_data.xlsx\")\n\n# A tibble: 5 × 3\n  Pat_num       Year Domain\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; \n1 WO200214562   2002 B60C  \n2 WO2023738962  2023 B60C  \n3 EP2023778962  2023 B29D  \n4 FR2019272698  2019 C08K  \n5 FR201922671   2019 C08K",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#rdata",
    "href": "Intro_programming_DataImportAndPrep.html#rdata",
    "title": "Importing in R",
    "section": "rdata",
    "text": "rdata\n\n\n# use import function in rstudio, or double click",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#bib",
    "href": "Intro_programming_DataImportAndPrep.html#bib",
    "title": "Importing in R",
    "section": "Bib",
    "text": "Bib\n.bib files are a popular bibliographic data format. This data can be imported and transformed into a dataframe using the bibliometrix package.\n\nlibrary(bibliometrix)\nconvert2df(file, dbsource = \"scopus\", format = \"bibtex\")\n\n\n\n\n\n\n\nPay Attention\n\n\n\nThe bibliometrix package is designed for bibliometic analysis, it might change the names of the columns and the format of some of the data to adjust to what it is supped to do.",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#json",
    "href": "Intro_programming_DataImportAndPrep.html#json",
    "title": "Importing in R",
    "section": "json",
    "text": "json\n\nlibrary(jsonlite)\n\n# Specify the path to the JSON file\njson_file &lt;- \"example_1.json\"\n\n# Import the JSON file\ndata &lt;- fromJSON(json_file)\n\n# Display the imported JSON data\nprint(data)\n\n$fruit\n[1] \"Apple\"\n\n$size\n[1] \"Large\"\n\n$color\n[1] \"Red\"",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#jsonl",
    "href": "Intro_programming_DataImportAndPrep.html#jsonl",
    "title": "Importing in R",
    "section": "jsonl",
    "text": "jsonl\njsonl files are a list of json files. We find this format for exampl in the lens.org database. Since each row is a json, we first read the files as text files and then apply the fromJSON function to extract the information. The result is a list of data objects.\n\nlibrary(jsonlite)\ntmp&lt;-readLines(\"data_test.jsonl\")\n\nWarning in readLines(\"data_test.jsonl\"): incomplete final line found on\n'data_test.jsonl'\n\ntmp &lt;- lapply(tmp, jsonlite::fromJSON)\n\n\n\nClick here to see output\n\n\nprint(tmp)\n\n[[1]]\n[[1]]$lens_id\n[1] \"000-136-263-451-165\"\n\n[[1]]$title\n[1] \"The influence of displaced femoral neck fractures on the surgical incision for hemiarthroplasty using the Hardinge approach\"\n\n[[1]]$publication_type\n[1] \"journal article\"\n\n[[1]]$year_published\n[1] 2002\n\n[[1]]$date_published_parts\n[1] 2002    1\n\n[[1]]$created\n[1] \"2018-05-12T00:07:16.368000+00:00\"\n\n[[1]]$external_ids\n   type              value\n1 magid         2083116342\n2   doi 10.1007/bf03170381\n\n[[1]]$authors\n  first_name last_name initials               ids\n1         M.    Yousef        M magid, 2518579655\n2         E. Masterson        E magid, 2607514729\n                                                                                                                                                                                    affiliations\n1 Mid-Western Regional Hospital, Department of Orthopaedic Surgery, Mid-Western Regional Hospital, Limerick, Ireland., magid, grid, ror, 2799956115, grid.415964.b, 00r5kgd36, grid.415964.b, IE\n2 Mid-Western Regional Hospital, Department of Orthopaedic Surgery, Mid-Western Regional Hospital, Limerick, Ireland., magid, grid, ror, 2799956115, grid.415964.b, 00r5kgd36, grid.415964.b, IE\n\n[[1]]$source\n[[1]]$source$title\n[1] \"Irish Journal of Medical Science\"\n\n[[1]]$source$type\n[1] \"Journal\"\n\n[[1]]$source$publisher\n[1] \"Springer Science and Business Media LLC\"\n\n[[1]]$source$issn\n        type    value\n1      print 00211265\n2 electronic 18634362\n3      print 03321029\n4      print 0790231x\n5      print 07902328\n\n[[1]]$source$country\n[1] \"Ireland\"\n\n[[1]]$source$asjc_codes\n[1] \"2700\"\n\n[[1]]$source$asjc_subjects\n[1] \"General Medicine\"\n\n\n[[1]]$fields_of_study\n[1] \"Surgery\"                \"Surgical incision\"      \"Hardinge approach\"     \n[4] \"Femoral Neck Fractures\" \"Medicine\"              \n\n[[1]]$volume\n[1] \"171\"\n\n[[1]]$issue\n[1] \"1\"\n\n[[1]]$languages\n[1] \"en\"\n\n[[1]]$source_urls\n  type                                                          url\n1 html         https://link.springer.com/article/10.1007/BF03170381\n2 &lt;NA&gt; https://link.springer.com/content/pdf/10.1007/BF03170381.pdf\n\n[[1]]$start_page\n[1] \"31\"\n\n[[1]]$end_page\n[1] \"31\"\n\n[[1]]$author_count\n[1] 2\n\n\n[[2]]\n[[2]]$lens_id\n[1] \"000-150-493-965-760\"\n\n[[2]]$title\n[1] \"Peroxidase activity in soybeans following inoculation with Phytophthora sojae.\"\n\n[[2]]$publication_type\n[1] \"journal article\"\n\n[[2]]$year_published\n[1] 2006\n\n[[2]]$date_published_parts\n[1] 2006    1\n\n[[2]]$created\n[1] \"2018-05-12T03:57:41.820000+00:00\"\n\n[[2]]$external_ids\n   type                     value\n1   doi 10.1007/s11046-005-0721-y\n2  pmid                  16389483\n3 magid                2006874664\n\n[[2]]$authors\n  first_name last_name initials               ids\n1    Jose C.    Melgar       JC magid, 2634470543\n2  Thomas S.     Abney       TS magid, 2677788953\n3 Richard A.  Vierling       RA magid, 1981461697\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       affiliations\n1                                                                                                                                                                                                                                                                      Purdue University, Botany and Plant Pathology Department, Purdue University, West Lafayette, IN 47907, USA., magid, grid, fundref, wikidata, ror, 219193219, grid.169077.e, 100006377, q217741, 02dqehb95, grid.169077.e, US\n2 United States Department of Agriculture, Agricultural Research Service, Crop Production and Pathology Unit, Agricultural Research Service, U.S. Department of Agriculture, West Lafayette, USA, Crop Production and Pathology Unit, Agricultural Research Service, U.S. Department of Agriculture, West Lafayette, USA, magid, grid, fundref, ror, 1336096307, grid.417548.b, 100000199, 01na82s61, grid, fundref, ror, grid.463419.d, 100007917, 02d2m2044, grid.417548.b, grid.463419.d, US, US\n3                                                                                                                                                                                                                                                           Purdue University, Botany & Plant Pathology Department and Agronomy Department, Purdue University, West Lafayette, USA, magid, grid, fundref, wikidata, ror, 219193219, grid.169077.e, 100006377, q217741, 02dqehb95, grid.169077.e, US\n\n[[2]]$source\n[[2]]$source$title\n[1] \"Mycopathologia\"\n\n[[2]]$source$type\n[1] \"Journal\"\n\n[[2]]$source$publisher\n[1] \"Springer Science and Business Media LLC\"\n\n[[2]]$source$issn\n        type    value\n1      print 0301486x\n2 electronic 15730832\n\n[[2]]$source$country\n[1] \"Netherlands\"\n\n[[2]]$source$asjc_codes\n[1] \"2404\" \"2402\" \"1110\" \"1102\" \"3401\"\n\n[[2]]$source$asjc_subjects\n[1] \"Agronomy and Crop Science\"             \n[2] \"Applied Microbiology and Biotechnology\"\n[3] \"Microbiology\"                          \n[4] \"Plant Science\"                         \n[5] \"Veterinary (miscalleneous)\"            \n[6] \"Veterinary (miscellaneous)\"            \n\n\n[[2]]$fields_of_study\n[1] \"Gene\"               \"Horticulture\"       \"Peroxidase\"        \n[4] \"Inoculation\"        \"Phytophthora sojae\" \"Phytophthora\"      \n[7] \"Botany\"             \"Biology\"           \n\n[[2]]$volume\n[1] \"161\"\n\n[[2]]$issue\n[1] \"1\"\n\n[[2]]$languages\n[1] \"en\"\n\n[[2]]$references\n               lens_id\n1  004-042-571-377-321\n2  005-317-066-947-203\n3  020-042-216-048-371\n4  032-446-103-377-888\n5  034-601-439-886-731\n6  062-700-560-648-136\n7  066-443-980-641-011\n8  088-133-232-334-221\n9  106-456-828-269-00X\n10 168-522-110-138-272\n11 192-341-655-765-695\n\n[[2]]$mesh_terms\n    mesh_heading       qualifier_name mesh_id qualifier_id\n1    Peroxidases           metabolism D010544      Q000378\n2   Phytophthora growth & development D010838      Q000254\n3 Plant Diseases         microbiology D010935      Q000382\n4 Plant Proteins           metabolism D010940      Q000378\n5      Seedlings           enzymology D036226      Q000201\n6       Soybeans           enzymology D013025      Q000201\n\n[[2]]$chemicals\n  substance_name registry_number mesh_id\n1 Plant Proteins               0 D010940\n2    Peroxidases     EC 1.11.1.- D010544\n\n[[2]]$source_urls\n  type                                                         url\n1 html https://link.springer.com/article/10.1007/s11046-005-0721-y\n2 html                https://www.ncbi.nlm.nih.gov/pubmed/16389483\n3 html                  https://europepmc.org/article/MED/16389483\n\n[[2]]$references_count\n[1] 11\n\n[[2]]$scholarly_citations_count\n[1] 12\n\n[[2]]$start_page\n[1] \"37\"\n\n[[2]]$end_page\n[1] \"42\"\n\n[[2]]$scholarly_citations\n [1] \"027-668-154-909-322\" \"031-369-742-281-943\" \"034-566-544-087-375\"\n [4] \"041-477-983-598-256\" \"041-552-411-498-281\" \"042-968-597-273-042\"\n [7] \"045-790-949-621-105\" \"062-868-088-589-29X\" \"107-788-244-832-10X\"\n[10] \"132-744-976-798-754\" \"153-187-658-237-284\" \"160-817-571-772-644\"\n\n[[2]]$author_count\n[1] 3\n\n\n[[3]]\n[[3]]$lens_id\n[1] \"000-161-224-328-417\"\n\n[[3]]$title\n[1] \"Astronomy: Planets in chaos\"\n\n[[3]]$publication_type\n[1] \"journal article\"\n\n[[3]]$year_published\n[1] 2014\n\n[[3]]$date_published\n[1] \"2014-07-02T00:00:00.000000+00:00\"\n\n[[3]]$date_published_parts\n[1] 2014    7    2\n\n[[3]]$created\n[1] \"2018-05-12T07:57:28.771000+00:00\"\n\n[[3]]$external_ids\n   type           value\n1 magid      2014980602\n2  pmid        24990727\n3   doi 10.1038/511022a\n\n[[3]]$open_access\n[[3]]$open_access$colour\n[1] \"bronze\"\n\n\n[[3]]$authors\n  first_name  last_name initials               ids affiliations\n1        Ann Finkbeiner        A magid, 2633978770         NULL\n\n[[3]]$source\n[[3]]$source$title\n[1] \"Nature\"\n\n[[3]]$source$type\n[1] \"Journal\"\n\n[[3]]$source$publisher\n[1] \"Springer Science and Business Media LLC\"\n\n[[3]]$source$issn\n        type    value\n1 electronic 14764687\n2      print 00280836\n\n[[3]]$source$country\n[1] \"United Kingdom\"\n\n[[3]]$source$asjc_codes\n[1] \"1000\"\n\n[[3]]$source$asjc_subjects\n[1] \"Multidisciplinary\"\n\n\n[[3]]$fields_of_study\n[1] \"Exoplanetology\"           \"Physics\"                 \n[3] \"Astronomy\"                \"Star (graph theory)\"     \n[5] \"Planetary science\"        \"CHAOS (operating system)\"\n[7] \"Planet\"                   \"Astrobiology\"            \n\n[[3]]$volume\n[1] \"511\"\n\n[[3]]$issue\n[1] \"7507\"\n\n[[3]]$languages\n[1] \"en\"\n\n[[3]]$references\n              lens_id\n1 002-401-641-595-121\n2 002-872-990-703-478\n3 031-880-830-498-497\n4 082-960-570-095-64X\n5 149-177-951-839-803\n\n[[3]]$source_urls\n     type                                                           url\n1 unknown                   https://www.nature.com/articles/511022a.pdf\n2    html                       https://www.nature.com/articles/511022a\n3    html http://ui.adsabs.harvard.edu/abs/2014Natur.511...22F/abstract\n4    html                    https://europepmc.org/article/MED/24990727\n\n[[3]]$abstract\n[1] \"The discovery of thousands of star systems wildly different from our own has demolished ideas about how planets form. Astronomers are searching for a whole new theory.\"\n\n[[3]]$references_count\n[1] 5\n\n[[3]]$scholarly_citations_count\n[1] 3\n\n[[3]]$start_page\n[1] \"22\"\n\n[[3]]$end_page\n[1] \"24\"\n\n[[3]]$scholarly_citations\n[1] \"020-515-110-880-837\" \"084-952-180-195-058\" \"086-879-116-036-663\"\n\n[[3]]$author_count\n[1] 1\n\n[[3]]$is_open_access\n[1] TRUE",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#txt-csv-tsv-1",
    "href": "Intro_programming_DataImportAndPrep.html#txt-csv-tsv-1",
    "title": "Importing in R",
    "section": "TXT, CSV, TSV",
    "text": "TXT, CSV, TSV\npackage required openpyxl, pandas\n\nimport pandas as pd\n\n# Import CSV File\ncsv_file = \"sample_data.csv\"\ndf_csv = pd.read_csv(csv_file, sep = \";\")\n\n\n# Import TXT File (Space-Separated)\ntxt_file = \"sample_data.txt\"\ndf_txt = pd.read_csv(txt_file, sep=\" \")\n\n# Import TSV File (Tab-Separated)\ntsv_file = \"sample_data.tsv\"\ndf_tsv = pd.read_csv(tsv_file, sep=\"\\t\")\n\n\n# Import XLS (Excel) File\nxls_file = \"sample_data.xlsx\"\ndf_xls = pd.read_excel(xls_file)\nprint(df_xls)\n\n# Import JSON File\n#json_file = \"sample_data.json\"\n#df_json = pd.read_json(json_file)",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#xlsxlsx-excel-data-format",
    "href": "Intro_programming_DataImportAndPrep.html#xlsxlsx-excel-data-format",
    "title": "Importing in R",
    "section": "XLS/XLSX (excel data format)",
    "text": "XLS/XLSX (excel data format)\nExcel files can be imported with help from the pandas package. If you do not specify a sheet to import, the first sheet will be taken. You can specify the sheet with the argument: sheet_name=‘Sheet1’\n\nimport pandas as pd\n# Import XLS (Excel) File\ndf_xls = pd.read_excel(\"sample_data.xlsx\")\nprint(df_xls)",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#json-and-jsonl",
    "href": "Intro_programming_DataImportAndPrep.html#json-and-jsonl",
    "title": "Importing in R",
    "section": "Json and jsonl",
    "text": "Json and jsonl\n\n# Import JSON File\n#json_file = \"sample_data.json\"\n#df_json = pd.read_json(json_file)\n\n\n# import jsonl\nimport jsonlines\n\n# First we open the file and provide it with a name (data_from_jsonl)\nwith jsonlines.open('data_test.jsonl', 'r') as data_from_jsonl:\n      # extract the information from each row and put into object\n     results = [row for row in data_from_jsonl] \ndata_from_jsonl.close() # close the file again",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#data-conformity-in-r",
    "href": "Intro_programming_DataImportAndPrep.html#data-conformity-in-r",
    "title": "Importing in R",
    "section": "Data conformity in R",
    "text": "Data conformity in R\nBefore you start working on your analysis, you want to make sure the data is conform to your expectations. This means:\n\nThe number of observation is correct, and the variables you need are included.\nMaking sure each column is of the right type (years are dates, text are factors, numbers are integers or reals, …)\nThere are no trailing spaces\n“.” and “,” aren’t mixed up\nThe units of measurement are correct (km vs. miles, K€ vs 1.000, celsius vs fahrenheit etc.)\nAny missing values have been dealt with, or you have a valid reason to ignore them\nheaders are easy to use (no spaces or other characters that will make your life difficult).\n\nIn this section we will look into checking your data and adjusting it in preparation of your analysis.\n\nChecking data types\nThere are many different data formats. Most frequently we encounter numbers, strings, factors, dates and booleans. Consider the following dataset:\n\n\n\nID\nYear\nCool Domain\nFrequency\nAlive\n\n\n\n\nID1\n2002\nB60C\n42\nTRUE\n\n\nID2\n2003\nB29K\n21\nFALSE\n\n\nID3\n2009\nC08K\n12\nTRUE\n\n\n\nThe first column has textual data that is used as a key for each row. The Year should be considered either as an integer or a date. This will depend upon the type of analysis you want to perform. Specifically time series analysis will require this to be a date, while it’s enough for other analyses to ensure that it is an integer. The most common issue with this type of observation is that the year is considered as text instead of a number. If you see any ““” next to your numbers this is a bad sign which indicates that it is in fact, text.\nThe “Cool Domain” contains classifications which we want to be textual values. “Frequency” will need to be an integer while “Alive” we want to be a boolean. For each of the formats there is a function to test whether or not the format is the one we expect. These functions are usually in the form of is.numerical(), is.logical(), is.character(). The argument is then simply the column of the dataframe or an observation. Suppose we create the previous dataframe in R:\n\ndata &lt;- data.frame(ID = c(\"ID1\", \"ID2\", \"ID3\"), Year = c(2002, 2003, 2009), `Cool Domain` = c(\"B60C\", \"B29K\",\"B29K\"), Frequency = c(42, 21, 12),Alive = c(TRUE, FALSE, TRUE))\n\n\nprint(data)\n\n   ID Year Cool.Domain Frequency Alive\n1 ID1 2002        B60C        42  TRUE\n2 ID2 2003        B29K        21 FALSE\n3 ID3 2009        B29K        12  TRUE\n\n\nWe can then check the format of each variables:\n\nis.numeric(data$Frequency)\n\n[1] TRUE\n\nis.character(data$ID)\n\n[1] TRUE\n\nis.logical(data$Alive)\n\n[1] TRUE\n\n\nThis can also be done in a more consise way with the summary() function. This compute, for each of the columns, some statistics and provides the class of the class of the variables. This comes in handy as it allows us verify not only the class of the variables but also the distribution:\n\nsummary(data)\n\n      ID                 Year      Cool.Domain          Frequency   \n Length:3           Min.   :2002   Length:3           Min.   :12.0  \n Class :character   1st Qu.:2002   Class :character   1st Qu.:16.5  \n Mode  :character   Median :2003   Mode  :character   Median :21.0  \n                    Mean   :2005                      Mean   :25.0  \n                    3rd Qu.:2006                      3rd Qu.:31.5  \n                    Max.   :2009                      Max.   :42.0  \n   Alive        \n Mode :logical  \n FALSE:1        \n TRUE :2        \n                \n                \n                \n\n\nWhen the variable is a character, the output shows the length (number of observations) and the class. For numeric variables the output shows the distribution of the data (min, max, etc.). Not all text variables are created equal. It is possible to ensure that text is considered as a factor. A factor is basically a category, dummy variables in regressions are usually programmed as factors.\n\ndata2 &lt;- data.frame(ID = c(\"ID1\", \"ID2\", \"ID3\", \"ID4\"), Year = c(2002, 2003, 2009, 2010), `Cool Domain` = c(\"B60C\", \"B29K\",\"B29K\", \"B60C\"), Frequency = c(42, 21, 12, NA),Alive = c(TRUE, FALSE, FALSE, FALSE), Country = as.factor(c(\"France\", \"France\", \"Germany\", \"Netherlands\")))\n\nsummary(data2)\n\n      ID                 Year      Cool.Domain          Frequency   \n Length:4           Min.   :2002   Length:4           Min.   :12.0  \n Class :character   1st Qu.:2003   Class :character   1st Qu.:16.5  \n Mode  :character   Median :2006   Mode  :character   Median :21.0  \n                    Mean   :2006                      Mean   :25.0  \n                    3rd Qu.:2009                      3rd Qu.:31.5  \n                    Max.   :2010                      Max.   :42.0  \n                                                      NA's   :1     \n   Alive                Country \n Mode :logical   France     :2  \n FALSE:3         Germany    :1  \n TRUE :1         Netherlands:1  \n                                \n                                \n                                \n                                \n\n\nIn this output we see that the NA is taken into account and counted separedly in the date. It also shows that “Country” is considered as a factor. The summary then shows how many observations we have for each of the factors.\nGiven that we created the dataframe ourselves we can be relatively sure that the data was is the right format. However, often enough when we download data from online sources, we run into issues. If the format is not as expected there are different functions that allows us to transform the format of the data. Mainly we use as.numeric(), as.character() and as.logical(). These functions require only one argument which is the column we are trying to convert:\n\n# transform into a numeric value\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\n# transform in to a character string\ndata$Frequency &lt;- as.character(data$ID)\n\n# transform into a boolean\ndata$Frequency &lt;- as.logical(data$Alive)\n\nAs an example consider we will create a dataframe that mimics data that was not imported correctly:\n\nMessy_data &lt;- data.frame(ID = c(\"ID1\", \"ID2\", \"ID3\", \"ID4\"), Year = c(\"2002\", \"2003\", \"2009\", \"2010\"), `Cool Domain` = c(\"B60C\", \"B29K\",\"B29K\", \"B60C\"), Frequency = c(\"42\", \"21\", \"12\", \"NA\"),Alive = c(\"TRUE\", \"FALSE\", \"FALSE\", \"FALSE\"), Country = c(\"France\", \"France\", \"Germany\", \"Netherlands\"))\n\nsummary(Messy_data)\n\n      ID                Year           Cool.Domain         Frequency        \n Length:4           Length:4           Length:4           Length:4          \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n    Alive             Country         \n Length:4           Length:4          \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\nThe output clearly shows that all variables are considered as characters which we do not want. We cannot comput the average on the Frequency while R thinks that it is text. This also shows that there is a difference between the string NA (“NA”) and the operator NA.\n\n# transform into a numerical value\nMessy_data$Year &lt;- as.numeric(Messy_data$Year)\nMessy_data$Frequency &lt;- as.numeric(Messy_data$Frequency)\n\nWarning: NAs introduced by coercion\n\n# transform into a boolean\nMessy_data$Alive &lt;- as.logical(Messy_data$Alive)\n# transform into a factor (category)\nMessy_data$Country &lt;- as.factor(Messy_data$Country)\n\nsummary(Messy_data)\n\n      ID                 Year      Cool.Domain          Frequency   \n Length:4           Min.   :2002   Length:4           Min.   :12.0  \n Class :character   1st Qu.:2003   Class :character   1st Qu.:16.5  \n Mode  :character   Median :2006   Mode  :character   Median :21.0  \n                    Mean   :2006                      Mean   :25.0  \n                    3rd Qu.:2009                      3rd Qu.:31.5  \n                    Max.   :2010                      Max.   :42.0  \n                                                      NA's   :1     \n   Alive                Country \n Mode :logical   France     :2  \n FALSE:3         Germany    :1  \n TRUE :1         Netherlands:1  \n                                \n                                \n                                \n                                \n\n\nNow that our data has the right format and we are sure we have all the observations we might want to do some work to normalise the column names. This is done to make programming and refercing the columns easier. Whenever the is a space in the name of the column, we need to refer to the column using `Column name with spaces` which is not ideal. It’s generally good practice to not have any spaces. We can use the colnames() function to change some (or all of the column names).\n\n# change on column name:\ncolnames(Messy_data)[3] &lt;- \"Domain\"\n\n# change multiple names:\ncolnames(Messy_data)[c(1,2,5)] &lt;- c(\"Identifier\", \"Date\", \"Active\")\n\n# check\nsummary(Messy_data)\n\n  Identifier             Date         Domain            Frequency   \n Length:4           Min.   :2002   Length:4           Min.   :12.0  \n Class :character   1st Qu.:2003   Class :character   1st Qu.:16.5  \n Mode  :character   Median :2006   Mode  :character   Median :21.0  \n                    Mean   :2006                      Mean   :25.0  \n                    3rd Qu.:2009                      3rd Qu.:31.5  \n                    Max.   :2010                      Max.   :42.0  \n                                                      NA's   :1     \n   Active               Country \n Mode :logical   France     :2  \n FALSE:3         Germany    :1  \n TRUE :1         Netherlands:1  \n                                \n                                \n                                \n                                \n\n\nThe Janitor package offers an automated solution for many header issues.\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nMessy_data &lt;- data.frame(ID = c(\"ID1\", \"ID2\", \"ID3\", \"ID4\"), Year = c(\"2002\", \"2003\", \"2009\", \"2010\"), `Cool Domain` = c(\"B60C\", \"B29K\",\"B29K\", \"B60C\"), Frequency = c(\"42\", \"21\", \"12\", \"NA\"),Alive = c(\"TRUE\", \"FALSE\", \"FALSE\", \"FALSE\"), Country = c(\"France\", \"France\", \"Germany\", \"Netherlands\"))\n# the column names from the initial dataframe\nprint(colnames(Messy_data))\n\n[1] \"ID\"          \"Year\"        \"Cool.Domain\" \"Frequency\"   \"Alive\"      \n[6] \"Country\"    \n\n# Cleaning the column names\nMessy_data &lt;- clean_names(Messy_data)\nprint(colnames(Messy_data))\n\n[1] \"id\"          \"year\"        \"cool_domain\" \"frequency\"   \"alive\"      \n[6] \"country\"    \n\n\nYou can see that capital letter have been adjusted, some characters (µ, “)” ) have been replaced, spaces have been removed and replaced with “_”. This makes it easier and faster to refer to the columns and the dataframes when manipulating the data on a day-to-day basis.\nOnce we are happy with the data formats we can move on take care of other issues.\n\nData transformation\nIt often happens that we are not fully happy with the data that we receive or simply that we need apply some modifications to suit our needs. The most frequent of these operations are\n\nMathematical transformation of numerical data (multiplication, rounding up)\nExtract years, days or months from dates\nTrim whitespaces from data\nRename categories within the data\n\nThe main difficulty with the actions is to identify how to apply the functions. Some functions can be applied directly to a whole column while other nedd to be applied using mapping functions. Let’s start with mathematical transformations:\n\n# we start by creating a dataset to work with\ndata &lt;- data.frame(Identifier = c(\"ID1\", \"ID2\", \"ID3\", \"ID4 \"), Date = c(2002, 2003, 2009, 2010), Domain = c(\"B60C\", \"B29K\",\"B29K \", \"B60C\"), Distance = c(62.1371, 124.274, 93.2057, 186.411), Active = c(TRUE, FALSE, FALSE, FALSE), Country = as.factor(c(\"France\", \"France\", \"Germany\", \"Netherlands\")))\n\n# The Distance is given in miles in this data, we would like to have kilometers. The formula to transform a mile to kilometers is a multiplication by 1.609.\n\ndata$KM &lt;- data$Distance * 1.609344\n\nprint(data)\n\n  Identifier Date Domain Distance Active     Country        KM\n1        ID1 2002   B60C  62.1371   TRUE      France  99.99997\n2        ID2 2003   B29K 124.2740  FALSE      France 199.99962\n3        ID3 2009  B29K   93.2057  FALSE     Germany 150.00003\n4       ID4  2010   B60C 186.4110  FALSE Netherlands 299.99942\n\n\nWe can also round this up. The round() function takes two arguments, a first will be the data, the second will be the number of decimals:\n\ndata$KM &lt;- round(data$KM, 2)\nprint(data)\n\n  Identifier Date Domain Distance Active     Country  KM\n1        ID1 2002   B60C  62.1371   TRUE      France 100\n2        ID2 2003   B29K 124.2740  FALSE      France 200\n3        ID3 2009  B29K   93.2057  FALSE     Germany 150\n4       ID4  2010   B60C 186.4110  FALSE Netherlands 300\n\n\nWe notice that for some of the domains there is a trailing whitespace, for example we have “B29K” in the data. These can be removed with the trimws() function. These whitespaces can also be created when we split data. It’s good practise to always check this and remove any space to avoid unnecessary mistakes.\n\ndata$Domain &lt;- trimws(data$Domain)\nprint(data)\n\n  Identifier Date Domain Distance Active     Country  KM\n1        ID1 2002   B60C  62.1371   TRUE      France 100\n2        ID2 2003   B29K 124.2740  FALSE      France 200\n3        ID3 2009   B29K  93.2057  FALSE     Germany 150\n4       ID4  2010   B60C 186.4110  FALSE Netherlands 300\n\n\nSuppose that we want to change the labels for the countries. It’s much more common to use ISO-2 level codes for countries. The gsub() function for the replacement of text. The gsub() function requires three arguments: the patter to search, the pattern to replace with and the data. Caution while using this function since it searches for the pattern even withing words. If one searches for the string “land” to replace it with “country”, then the string “netherlands” will become “nethercountrys”.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n  data$Country &lt;- gsub(\"France\", \"FR\", data$Country)\n  data$Country &lt;- gsub(\"Netherlands\" , \"NL\",data$Country)\n  data$Country &lt;- gsub(\"Germany\" ,\"DE\",data$Country)\n\nprint(data)\n\n  Identifier Date Domain Distance Active Country  KM\n1        ID1 2002   B60C  62.1371   TRUE      FR 100\n2        ID2 2003   B29K 124.2740  FALSE      FR 200\n3        ID3 2009   B29K  93.2057  FALSE      DE 150\n4       ID4  2010   B60C 186.4110  FALSE      NL 300\n\n\nIn the special case of categorical variables, we can also use the dplyr package to replace strings using the case_when function. It is a flexible and powerful function that allows you to perform conditional operations on a vector or data frame, similar to a series of nested if-else statements. It’s especially useful when you need to create or modify variables in your data based on specific conditions. The syntax follows:\n\ncase_when(\n  condition1 ~ result1,\n  condition2 ~ result2,\n  condition3 ~ result3,\n  ...\n)\n\nIn the case of changing categories, if we want to switch the name of the country for the ISO identifyers of the countries:\n\nlibrary(tidyverse)\ndata &lt;- data.frame(Identifier = c(\"ID1\", \"ID2\", \"ID3\", \"ID4 \"), Date = c(2002, 2003, 2009, 2010), Domain = c(\"B60C\", \"B29K\",\"B29K \", \"B60C\"), Distance = c(62.1371, 124.274, 93.2057, 186.411), Active = c(TRUE, FALSE, FALSE, FALSE), Country = as.factor(c(\"France\", \"France\", \"Germany\", \"Netherlands\")))\n\ndata$Country &lt;- case_when(\n  data$Country == \"France\" ~ \"FR\",\n  data$Country == \"Netherlands\" ~ \"NL\",\n  data$Country == \"Germany\" ~ \"DE\"\n)\n\nprint(data)\n\n  Identifier Date Domain Distance Active Country\n1        ID1 2002   B60C  62.1371   TRUE      FR\n2        ID2 2003   B29K 124.2740  FALSE      FR\n3        ID3 2009  B29K   93.2057  FALSE      DE\n4       ID4  2010   B60C 186.4110  FALSE      NL\n\n\n\n\n\nExtract substrings\n\n\nMissing values\nIt is common to encounter missing values in your data. There are multiple methods to deal with missing data ranging for simply removing them to using statistical methods to replace the missing values with other values. Whatever you decide to do with your missing data, think thoroughly about the consequences. Always consider why the data is missing, are the missing observations random or is there a pattern to the missing observations? For example if, suppose we are working on a dataset with pollution measurements around the world. It’s possible that missing values come from countries in a specific region of the world. Removing these observations will result in excluding a whole region from the analysis. If the missing observations are more randomly dispersed over different regions in the world this might be less of an issue. The first step to take when dealing with missing values is to check for patterns in the data. Based on the type of missing values you are dealing with, you can decide to on the most efficient method to treat them:\n\nRemove the missing values\nLinear interpolation\nK-means\n\nThis book on missing values and This book with a chapter on how to handle this king of data",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "Intro_programming_DataImportAndPrep.html#data-conformity-in-python",
    "href": "Intro_programming_DataImportAndPrep.html#data-conformity-in-python",
    "title": "Importing in R",
    "section": "Data conformity in Python",
    "text": "Data conformity in Python\nBefore you start working on your analysis, you want to make sure the data is conform to your expectations. This means:\n\nThe number of observation is correct, and the variables you need are included.\nMaking sure each column is of the right type (years are dates, text are factors, numbers are integers or reals, …)\nThere are no trailing spaces\n“.” and “,” aren’t mixed up\nThe units of measurement are correct (km vs. miles, K€ vs 1.000, celsius vs fahrenheit etc.)\nAny missing values have been dealt with, or you have a valid reason to ignore them\nheaders are easy to use (no spaces or other characters that will make your life difficult).\n\nIn this section we will look into checking your data and adjusting it in preparation of your analysis.\n\nChecking data types\nThere are many different data formats. Most frequently we encounter numbers, strings, factors, dates and booleans. Consider the following dataset:\n\n\n\nID\nYear\nCool Domain\nFrequency\nAlive\n\n\n\n\nID1\n2002\nB60C\n42\nTRUE\n\n\nID2\n2003\nB29K\n21\nFALSE\n\n\nID3\n2009\nC08K\n12\nTRUE\n\n\n\nThe first column has textual data that is used as a key for each row. The Year should be considered either as an integer or a date. This will depend upon the type of analysis you want to perform. Specifically time series analysis will require this to be a date, while it’s enough for other analyses to ensure that it is an integer. The most common issue with this type of observation is that the year is considered as text instead of a number. If you see any ““” next to your numbers this is a bad sign which indicates that it is in fact, text.\nThe “Cool Domain” contains classifications which we want to be textual values. “Frequency” will need to be an integer while “Alive” we want to be a boolean. For each of the formats there is a function to test whether or not the format is the one we expect. These functions are usually in the form of is_numeric_dtype(), is_bool_dtype(), is_string_dtype(). The argument is then simply the column of the dataframe or an observation. Suppose we create the previous dataframe in Python:\n\nimport pandas as pd\ndata = {\n  'ID' : [\"ID1\", \"ID2\", \"ID3\"], \n  'Year' : [2002, 2003, 2009], \n  'Cool Domain' : [\"B60C\", \"B29K\",\"B29K\"], \n  'Frequency' : [42, 21, 12],\n  'Alive' : [True, False, True]\n}\n\ndataframe = pd.DataFrame(data)\n# check if the information in the column Year is numeric\npd.api.types.is_numeric_dtype(dataframe['Year'])\n\nTrue\n\n# check if the information in the column is logic\npd.api.types.is_bool_dtype(dataframe['Alive'])\n\nTrue\n\n# check if the information in the column is character\npd.api.types.is_string_dtype(dataframe['ID'])\n\nTrue\n\n\n\npd: This is an alias for the pandas library, which is a popular data manipulation library in Python. The alias is used for convenience.\napi: This is the namespace within the pandas library where various utility functions and types are organized. It’s considered part of the public API of the library, which means that it’s stable and can be used by developers without concerns about major changes between different versions of pandas.\ntypes: This is a subnamespace within the api namespace that provides functions and types related to data types and data type checking.\nis_numeric_dtype(): This is a function provided by the pandas library in the types subnamespace. It’s used to check if a given data type is numeric.\n\nThis can also be done in a more consise way with the dataframe.describe() function. This compute, for each of the columns, some statistics and provides the class of the class of the variables. This comes in handy as it allows us verify not only the class of the variables but also the distribution:\n\ndata = {\n  'ID' : [\"ID1\", \"ID2\", \"ID3\"], \n  'Year' : [2002, 2003, 2009], \n  'Cool Domain' : [\"B60C\", \"B29K\",\"B29K\"], \n  'Frequency' : [42, 21, 12],\n  'Alive' : [True, False, True]\n}\ndataframe = pd.DataFrame(data)\ndataframe.describe()\n\n              Year  Frequency\ncount     3.000000   3.000000\nmean   2004.666667  25.000000\nstd       3.785939  15.394804\nmin    2002.000000  12.000000\n25%    2002.500000  16.500000\n50%    2003.000000  21.000000\n75%    2006.000000  31.500000\nmax    2009.000000  42.000000\n\n\nMissing values in a dataframe will be automatically ignored. We can also check the types of each variable to ensure that it corresponds to what we want (integers are integers, text is text, etc). The latter can be done with the .dftypes function.\n\nimport pandas as pd\ndata = {\n  'ID' : [\"ID1\", \"ID2\", \"ID3\", \"ID4\"], \n  'Year' : [2002, 2003, 2009, 2010], \n  'Cool Domain' : [\"B60C\", \"B29K\",\"B29K\", \"B60C\"], \n  'Frequency' : [42, 21, 12, None],\n  'Weight' : [12.1, 34.2, 21.3, 93.2],\n  'Alive' : [True, False, False, False],\n  'Country' : [\"France\", \"France\", \"Germany\", \"Netherlands\"]\n}\ndf = pd.DataFrame(data)\ndf.describe()\ndf.dtypes\n\nIn this output we can see that the None value is ignored and the statistics are computed. The format of the variables is shown: ID is considered “object” signifying that has no specific nature (object is a generic format for anything used in Python). int64 represents an integer, float64 represents a reel number, bool is a boolean operator.\nGiven that we created the dataframe ourselves we can be relatively sure that the data was is the right format. However, often enough when we download data from online sources, we run into issues. If the format is not as expected there are different functions that allows us to transform the format of the data. Mainly we use .astype(float), .astype(int) and .astype(str).\n\ndata = {\n  'ID' : [\"ID1\", \"ID2\", \"ID3\", \"ID4\"], \n  'Year' : [2002, 2003, 2009, 2010], \n  'Cool Domain' : [\"B60C\", \"B29K\",\"B29K\", \"B60C\"], \n  'Frequency' : [42, 21, 12, 12],\n  'Weight' : [12.1, 34.2, 21.3, 93.2],\n  'Alive' : [True, False, False, False],\n  'Country' : [\"France\", \"France\", \"Germany\", \"Netherlands\"]\n}\ndf = pd.DataFrame(data)\n# Change the data type of 'ID' to string\ndf['ID'] = df['ID'].astype(str)\n\n# Change the data type of 'Weight' to uppercase string\ndf['Weight'] = df['Weight'].astype(float)\n\n# Change the data type of 'Frequency' to int\n# We first need to take care of the missing value\ndf['Frequency'] = df['Frequency'].astype(int)\ndf.dtypes\n\nID              object\nYear             int64\nCool Domain     object\nFrequency        int64\nWeight         float64\nAlive             bool\nCountry         object\ndtype: object\n\n\nNow that our data has the right format and we are sure we have all the observations we might want to do some work to normalise the column names. This is done to make programming and refercing the columns easier. Whenever the is a space in the name of the column, we need to refer to the column using `Column name with spaces` which is not ideal. It’s generally good practice to not have any spaces. We can use the pyjanitor package to automate this process:\n\nimport pandas as pd\nimport janitor\n\ndata = {'Column 1': [1, 2, None, 4],\n        'Column (2)': [None, 6, 7, 8],\n        'PM(µ &gt;3)': [9, 10, 11, 12]}\ndf = pd.DataFrame(data)\nprint(df)\n\n   Column 1  Column (2)  PM(µ &gt;3)\n0       1.0         NaN         9\n1       2.0         6.0        10\n2       NaN         7.0        11\n3       4.0         8.0        12\n\n# Using clean_pandas\ndf = df.clean_names()\nprint(df)\n\n   column_1  column_2_  pm_µ_&gt;3_\n0       1.0        NaN         9\n1       2.0        6.0        10\n2       NaN        7.0        11\n3       4.0        8.0        12\n\n\n\n\nData transformation",
    "crumbs": [
      "Home",
      "Intro to Programming",
      "Data Import and preparation"
    ]
  },
  {
    "objectID": "CE_network.html",
    "href": "CE_network.html",
    "title": "Collaboration network: Research on the circular economy",
    "section": "",
    "text": "The network is created from co-authorship from publications written between 2010 and 2024. Only published articles and books were selected from the Scopus database.\nAffiliation were obtained from scopus in a raw format (hence the many variations in the list), some small cleaning steps were taken to identify universities and tag researchers from copernicus.\n\nlibrary(visNetwork)\nlibrary(igraph)\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\nload(\"~/Desktop/Projects/3GPP/3GPP/CE_g_GC.rdata\")\nvisIgraph(CE_g_GC)  %&gt;% \n  visOptions(selectedBy = \"Affiliation\", highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;% \n  visPhysics(\n              solver = \"forceAtlas2Based\",\n              forceAtlas2Based = list(\n              gravitationalConstant = -50,\n              centralGravity = 0.01,\n              springLength = 100,\n              springConstant = 0.08,\n              damping = 0.4,\n              avoidOverlap = 1   # This tries to avoid overlap; values 0 (off) to 1 (on)\n              ))",
    "crumbs": [
      "Home",
      "Network Analysis",
      "Collaboration network: Research on the circular economy"
    ]
  },
  {
    "objectID": "Network_analysis_basics.html",
    "href": "Network_analysis_basics.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "We are familiar with the statistical processing of object data (sum, mean, variance, histograms, etc.). However, there is a different kind of data with a high informational value: relational data. We are referring here to data that indicates a relationship, e.g a collaborative relationship, a citation link, a financial investment, a social link, etc. Relational data is rich in information. Relational data show their informational richness with different tools and methods. In fact, in relational data we’re mainly interested in the structure of these relationships and the role that entities play in this structure. This idea is illustrated in Figure 1.\n\nThe bar chart shows the frequency of patent technology classifications in a portfolio. The diagram provides information on the fields in which players seek to protect their inventions. Code B60C1/00 corresponds to “Tires characterized by chemical composition”, code C08K003/04 corresponds to “Use of inorganic substances as adjuvants” and so on. The network is generated by connecting two classifications present on the same patent. Each node in this network is therefore a classification, and each link indicates that both classifications were observed on the same patent.\nThe network representation of these classifications enables us to learn more about the combination of classifications. We can see which classifications are often combined, which are never combined, which play a central role, and so on. What interests us in networks is the structure. Analyzing structure allows us to quantify which nodes group together in communities, which nodes are central, which nodes connect different parts of the network (or clusters) and much more.\nHowever, to get the most out of this data, we need specific metrics and methods. These tools fall into a field known as Social Network Analysis (SNA). The name is misleading, coming from the pioneering methods that began work on social systems analysis. The methods have subsequently been employed in every field of science, but the name hasn’t really changed.\nWhatever the data source, the methods and indicators are the same. Interpretation, however, depends on the data: a co-classification link, a shareholding, a citation or a collaboration are not interpreted in the same way. And even between the same types of data, interpretation can differ. For example, a collaboration link in a project funded by the French National Research Agency (ANR) is not interpreted in the same way as a collaboration link in a European project. It is therefore essential to understand both the theory (indicators and methods) associated with SNA and the origin of the data. To this end, this manual has a dual objective. The first is to present the theoretical aspects of network analysis. The second is to present use cases mobilizing different data sources. These cases are divided into three parts:\n\nPresent the origin of the data, how to interpret the data\nPresent how the network was generated (what information was mobilized, what clean-ups were carried out, etc.).\nAnalysis of the network with an interpretation in line with the use case and the data mobilized.\n\nParticular attention is paid to the explanation of citation analysis in patents, and textual analysis, which present a higher level of complexity than other data sources.\nThis manual is aimed at Masters students and intelligence professionals who wish to include network analysis in their toolbox.\n\n\n\nIn its most basic form, a network is an object based on a set of nodes and links. The nature of the nodes and links is unlimited. A network can be created from any type of object with an interaction link. Network analysis can be applied to the analysis of social interactions: social interactions between cows, interactions between brain areas, equity investment links between firms, financial exposures between banks, collaborations between firms, links between criminals - the list goes on.\n\n\n\n\n\n\n\n\n\nFigure 2 shows the construction of a network in its simplest form. Objects are represented by circles we call “nodes” and interactions by connections we call “links”. The network here shows object 1 interacting with object 2. This could be two researchers co-authoring a paper, two firms collaborating on a project, etc.\n\n\n\n\n\n\nWarning\n\n\n\nA network is the sum of the objects and interconnections between these objects.\n\n\nThe concept is relatively simple, and we may be tempted to create networks from any kind of data. In order to avoid errors and misinterpretations, it is important to be able to identify the type of network we are creating and how this impacts on the validity or choice of indicators for analysis. In this section, we present the different types of networks and introduce a few vocabulary elements that will serve as a basis for the cases applied in the following.\n\n\n\n\n\nIn the simplest cases, an interaction between two objects can go either way. Suppose a collaborative link between two researchers: if \\(\\textit{A}\\) collaborates with \\(\\textit{B}\\), then \\(\\textit{B}\\) collaborates with \\(\\textit{A}\\). Another example is a network of criminals, a network based on participants in conferences or projects.\nIf the interaction between objects takes place in both directions, we speak of a \\(\\textbf{nondirected}\\) network. Classically, this type of interaction is represented by a line without arrows between objects. Figure \\(\\ref{nw_simple_nondirected}\\) gives an example of this type of network.\n\n\n\n\n\n\n\na. Unweighted Network\n\n\n\n\n\n\n\n\n\n\n\n\nb. Weighted Network\n\n\n\n\n\n\nIt is possible to quantify the link between objects, in which case we speak of a weighted network. If we take the example of a collaboration network, a collaboration link can be weighted by the number of collaborations between two actors. The number is then associated as a weight on the link and can be visualized by a different link thickness. In figure \\(\\ref{nw_simple_non_dirige_pondere}\\) we have the same structure, but different weights on the links. The weights are visualized by the link thickness and by the number displayed next to the links. It is often important to associate this type of information, both for the relevance of the calculation of certain indicators, and for visualization purposes. The latter makes it easier to see the structure of interactions between objects, and quickly identifies the densest areas of the network.\nIt’s possible to observe a link in a network that points from a node to itself. This is called a \\(\\textbf{loop}\\) and is represented in figure \\(\\ref{nw_simple_non_dirige_pondere}\\) by a red broken line. Let’s assume that each node in the illustrated network represents a country, and each link represents collaboration between players in the territory. The network shows us that two actors from country \\(\\textit{A}\\) collaborate with actors from country \\(\\textit{C}\\). In this context, the loop indicates that actors in country \\(\\textit{C}\\) are also working together.\n\n\n\nWhen the interaction goes from one node to another, the network is \\(\\textbf{directed}\\). A financial transaction is directed from one account to another. Quotations are another example: a document quoting a pre-existing document. So is the infection of one person by another. The virus spreads from the infected individual to the healthy one. The direction must be unilateral for the network to be considered directed. Figure \\(\\ref{nw_simple_dirige}\\) shows an example of a directed network. By convention, to indicate the direction of interaction, the link is represented by an arrow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this illustration, the node \\(\\textit{A}\\) interacts with three other nodes, with varying intensity. However, no node interacts with it. Node \\(\\textit{B}\\), on the other hand, both receives and gives interactions (with node \\(\\textit{C}\\)). Suppose we’re dealing with a financial network in which each node is a bank account held by an individual. In this case, node $textit{A}$ sends money to three other individuals, sending the largest amount (3 units) to node \\(\\textit{B}\\). The latter receives money from two individuals and sends money to individual \\(\\textit{C}\\).\nThe fact that a network is directed does not prevent two-way interaction. The difference with an undirected (or bilateral) network is that this interaction is split into two directed interactions. In figure \\(\\ref{nw_simple_directed_loop}\\) this is visualized between nodes \\(\\textit{D}\\) and \\(\\text{E}\\) by two red directed links. It is therefore possible that the weight of the link from \\(\\textit{D}\\) to \\(\\textit{E}\\) is different from the weight of \\(\\textit{E}\\) to \\(\\textit{D}\\). In an undirected network, the weight is unique.\nAs in an undirected network, a loop on a node is possible, with the difference that this loop is directed.\n\n\n\nIt is possible to create a network containing different types of objects. Suppose we want to analyze the career of inventors by mapping the companies they have worked for. This implies having two types of nodes: inventors and companies. When two types of nodes are interconnected in the same network, this network is called a \\(\\textit{bi-modal}\\) network.\n\n\n\n\n\n\n\n\n\nAn example is given in figure \\(\\ref{Reseau_bimodal}\\). We have three inventors and two companies. Two inventors are common to both companies, and one inventor is specific to one company. It’s important to note that in this type of network, there is no direct link between companies or inventors. Indeed, a link between inventors would represent a different type of link: the network would no longer be solely bi-modal, but would also be a multi-graph (network with more types of links, see \\(\\ref{muli_graph}\\)). In the network represented here, we have only one type of link: the company membership link.\nHowever, it is possible to transform a bi-modal network into a uni-modal one. The idea is to create a link between two inventors who have belonged to the same companies, and to create a link between two companies that have employed the same inventor. The nature of the link changes, but we’re trying to show the same thing. In figure \\(\\ref{Reseau_bimodal_ecalte}\\) the transformation is visualized.\n\n\n\n\n\n\n\n\n\nThe first network is a simple link between the two firms, with a weight of two because they have employed two inventors in common. The second network is composed solely of inventors. Here, the blue inventor has one firm in common with the orange inventor, and the orange inventor has two firms in common with the grey inventor.\nThis type of transformation facilitates analysis and, above all, the calculation of indicators that are more complex to calculate in an n-modal network.\n\n\n\nLet’s suppose we want to represent different types of interaction in the same network. This translates into the possible existence of two (or more) links between two objects in the same network. Figure \\(\\ref{Network_two_types_of_links}\\) visualizes this idea with two links between nodes \\(\\textit{C}\\) and \\(\\textit{D}\\). The first link is shown in black, the second in red. A network containing different types of links is called a \\(\\textbf{multi-graph}\\).\nAs with any network, it can be analyzed visually or by calculating indicators. In the particular case of a multigraph, the vast majority of indicators would be false (or incalculable) if two types of link co-existed. We must therefore be careful with multigraphs. While a visual analysis can be carried out without too many constraints, an analysis using indicators requires vigilance with regard to the indicators calculated (check in the software that the indicators take into account the different links). Indeed, if the software doesn’t distinguish between them, each link will be treated as identical, undermining the additional informational value of the multigraph.\n\n\n\n\n\n\n\n\n\nLet’s assume that the nodes in this network are companies, with black links representing collaborative links in patents, and red links representing collaborations in these scientific publications. We can read in this network that firm \\(\\textit{A}\\) has co-authored a paper with firms \\(\\textit{E}\\) and \\(\\textit{B}\\) but has co-filed a patent with firm \\(\\textit{C}\\). Although these are collaborative links in both cases, the implications are not at all the same. A collaborative link in patents implies an intellectual property parte, whereas publication shows above all a fundamental research activity between the two entities. A visual analysis here is interesting and relevant. However, the calculation of classic indicators would be wrong, as it considers both types of link to be identical. To overcome this problem, it is possible to create two networks from the first, each with only one type of link. The alternative relies on the use of more complex indicators, which are far more difficult to interpret and limit the impact these analyses can have on decision-making.\nMore effective than the multigraph are multiplex networks and interconnected networks, which we will describe in detail below.\n\n\n\nA different approach to multigraphs and bi-modal graphs is to consider each typology of links or nodes as a specific network, and to create links between networks. There are two types of multi-layer network, the multiplex network and the interconnected network.\nA different approach to the multigraph is to consider each link typology in a specific network and create links between networks. For example, in the network shown in figure \\(\\ref{nw_multiplex}\\) we have two networks represented simultaneously. Each network is also called layer here, referring to the idea that each layer handles a particular interaction (Kivelä et al. 2014). Thus, links connecting nodes in the same network are called intra-layer links. In this type of network, links between layers (inter-layer) exist only to notify the presence of the same node in different layers of the network (Sole-Ribalta et al. 2013).\n\nWe can thus imagine co-patenting in a first layer and co-publishing in a second layer. Visualizing this type of network is possible for small networks, but quickly becomes unmanageable for larger ones. For the latter, a layered network remains the best solution. To calculate indicators, a multiplex network is possible, provided the software can handle them (R, Python, networkX).\n\nUnlike multi-layer networks, which assign a layer to each type of interaction, the interconnected network considers one object per layer. For example, the first network may consist of all interactions in a given region, the second network a different region. As a result, inter-layer links can be made between different nodes, unlike in a multiplex network. For example, in the network shown in figure \\(\\ref{nw_interconnected}\\), company A in region 1 collaborates with company B in region 2.\n\nBoth the multiplex and the interconnected network require specific algorithms for calculating indicators, and few software packages are able to manage this type of network.\n\n\n\n\nThe analysis of connections between nodes often raises the question of distance. A node that is close to all the nodes in the network is, a priori, an important node. Many centrality indicators are based on notions of distance between a node and other nodes in the network. By the same token, to judge the size of a network, we’d like to know the distance separating the most outlying nodes. In other cases, it may be important to know whether it’s simply possible to navigate from one node to another in a network.\nTo answer these questions, we first need to introduce the notion of “path”.\n\n\nIn network analysis, a is a sequence of nodes through which you must pass to reach a given node from a given node. Suppose we’re looking for a path between node and node in the network shown in figure \\(\\ref{Notion_de_chemin_bilat}\\). As the network is undirected, we can take the direction of the link as we wish. Thus, we start from node to node , then continue from node to node . The path connecting nodes G and A is therefore G, F, A.\nIf the network is directed, a path follows the direction of the links. So, in figure \\(\\ref{Notion_of_path_unilat}\\), the only path between and is A \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) C. Since the link between A and B is in the opposite direction, we cannot go from A to B directly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn both examples, the path between the two nodes is unique. However, several paths can coexist. The more nodes and links a network contains, the greater the number of paths. A key factor in differentiating these paths will then be their length, or the separating the nodes.\nThe distance between two nodes is given by the length of the path separating two nodes. The length is then given by the number of links separating the two nodes. For example, in figure two different paths are given to connect nodes G and C. The first path, in figure \\(\\ref{notion_distance_bi}\\) has a length of 4, giving a distance of 4 between the two nodes. The second path in figure \\(\\ref{notion_distance_uni}\\) has a distance of 6, giving a distance of 6 between G and C. Both distances and paths are valid, but we’ll often use the . This is calculated by identifying all possible paths between two nodes and retaining the shortest distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the case where no path exists between two nodes, by convention, we consider the distance between the two nodes to be \\(\\infty\\)..\nA special case of a path is the . A loop is a path leading from a node to itself. In figure \\(\\ref{Notion_loop}\\), a loop is shown in a directed network and in an undirected network. Loops can be problematic in some cases when we want to calculate distances or identify paths. For example, to calculate the distance between A and C, a correct path would be:\n\nA \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) A \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) C\n\nTheoretically, there are an infinite number of paths between A and C, as we could include the loop A - D - E - B - A an infinite number of times. In some cases, software will require an acyclic network, which implies a loop-free network, to enable algorithms to run.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we’ll start by presenting a number of indicators. Initially, we’ll focus on indicators at node level.\nIn order to illustrate the different indicators, we will mobilize a network extracted from real data. This is a collaborative network extracted from research projects funded by the French National Research Agency around 5G technologies. The network is given in figure 3.\n\nWhen analyzing a network, the first valuable piece of information is the number of connections for each node. This provides a measure of the number of collaborators (or number of citations, investors, co-authors, etc.). This indicator is called the .\n\n\n\n\n\n\nDefinition: degree\n\n\n\nThe degree of a node is the number of nodes it connects to. We can measure the degree by counting the number of links.\n\n\nIn the example of the network in figure \\(\\ref{fig_ind_degree}\\), node E has two connections (with and ), so its degree is 2. We then note \\(d_E = 2\\). The nodes and have a degree of 2, and a degree of 3.\n\n\n\n\n\nThis is a caption for the figure. \n\n\n\n\nIf we calculate the degrees of the network nodes in 1 we can visualize the degree by a color gradient. In 1 we visualize the degree of the nodes by a green gradient. The darker the color, the higher the degree.\n\nThe degree visualization shows that some nodes, even if central, have a relatively low degree (ETIS, IRCICA). Orange has the highest degree, and therefore the largest number of employees, followed by the Institut Mines Telecom (which is more off-center).\nThe degree is a simple but effective measure for differentiating the positioning of nodes in a network.\nThe Degree in a directed network\nIf the network we’re analyzing is a directed one, we need to take into account the direction of interaction. A node can have both incoming and outgoing links. It therefore seems natural to propose two degrees in a directed network.\n\n\n\n\n\n\nDefinition: in-degree\n\n\n\nThe in-degree of a node corresponds to the number of links pointing to the node.\n\n\n\n\n\n\n\n\nDefinition: out-degree\n\n\n\nThe in-degree of a node corresponds to the number of links originating from the node, pointing to other nodes.\n\n\nFor example, in figure \\(\\ref{Reseau_degre_dirige}\\), the incoming degree of node is \\(2\\), since two links point to it. Since no links point to other nodes from \\(E\\), its outgoing degree is \\(0\\). The logic is therefore the same as for the classic degree, with the only difference being the directions.\n\n\n\n\n\n\n\n\n\nDegree in a weighted network\nThe weights of interactions can be included in the degree calculation, so we speak of . The latter is obtained by summing the weights of the node’s links. For example, in Figure 1, node E has two links, one with a weight of 1 and one with a weight of 2. The weighted degree of node E is therefore 3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the aims of network analysis is to identify nodes with an important position in the network structure. Important can be defined in different ways: in the case of degree, it’s the number of links, but we can also look at things in a more structural way. If we take the network shown in figure , it’s clear that the position of Orange and CEA differ. The role of these two players is therefore different from a structural point of view. Orange has a more position than CEA. Intuitively, we can consider that the centrality of a node is linked to its importance in a network, as it reflects a higher level of interconnectivity than a node positioned more on the periphery. In other words, the smaller the distance between a node and all the other nodes in the network, the more central the node is in the network.\nThe idea of centrality is reflected in various indicators that measure the centrality of a node with a slightly different objective. We’ll introduce some of these indicators below.\nBetweenness Centrality\nA first vision of centrality is given in terms of flows in the network. Let’s suppose that the network represents interactions between individuals, and that a virus is circulating between them. A person on the periphery of the network is less likely to receive the virus, as it has to pass through a large number of nodes before reaching him or her. A central player is located on a large number of paths, exposing him to a greater risk of the virus reaching him before it reaches the entire network.\nIf we return to our collaboration network, we can consider that collaboration involves the exchange of ideas and information. These elements pass from node to node (possibly degrading or enriching themselves) in order to spread throughout the entire network. In the context of R&D, the pooling of the knowledge of different players can result in innovation. The logic then is that the position of a node is important if it is able to capture the information flows passing through the network.\nTo measure the importance of a node from a flow capture point of view, we’re going to identify the number of on which a node is positioned. More precisely, we’ll identify the number of shortest paths between each pair of nodes, and how many of them a given node is positioned on.\nFormally, the betweenness centrality is computed by:\\\n\n\n\n\n\n\nBetweennes Centrality formula:\n\n\n\n\\[\\begin{equation}\n    BC = \\sum_{k \\neq j, i \\in \\{k,j\\}} \\frac{\\frac{P_i(kj)}{P(kj)}}{\\frac{(n-1)\\cdot(n-2)}{2}}\\\\\n\\end{equation}\\]\n\n\nThe nominator calculates the number of paths between nodes \\(k\\) and \\(j\\) (\\(P(kj)\\)) and the number of paths between \\(k\\) and \\(j\\) on which node \\(i\\) is positioned (\\(P_i(kj)\\)). The ratio therefore gives the fraction of paths between \\(k\\) and \\(j\\) on which \\(i\\) is positioned.\nThe denominator is there to normalize the value for the size of the network. Betweenness Centrality therefore has a value between 0 and 1, the higher the value, the more central the node. This indicator was proposed by (Freeman 1977).\nExample\nLet’s take an example of calculation using the network in Figure 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s calculate the BC of node \\(C\\). We need to start by identifying the shortest paths between all the nodes and then count how many of these paths node \\(C\\) is positioned on. In the table \\(\\ref{table_example_BC}\\), the first column lists each pair of nodes. There’s only one path between \\(A\\) and \\(B\\) (\\(A \\mapsto C \\mapsto B\\)). There is therefore a single shortest path (\\(P(AB) = 1\\)) and node \\(C\\) is positioned on this path (\\(P_C(AB)\\)). This calculation is repeated for every other pair of nodes as shown in the table below:\n\n\n\n\n\n\n\n\nLink\nNumber of shortest paths with C \\(P_i(kj)\\)\nNumber of shortest paths \\(P(kj)\\)\n\n\n\n\nA-B\n1\n1\n\n\nA-J\n1\n1\n\n\nB-J\n1\n1\n\n\nA-G\n1\n1\n\n\nB-G\n1\n1\n\n\nB-I\n1\n1\n\n\nA-I\n1\n1\n\n\nB-H\n1\n1\n\n\nA-H\n1\n1\n\n\n\nThe final centrality score of node \\(C\\) is given by:\n\\[\\begin{equation}\nBC_C = \\frac{1+1+1+1+1+1+1+1+1}{\\frac{(n-1)\\cdot(n-2)}{2}} = \\frac{9}{15} = 0.6\n\\end{equation}\\]\nif we apply this calculation to the collaboration network, and color the nodes according to the betweenness centrality score, we obtain the network in Figure 1.\n\nThis centrality indicator highlights a number of players, notably Orange, CEA and ITM, with high centrality. The CEA, which is located on the periphery, positions itself on short paths by connecting to more central players, reducing the distance between itself and the other players in the network.\nThe BC thus makes it possible to identify players who act as links between different parts of the network. The CEA, for example, is the only actor linking the actors to its left with the rest of the network. As such, it controls the flow of information between these two parts of the network. This position is called . The BC makes it easier to identify actors with this particular (and valuable) position.\nCloseness Centrality\nIn contrast to BC, we can also consider a node to be important in a network when it is able to disseminate information rapidly throughout the network. For this reason, the indicator is called Closeness Centrality (CC) and was proposed by Mark Newman ((Newman 2005)). Instead of considering that a node is central because it receives a lot of information, we’ll consider that a node is central when it can quickly reach the other nodes. Once again, the concepts of distance and path are used here. The idea and calculation are similar to those of BC.\nIn mathematical terms, the Closeness Centrality is measured as follows:\\ \\[\\begin{equation}\nCC_i = \\frac{1}{avg(L(n,m)}\n\\end{equation}\\]\nWith \\(L(n,m)\\) the distance of the shortest path between \\(n\\) and \\(m\\).\\\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compare the Closeness Centrality of nodes \\(C\\) and \\(D\\) in figure \\(\\ref{nw_example_CC}\\). We need the shortest distance between \\(C\\) and all other nodes. For nodes \\(C\\) we get:\n\nC-A Shortest path at a distance of 1\nC-B Shortest path at a distance of 1\nC-C The path from the node to itself is not taken into account\nC-D Shortest path at a distance of 2\nC-E Shortest path at a distance of 2\nC-F Shortest path at a distance of 1\n\n\\[\\begin{equation}\nCC(C)\\frac{1}{\\frac{1+1+2+2+1}{5}} = 0.71   \n\\end{equation}\\]\nFor node \\(D\\) we have:\n\nD-A Shortest path at a distance of 1\nD-B Shortest path at a distance of 2\nD-C Shortest path at a distance of 2\nD-D The path from the node to itself is not taken into account\nD-E Shortest path at a distance of 3\nD-F Shortest path at a distance of 1\n\n\\[\\begin{equation}\nCC(D)\\frac{1}{\\frac{1+2+2+3+1}{5}} = 0.55\n\\end{equation}\\]\nSo node \\(C\\) has a more central position in the network than node \\(D\\). From an information dissemination point of view, node \\(C\\) has a more important position than node \\(D\\).\nBy calculating the CC in the ANR project network and coloring the nodes according to CC, we obtain the network shown in figure \\(\\ref{Reseau_fil_rouge_CC}\\). Closeness Centrality identifies actors other than the BC. Nodes with the highest BC have a lower CC. Nodes in a dense community tend to have a higher score, as they reach their neighbors in a very empty space. We also identify nodes at the heart of the network that have a more important position from a broadcasting point of view, notably CNES and Parrot.\n\nRadiality\nOne way of assessing the importance of a node in a network is to quantify its impact on the rest of the network. This impact strongly depends on the node’s position in the network. A node that interconnects several communities is important for the structuring of the network; if the node were to leave the network, the impact on the structure would be great. A node with a relatively isolated position has a more limited impact. To quantify just how important a node is, we look at a measure introduced by Valente and Foreman (1998): Radiality.\nRadiality is a measure similar to centrality, quantifying the importance of the node in the network from a community interconnection point of view. The major difference with a centrality measure is that radiality is adjusted by the diameter of the network: the higher the radiality, the closer the node is to all other nodes and therefore impacts different communities (but not necessarily all of them). If the radiality is low, the node is only locally important, so it’s more likely to be on the periphery of the network. Its calculation is based on the average distance between the node and all other nodes in the network. This distance is then adjusted by the size of the network (its diameter).\n\\[\\begin{equation}\nR_a=\\frac{\\sum_{i \\in N_a}[L(a,m)-(D+1)]}{k}\n\\end{equation}\\]\nWhere \\(L(a,m)\\) is the shortest path between node \\(a\\) and all other nodes in the network, \\(D\\) is the diameter of the network and \\(k\\) is the number of nodes in the component. The measure is a relative one, giving an idea of isolation (far from all the others) or proximity (close to all the others).\nSince this is not really a measure of centrality, it must be interpreted in combination with other indicators such as eccentricity and centrality. A node with a high score on these three indicators has an important, central position in the network.\nIn the case of our example network of ANR projects, the result is shown in figure \\(\\ref{Reseau_fil_rouge_rad}\\). The nodes that are highlighted do indeed connect different communities. The notion is slightly different from that of BC, which captures the importance between a community and the rest of the network. Here, nodes at the heart of the network have a higher score than the periphery, with the highest score for nodes located on a higher number of communities.\n\nTopology Coefficient\nIt can be interesting to identify the nodes that play an important role in a community, as well as the nodes that interconnect communities. A node at the center of its community is an influential node that can influence decision-making for its community. A node that connects different communities has a strategic position, giving it access to information from different communities. This position is favorable for an innovator, for example, seeking to combine different technologies. To identify community leaders and gatekeepers, we use the topology coefficient introduced by Ravasz et al. in 2002 : .\nThe topology coefficient quantifies the proportion of nodes that an individual has in common with the other individuals in his network (on average). The higher the score for a node, the denser its neighborhood. The probability of being at the center of a community increases as the score rises. If the score is low, the firm has very few neighbors in common, so it’s in a strategic position connecting different communities. The more neighbors a node has in common with the rest of the network, the denser its direct neighborhood. The calculation of the topology coefficient is therefore based on identifying the number of neighbors that the node has in common with all the other nodes in the network (on average).\n\\[\\begin{equation}\nT_n = \\frac{avg(J(n,m))}{k_n}\n\\end{equation}\\]\nIn this formula, \\(J(n,m)\\) corresponds to the number of neighbors shared by two nodes in the network. \\(avg()\\) implies that we take the average. \\(k_n\\) is the number of neighbors of node \\(n\\). The score is between 0 and 1. To illustrate, let’s take the network shown in figure \\(\\ref{nw_example_topo}\\) and calculate the score for node \\(m\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe begin by looking at the neighbors of node \\(m\\): - \\(m - h\\) - There are no common neighbors, but there is a direct link between \\(m\\) and \\(h\\). J(m,h) = 1. - \\(m - i\\) - There are no common neighbors, but there is a direct link between \\(m\\) and \\(i\\). J(m,i) = 1. - \\(m - c\\) - There are no common neighbors, but there is a direct link between \\(m\\) and \\(c\\). J(m,c) = 1.\nFor all other nodes, there is neither a direct link nor a common neighbor, so for all these links \\(J(\\cdot,\\cdot) = 0\\).\nWe therefore have \\(T_m = \\frac{avg(1+1+1}{3}=\\frac{\\frac{1}{3}(3)}{3} = 0. 333\\)\nAs the number of nodes in the network increases ( \\(k_n\\) increases), the coefficient mechanically decreases. We therefore interpret this coefficient in relative terms, comparing it with the scores of other nodes in the network.\\(\\ref{Reseau_fil_rouge_topology}\\) gives an illustration of this indicator in the ANR project network.\n\nEigenvector Centrality\nThe centrality measures proposed so far identify the importance of a node based on a measure of its distance from other nodes. We can also consider that the importance of a node depends on the nodes to which it is connected. In other words, a node is important because it is connected to important nodes.\nImagine that a researcher with few connections, but who works with the most important researchers, could be more important than a researcher with many connections to lesser-known researchers. Eigenvector centrality (EC) is a measure that integrates this dimension. A node with a high EC is connected to important nodes in the network. The potential for this node to become important in the future is high. This measure ranges from 0 to 1. The higher the measure, the more important the node.\nThe calculation of this measure is more complex than the others. The indicator is obtained by spectral decomposition. Using the Perron-Frobenius theorem, we know that the largest eigenvector of this decomposition contains the centrality scores, hence the name: . To illustrate the indicator, the CE has been calculated for the various nodes in the network of ANR projects shown in figure \\(\\ref{Reseau_fil_rouge_EC}\\).\n\nFrom a mathematical perspective: If \\(A\\) is the adjacency matrix where \\(A_{ij}= 1\\) when there is a link between nodes \\(i\\) and \\(j\\) and \\(A_{ij} = 0\\) when there is no link. Then the eigenvectos centrality of node \\(i\\) is proportional to the sum of the centralities of its neighbors:\n\\[\\begin{equation}\nEC_i = \\frac{1}{\\lambda}\\sum_j A_{ij}EC_j\n\\end{equation}\\]\nThis shows that when we use compute the EC of a node, the value increases when the node is connected to nodes with a high EC.\nLe clustering\nBeyond the number of links and the centrality of the node, the direct neighborhood of the node also holds importance. A node positioned in a dense neighborhood (textit{i.e}, with a significant number of connections between its neighbors) does not have the same impact on the network as a node positioned with sparse connections.\nIn order to assess the level of interconnection in the direct neighborhood of the node, it is necessary to measure the propensity for interconnection among the neighbors of a node. At the maximum, all of a node’s neighbors are interconnected (the network is then complete, as in the network on the left in Figure \\(\\ref{nw_exemple_clust}\\)). Thus, for node \\(a\\), the entire set of neighbors is interconnected, unlike node \\(g\\), which has two neighbors that are not connected. The neighborhood of node \\(g\\) is therefore less dense than the neighborhood of node \\(b\\).\nMeasuring the level of interconnection relies on the identification of triangles, i.e., groups of three connected nodes ({\\(a\\) - \\(b\\) - \\(c\\)} in the network on the left). By counting how many of these triangles are present relative to the number of possible triangles, we obtain a measure known as clustering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{equation}\nCl_a = \\displaystyle\\frac{\\mbox{Number of observed triangles}}{\\mbox{Number of possible triangles}}\n\\end{equation}\\]\nThe clustering coefficient gives a value between 0 and 1; the higher the value, the greater the clustering. The number can be interpreted as the fraction of triangles formed between the node and its neighborhood.\nTo illustrate, let’s calculate the clustering coefficient for nodes \\(a\\) and \\(e\\).\nWe begin by identifying the number of triangles:\nFor \\(a\\): \\({a - b - d}\\), \\({a - c - d}\\), and \\({a - b - c}\\) For \\(e\\): No triangles, so the numerator for \\(Cl_e = 0\\)\nIl convient maintenant de trouver le nombre potentiel de triangles. Autrement dit le nombre de triangles qui pourraient exister en connectant les voisins directs qui ne sont pas déjà connectés.\\\nNow, we need to find the potential number of triangles. In other words, the number of triangles that could exist by connecting the direct neighbors that are not already connected.\nFor \\(a\\): All the neighbors are interconnected, so the potential number is equal to the observed number: 3. For \\(e\\): {\\(e\\) - \\(g\\) - \\(h\\)}, {\\(e\\) - \\(g\\) - \\(f\\)}, and {\\(e\\) - \\(h\\) - \\(f\\)}. \\(e\\) could potentially be part of 3 triangles. Finally, by taking the ratio of the two:\nFor \\(a\\): \\(Cl_a = \\frac{3}{3} = 1\\) For \\(e\\): \\(Cl_e = \\frac{0}{3} = 0\\)\nA node with a high clustering coefficient is therefore densely interconnected in its neighborhood and, consequently, within its community. A node with a low clustering coefficient is more isolated compared to other nodes.\nIn Figure \\(\\ref{Reseau_fil_rouge_clust}\\), clustering is calculated for the network of ANR projects. Since the neighborhood of nodes is entirely defined by the partners of a project, we know that the number of triangles is significant in this type of network (each actor is linked to every other actor in a project). The most central nodes have a lower clustering coefficient than some other nodes in the core (Parrot, CNES, L2S, TRCOM). At the same time, the central nodes also participate in more projects. If the clustering for these nodes is lower, it implies that the central actors collaborate more with different actors, who in turn do not collaborate with each other. These actors therefore play a role as intermediaries.",
    "crumbs": [
      "Home",
      "Network Analysis",
      "The basics of Network Analysis"
    ]
  },
  {
    "objectID": "Network_analysis_basics.html#the-basics-of-network-analysis",
    "href": "Network_analysis_basics.html#the-basics-of-network-analysis",
    "title": "NetworkIsLife",
    "section": "",
    "text": "We are familiar with the statistical processing of object data (sum, mean, variance, histograms, etc.). However, there is a different kind of data with a high informational value: relational data. We are referring here to data that indicates a relationship, e.g a collaborative relationship, a citation link, a financial investment, a social link, etc. Relational data is rich in information. Relational data show their informational richness with different tools and methods. In fact, in relational data we’re mainly interested in the structure of these relationships and the role that entities play in this structure. This idea is illustrated in Figure 1.\n\nThe bar chart shows the frequency of patent technology classifications in a portfolio. The diagram provides information on the fields in which players seek to protect their inventions. Code B60C1/00 corresponds to “Tires characterized by chemical composition”, code C08K003/04 corresponds to “Use of inorganic substances as adjuvants” and so on. The network is generated by connecting two classifications present on the same patent. Each node in this network is therefore a classification, and each link indicates that both classifications were observed on the same patent.\nThe network representation of these classifications enables us to learn more about the combination of classifications. We can see which classifications are often combined, which are never combined, which play a central role, and so on. What interests us in networks is the structure. Analyzing structure allows us to quantify which nodes group together in communities, which nodes are central, which nodes connect different parts of the network (or clusters) and much more.\nHowever, to get the most out of this data, we need specific metrics and methods. These tools fall into a field known as Social Network Analysis (SNA). The name is misleading, coming from the pioneering methods that began work on social systems analysis. The methods have subsequently been employed in every field of science, but the name hasn’t really changed.\nWhatever the data source, the methods and indicators are the same. Interpretation, however, depends on the data: a co-classification link, a shareholding, a citation or a collaboration are not interpreted in the same way. And even between the same types of data, interpretation can differ. For example, a collaboration link in a project funded by the French National Research Agency (ANR) is not interpreted in the same way as a collaboration link in a European project. It is therefore essential to understand both the theory (indicators and methods) associated with SNA and the origin of the data. To this end, this manual has a dual objective. The first is to present the theoretical aspects of network analysis. The second is to present use cases mobilizing different data sources. These cases are divided into three parts:\n\nPresent the origin of the data, how to interpret the data\nPresent how the network was generated (what information was mobilized, what clean-ups were carried out, etc.).\nAnalysis of the network with an interpretation in line with the use case and the data mobilized.\n\nParticular attention is paid to the explanation of citation analysis in patents, and textual analysis, which present a higher level of complexity than other data sources.\nThis manual is aimed at Masters students and intelligence professionals who wish to include network analysis in their toolbox.\n\n\n\nIn its most basic form, a network is an object based on a set of nodes and links. The nature of the nodes and links is unlimited. A network can be created from any type of object with an interaction link. Network analysis can be applied to the analysis of social interactions: social interactions between cows, interactions between brain areas, equity investment links between firms, financial exposures between banks, collaborations between firms, links between criminals - the list goes on.\n\n\n\n\n\n\n\n\n\nFigure 2 shows the construction of a network in its simplest form. Objects are represented by circles we call “nodes” and interactions by connections we call “links”. The network here shows object 1 interacting with object 2. This could be two researchers co-authoring a paper, two firms collaborating on a project, etc.\n\n\n\n\n\n\nWarning\n\n\n\nA network is the sum of the objects and interconnections between these objects.\n\n\nThe concept is relatively simple, and we may be tempted to create networks from any kind of data. In order to avoid errors and misinterpretations, it is important to be able to identify the type of network we are creating and how this impacts on the validity or choice of indicators for analysis. In this section, we present the different types of networks and introduce a few vocabulary elements that will serve as a basis for the cases applied in the following.\n\n\n\n\n\nIn the simplest cases, an interaction between two objects can go either way. Suppose a collaborative link between two researchers: if \\(\\textit{A}\\) collaborates with \\(\\textit{B}\\), then \\(\\textit{B}\\) collaborates with \\(\\textit{A}\\). Another example is a network of criminals, a network based on participants in conferences or projects.\nIf the interaction between objects takes place in both directions, we speak of a \\(\\textbf{nondirected}\\) network. Classically, this type of interaction is represented by a line without arrows between objects. Figure \\(\\ref{nw_simple_nondirected}\\) gives an example of this type of network.\n\n\n\n\n\n\n\na. Unweighted Network\n\n\n\n\n\n\n\n\n\n\n\n\nb. Weighted Network\n\n\n\n\n\n\nIt is possible to quantify the link between objects, in which case we speak of a weighted network. If we take the example of a collaboration network, a collaboration link can be weighted by the number of collaborations between two actors. The number is then associated as a weight on the link and can be visualized by a different link thickness. In figure \\(\\ref{nw_simple_non_dirige_pondere}\\) we have the same structure, but different weights on the links. The weights are visualized by the link thickness and by the number displayed next to the links. It is often important to associate this type of information, both for the relevance of the calculation of certain indicators, and for visualization purposes. The latter makes it easier to see the structure of interactions between objects, and quickly identifies the densest areas of the network.\nIt’s possible to observe a link in a network that points from a node to itself. This is called a \\(\\textbf{loop}\\) and is represented in figure \\(\\ref{nw_simple_non_dirige_pondere}\\) by a red broken line. Let’s assume that each node in the illustrated network represents a country, and each link represents collaboration between players in the territory. The network shows us that two actors from country \\(\\textit{A}\\) collaborate with actors from country \\(\\textit{C}\\). In this context, the loop indicates that actors in country \\(\\textit{C}\\) are also working together.\n\n\n\nWhen the interaction goes from one node to another, the network is \\(\\textbf{directed}\\). A financial transaction is directed from one account to another. Quotations are another example: a document quoting a pre-existing document. So is the infection of one person by another. The virus spreads from the infected individual to the healthy one. The direction must be unilateral for the network to be considered directed. Figure \\(\\ref{nw_simple_dirige}\\) shows an example of a directed network. By convention, to indicate the direction of interaction, the link is represented by an arrow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this illustration, the node \\(\\textit{A}\\) interacts with three other nodes, with varying intensity. However, no node interacts with it. Node \\(\\textit{B}\\), on the other hand, both receives and gives interactions (with node \\(\\textit{C}\\)). Suppose we’re dealing with a financial network in which each node is a bank account held by an individual. In this case, node $textit{A}$ sends money to three other individuals, sending the largest amount (3 units) to node \\(\\textit{B}\\). The latter receives money from two individuals and sends money to individual \\(\\textit{C}\\).\nThe fact that a network is directed does not prevent two-way interaction. The difference with an undirected (or bilateral) network is that this interaction is split into two directed interactions. In figure \\(\\ref{nw_simple_directed_loop}\\) this is visualized between nodes \\(\\textit{D}\\) and \\(\\text{E}\\) by two red directed links. It is therefore possible that the weight of the link from \\(\\textit{D}\\) to \\(\\textit{E}\\) is different from the weight of \\(\\textit{E}\\) to \\(\\textit{D}\\). In an undirected network, the weight is unique.\nAs in an undirected network, a loop on a node is possible, with the difference that this loop is directed.\n\n\n\nIt is possible to create a network containing different types of objects. Suppose we want to analyze the career of inventors by mapping the companies they have worked for. This implies having two types of nodes: inventors and companies. When two types of nodes are interconnected in the same network, this network is called a \\(\\textit{bi-modal}\\) network.\n\n\n\n\n\n\n\n\n\nAn example is given in figure \\(\\ref{Reseau_bimodal}\\). We have three inventors and two companies. Two inventors are common to both companies, and one inventor is specific to one company. It’s important to note that in this type of network, there is no direct link between companies or inventors. Indeed, a link between inventors would represent a different type of link: the network would no longer be solely bi-modal, but would also be a multi-graph (network with more types of links, see \\(\\ref{muli_graph}\\)). In the network represented here, we have only one type of link: the company membership link.\nHowever, it is possible to transform a bi-modal network into a uni-modal one. The idea is to create a link between two inventors who have belonged to the same companies, and to create a link between two companies that have employed the same inventor. The nature of the link changes, but we’re trying to show the same thing. In figure \\(\\ref{Reseau_bimodal_ecalte}\\) the transformation is visualized.\n\n\n\n\n\n\n\n\n\nThe first network is a simple link between the two firms, with a weight of two because they have employed two inventors in common. The second network is composed solely of inventors. Here, the blue inventor has one firm in common with the orange inventor, and the orange inventor has two firms in common with the grey inventor.\nThis type of transformation facilitates analysis and, above all, the calculation of indicators that are more complex to calculate in an n-modal network.\n\n\n\nLet’s suppose we want to represent different types of interaction in the same network. This translates into the possible existence of two (or more) links between two objects in the same network. Figure \\(\\ref{Network_two_types_of_links}\\) visualizes this idea with two links between nodes \\(\\textit{C}\\) and \\(\\textit{D}\\). The first link is shown in black, the second in red. A network containing different types of links is called a \\(\\textbf{multi-graph}\\).\nAs with any network, it can be analyzed visually or by calculating indicators. In the particular case of a multigraph, the vast majority of indicators would be false (or incalculable) if two types of link co-existed. We must therefore be careful with multigraphs. While a visual analysis can be carried out without too many constraints, an analysis using indicators requires vigilance with regard to the indicators calculated (check in the software that the indicators take into account the different links). Indeed, if the software doesn’t distinguish between them, each link will be treated as identical, undermining the additional informational value of the multigraph.\n\n\n\n\n\n\n\n\n\nLet’s assume that the nodes in this network are companies, with black links representing collaborative links in patents, and red links representing collaborations in these scientific publications. We can read in this network that firm \\(\\textit{A}\\) has co-authored a paper with firms \\(\\textit{E}\\) and \\(\\textit{B}\\) but has co-filed a patent with firm \\(\\textit{C}\\). Although these are collaborative links in both cases, the implications are not at all the same. A collaborative link in patents implies an intellectual property parte, whereas publication shows above all a fundamental research activity between the two entities. A visual analysis here is interesting and relevant. However, the calculation of classic indicators would be wrong, as it considers both types of link to be identical. To overcome this problem, it is possible to create two networks from the first, each with only one type of link. The alternative relies on the use of more complex indicators, which are far more difficult to interpret and limit the impact these analyses can have on decision-making.\nMore effective than the multigraph are multiplex networks and interconnected networks, which we will describe in detail below.\n\n\n\nA different approach to multigraphs and bi-modal graphs is to consider each typology of links or nodes as a specific network, and to create links between networks. There are two types of multi-layer network, the multiplex network and the interconnected network.\nA different approach to the multigraph is to consider each link typology in a specific network and create links between networks. For example, in the network shown in figure \\(\\ref{nw_multiplex}\\) we have two networks represented simultaneously. Each network is also called layer here, referring to the idea that each layer handles a particular interaction (Kivelä et al. 2014). Thus, links connecting nodes in the same network are called intra-layer links. In this type of network, links between layers (inter-layer) exist only to notify the presence of the same node in different layers of the network (Sole-Ribalta et al. 2013).\n\nWe can thus imagine co-patenting in a first layer and co-publishing in a second layer. Visualizing this type of network is possible for small networks, but quickly becomes unmanageable for larger ones. For the latter, a layered network remains the best solution. To calculate indicators, a multiplex network is possible, provided the software can handle them (R, Python, networkX).\n\nUnlike multi-layer networks, which assign a layer to each type of interaction, the interconnected network considers one object per layer. For example, the first network may consist of all interactions in a given region, the second network a different region. As a result, inter-layer links can be made between different nodes, unlike in a multiplex network. For example, in the network shown in figure \\(\\ref{nw_interconnected}\\), company A in region 1 collaborates with company B in region 2.\n\nBoth the multiplex and the interconnected network require specific algorithms for calculating indicators, and few software packages are able to manage this type of network.\n\n\n\n\nThe analysis of connections between nodes often raises the question of distance. A node that is close to all the nodes in the network is, a priori, an important node. Many centrality indicators are based on notions of distance between a node and other nodes in the network. By the same token, to judge the size of a network, we’d like to know the distance separating the most outlying nodes. In other cases, it may be important to know whether it’s simply possible to navigate from one node to another in a network.\nTo answer these questions, we first need to introduce the notion of “path”.\n\n\nIn network analysis, a is a sequence of nodes through which you must pass to reach a given node from a given node. Suppose we’re looking for a path between node and node in the network shown in figure \\(\\ref{Notion_de_chemin_bilat}\\). As the network is undirected, we can take the direction of the link as we wish. Thus, we start from node to node , then continue from node to node . The path connecting nodes G and A is therefore G, F, A.\nIf the network is directed, a path follows the direction of the links. So, in figure \\(\\ref{Notion_of_path_unilat}\\), the only path between and is A \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) C. Since the link between A and B is in the opposite direction, we cannot go from A to B directly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn both examples, the path between the two nodes is unique. However, several paths can coexist. The more nodes and links a network contains, the greater the number of paths. A key factor in differentiating these paths will then be their length, or the separating the nodes.\nThe distance between two nodes is given by the length of the path separating two nodes. The length is then given by the number of links separating the two nodes. For example, in figure two different paths are given to connect nodes G and C. The first path, in figure \\(\\ref{notion_distance_bi}\\) has a length of 4, giving a distance of 4 between the two nodes. The second path in figure \\(\\ref{notion_distance_uni}\\) has a distance of 6, giving a distance of 6 between G and C. Both distances and paths are valid, but we’ll often use the . This is calculated by identifying all possible paths between two nodes and retaining the shortest distance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the case where no path exists between two nodes, by convention, we consider the distance between the two nodes to be \\(\\infty\\)..\nA special case of a path is the . A loop is a path leading from a node to itself. In figure \\(\\ref{Notion_loop}\\), a loop is shown in a directed network and in an undirected network. Loops can be problematic in some cases when we want to calculate distances or identify paths. For example, to calculate the distance between A and C, a correct path would be:\n\nA \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) A \\(\\mapsto\\) D \\(\\mapsto\\) E \\(\\mapsto\\) B \\(\\mapsto\\) C\n\nTheoretically, there are an infinite number of paths between A and C, as we could include the loop A - D - E - B - A an infinite number of times. In some cases, software will require an acyclic network, which implies a loop-free network, to enable algorithms to run.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we’ll start by presenting a number of indicators. Initially, we’ll focus on indicators at node level.\nIn order to illustrate the different indicators, we will mobilize a network extracted from real data. This is a collaborative network extracted from research projects funded by the French National Research Agency around 5G technologies. The network is given in figure 3.\n\nWhen analyzing a network, the first valuable piece of information is the number of connections for each node. This provides a measure of the number of collaborators (or number of citations, investors, co-authors, etc.). This indicator is called the .\n\n\n\n\n\n\nDefinition: degree\n\n\n\nThe degree of a node is the number of nodes it connects to. We can measure the degree by counting the number of links.\n\n\nIn the example of the network in figure \\(\\ref{fig_ind_degree}\\), node E has two connections (with and ), so its degree is 2. We then note \\(d_E = 2\\). The nodes and have a degree of 2, and a degree of 3.\n\n\n\n\n\nThis is a caption for the figure. \n\n\n\n\nIf we calculate the degrees of the network nodes in 1 we can visualize the degree by a color gradient. In 1 we visualize the degree of the nodes by a green gradient. The darker the color, the higher the degree.\n\nThe degree visualization shows that some nodes, even if central, have a relatively low degree (ETIS, IRCICA). Orange has the highest degree, and therefore the largest number of employees, followed by the Institut Mines Telecom (which is more off-center).\nThe degree is a simple but effective measure for differentiating the positioning of nodes in a network.\nThe Degree in a directed network\nIf the network we’re analyzing is a directed one, we need to take into account the direction of interaction. A node can have both incoming and outgoing links. It therefore seems natural to propose two degrees in a directed network.\n\n\n\n\n\n\nDefinition: in-degree\n\n\n\nThe in-degree of a node corresponds to the number of links pointing to the node.\n\n\n\n\n\n\n\n\nDefinition: out-degree\n\n\n\nThe in-degree of a node corresponds to the number of links originating from the node, pointing to other nodes.\n\n\nFor example, in figure \\(\\ref{Reseau_degre_dirige}\\), the incoming degree of node is \\(2\\), since two links point to it. Since no links point to other nodes from \\(E\\), its outgoing degree is \\(0\\). The logic is therefore the same as for the classic degree, with the only difference being the directions.\n\n\n\n\n\n\n\n\n\nDegree in a weighted network\nThe weights of interactions can be included in the degree calculation, so we speak of . The latter is obtained by summing the weights of the node’s links. For example, in Figure 1, node E has two links, one with a weight of 1 and one with a weight of 2. The weighted degree of node E is therefore 3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the aims of network analysis is to identify nodes with an important position in the network structure. Important can be defined in different ways: in the case of degree, it’s the number of links, but we can also look at things in a more structural way. If we take the network shown in figure , it’s clear that the position of Orange and CEA differ. The role of these two players is therefore different from a structural point of view. Orange has a more position than CEA. Intuitively, we can consider that the centrality of a node is linked to its importance in a network, as it reflects a higher level of interconnectivity than a node positioned more on the periphery. In other words, the smaller the distance between a node and all the other nodes in the network, the more central the node is in the network.\nThe idea of centrality is reflected in various indicators that measure the centrality of a node with a slightly different objective. We’ll introduce some of these indicators below.\nBetweenness Centrality\nA first vision of centrality is given in terms of flows in the network. Let’s suppose that the network represents interactions between individuals, and that a virus is circulating between them. A person on the periphery of the network is less likely to receive the virus, as it has to pass through a large number of nodes before reaching him or her. A central player is located on a large number of paths, exposing him to a greater risk of the virus reaching him before it reaches the entire network.\nIf we return to our collaboration network, we can consider that collaboration involves the exchange of ideas and information. These elements pass from node to node (possibly degrading or enriching themselves) in order to spread throughout the entire network. In the context of R&D, the pooling of the knowledge of different players can result in innovation. The logic then is that the position of a node is important if it is able to capture the information flows passing through the network.\nTo measure the importance of a node from a flow capture point of view, we’re going to identify the number of on which a node is positioned. More precisely, we’ll identify the number of shortest paths between each pair of nodes, and how many of them a given node is positioned on.\nFormally, the betweenness centrality is computed by:\\\n\n\n\n\n\n\nBetweennes Centrality formula:\n\n\n\n\\[\\begin{equation}\n    BC = \\sum_{k \\neq j, i \\in \\{k,j\\}} \\frac{\\frac{P_i(kj)}{P(kj)}}{\\frac{(n-1)\\cdot(n-2)}{2}}\\\\\n\\end{equation}\\]\n\n\nThe nominator calculates the number of paths between nodes \\(k\\) and \\(j\\) (\\(P(kj)\\)) and the number of paths between \\(k\\) and \\(j\\) on which node \\(i\\) is positioned (\\(P_i(kj)\\)). The ratio therefore gives the fraction of paths between \\(k\\) and \\(j\\) on which \\(i\\) is positioned.\nThe denominator is there to normalize the value for the size of the network. Betweenness Centrality therefore has a value between 0 and 1, the higher the value, the more central the node. This indicator was proposed by (Freeman 1977).\nExample\nLet’s take an example of calculation using the network in Figure 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s calculate the BC of node \\(C\\). We need to start by identifying the shortest paths between all the nodes and then count how many of these paths node \\(C\\) is positioned on. In the table \\(\\ref{table_example_BC}\\), the first column lists each pair of nodes. There’s only one path between \\(A\\) and \\(B\\) (\\(A \\mapsto C \\mapsto B\\)). There is therefore a single shortest path (\\(P(AB) = 1\\)) and node \\(C\\) is positioned on this path (\\(P_C(AB)\\)). This calculation is repeated for every other pair of nodes as shown in the table below:\n\n\n\n\n\n\n\n\nLink\nNumber of shortest paths with C \\(P_i(kj)\\)\nNumber of shortest paths \\(P(kj)\\)\n\n\n\n\nA-B\n1\n1\n\n\nA-J\n1\n1\n\n\nB-J\n1\n1\n\n\nA-G\n1\n1\n\n\nB-G\n1\n1\n\n\nB-I\n1\n1\n\n\nA-I\n1\n1\n\n\nB-H\n1\n1\n\n\nA-H\n1\n1\n\n\n\nThe final centrality score of node \\(C\\) is given by:\n\\[\\begin{equation}\nBC_C = \\frac{1+1+1+1+1+1+1+1+1}{\\frac{(n-1)\\cdot(n-2)}{2}} = \\frac{9}{15} = 0.6\n\\end{equation}\\]\nif we apply this calculation to the collaboration network, and color the nodes according to the betweenness centrality score, we obtain the network in Figure 1.\n\nThis centrality indicator highlights a number of players, notably Orange, CEA and ITM, with high centrality. The CEA, which is located on the periphery, positions itself on short paths by connecting to more central players, reducing the distance between itself and the other players in the network.\nThe BC thus makes it possible to identify players who act as links between different parts of the network. The CEA, for example, is the only actor linking the actors to its left with the rest of the network. As such, it controls the flow of information between these two parts of the network. This position is called . The BC makes it easier to identify actors with this particular (and valuable) position.\nCloseness Centrality\nIn contrast to BC, we can also consider a node to be important in a network when it is able to disseminate information rapidly throughout the network. For this reason, the indicator is called Closeness Centrality (CC) and was proposed by Mark Newman ((Newman 2005)). Instead of considering that a node is central because it receives a lot of information, we’ll consider that a node is central when it can quickly reach the other nodes. Once again, the concepts of distance and path are used here. The idea and calculation are similar to those of BC.\nIn mathematical terms, the Closeness Centrality is measured as follows:\\ \\[\\begin{equation}\nCC_i = \\frac{1}{avg(L(n,m)}\n\\end{equation}\\]\nWith \\(L(n,m)\\) the distance of the shortest path between \\(n\\) and \\(m\\).\\\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compare the Closeness Centrality of nodes \\(C\\) and \\(D\\) in figure \\(\\ref{nw_example_CC}\\). We need the shortest distance between \\(C\\) and all other nodes. For nodes \\(C\\) we get:\n\nC-A Shortest path at a distance of 1\nC-B Shortest path at a distance of 1\nC-C The path from the node to itself is not taken into account\nC-D Shortest path at a distance of 2\nC-E Shortest path at a distance of 2\nC-F Shortest path at a distance of 1\n\n\\[\\begin{equation}\nCC(C)\\frac{1}{\\frac{1+1+2+2+1}{5}} = 0.71   \n\\end{equation}\\]\nFor node \\(D\\) we have:\n\nD-A Shortest path at a distance of 1\nD-B Shortest path at a distance of 2\nD-C Shortest path at a distance of 2\nD-D The path from the node to itself is not taken into account\nD-E Shortest path at a distance of 3\nD-F Shortest path at a distance of 1\n\n\\[\\begin{equation}\nCC(D)\\frac{1}{\\frac{1+2+2+3+1}{5}} = 0.55\n\\end{equation}\\]\nSo node \\(C\\) has a more central position in the network than node \\(D\\). From an information dissemination point of view, node \\(C\\) has a more important position than node \\(D\\).\nBy calculating the CC in the ANR project network and coloring the nodes according to CC, we obtain the network shown in figure \\(\\ref{Reseau_fil_rouge_CC}\\). Closeness Centrality identifies actors other than the BC. Nodes with the highest BC have a lower CC. Nodes in a dense community tend to have a higher score, as they reach their neighbors in a very empty space. We also identify nodes at the heart of the network that have a more important position from a broadcasting point of view, notably CNES and Parrot.\n\nRadiality\nOne way of assessing the importance of a node in a network is to quantify its impact on the rest of the network. This impact strongly depends on the node’s position in the network. A node that interconnects several communities is important for the structuring of the network; if the node were to leave the network, the impact on the structure would be great. A node with a relatively isolated position has a more limited impact. To quantify just how important a node is, we look at a measure introduced by Valente and Foreman (1998): Radiality.\nRadiality is a measure similar to centrality, quantifying the importance of the node in the network from a community interconnection point of view. The major difference with a centrality measure is that radiality is adjusted by the diameter of the network: the higher the radiality, the closer the node is to all other nodes and therefore impacts different communities (but not necessarily all of them). If the radiality is low, the node is only locally important, so it’s more likely to be on the periphery of the network. Its calculation is based on the average distance between the node and all other nodes in the network. This distance is then adjusted by the size of the network (its diameter).\n\\[\\begin{equation}\nR_a=\\frac{\\sum_{i \\in N_a}[L(a,m)-(D+1)]}{k}\n\\end{equation}\\]\nWhere \\(L(a,m)\\) is the shortest path between node \\(a\\) and all other nodes in the network, \\(D\\) is the diameter of the network and \\(k\\) is the number of nodes in the component. The measure is a relative one, giving an idea of isolation (far from all the others) or proximity (close to all the others).\nSince this is not really a measure of centrality, it must be interpreted in combination with other indicators such as eccentricity and centrality. A node with a high score on these three indicators has an important, central position in the network.\nIn the case of our example network of ANR projects, the result is shown in figure \\(\\ref{Reseau_fil_rouge_rad}\\). The nodes that are highlighted do indeed connect different communities. The notion is slightly different from that of BC, which captures the importance between a community and the rest of the network. Here, nodes at the heart of the network have a higher score than the periphery, with the highest score for nodes located on a higher number of communities.\n\nTopology Coefficient\nIt can be interesting to identify the nodes that play an important role in a community, as well as the nodes that interconnect communities. A node at the center of its community is an influential node that can influence decision-making for its community. A node that connects different communities has a strategic position, giving it access to information from different communities. This position is favorable for an innovator, for example, seeking to combine different technologies. To identify community leaders and gatekeepers, we use the topology coefficient introduced by Ravasz et al. in 2002 : .\nThe topology coefficient quantifies the proportion of nodes that an individual has in common with the other individuals in his network (on average). The higher the score for a node, the denser its neighborhood. The probability of being at the center of a community increases as the score rises. If the score is low, the firm has very few neighbors in common, so it’s in a strategic position connecting different communities. The more neighbors a node has in common with the rest of the network, the denser its direct neighborhood. The calculation of the topology coefficient is therefore based on identifying the number of neighbors that the node has in common with all the other nodes in the network (on average).\n\\[\\begin{equation}\nT_n = \\frac{avg(J(n,m))}{k_n}\n\\end{equation}\\]\nIn this formula, \\(J(n,m)\\) corresponds to the number of neighbors shared by two nodes in the network. \\(avg()\\) implies that we take the average. \\(k_n\\) is the number of neighbors of node \\(n\\). The score is between 0 and 1. To illustrate, let’s take the network shown in figure \\(\\ref{nw_example_topo}\\) and calculate the score for node \\(m\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe begin by looking at the neighbors of node \\(m\\): - \\(m - h\\) - There are no common neighbors, but there is a direct link between \\(m\\) and \\(h\\). J(m,h) = 1. - \\(m - i\\) - There are no common neighbors, but there is a direct link between \\(m\\) and \\(i\\). J(m,i) = 1. - \\(m - c\\) - There are no common neighbors, but there is a direct link between \\(m\\) and \\(c\\). J(m,c) = 1.\nFor all other nodes, there is neither a direct link nor a common neighbor, so for all these links \\(J(\\cdot,\\cdot) = 0\\).\nWe therefore have \\(T_m = \\frac{avg(1+1+1}{3}=\\frac{\\frac{1}{3}(3)}{3} = 0. 333\\)\nAs the number of nodes in the network increases ( \\(k_n\\) increases), the coefficient mechanically decreases. We therefore interpret this coefficient in relative terms, comparing it with the scores of other nodes in the network.\\(\\ref{Reseau_fil_rouge_topology}\\) gives an illustration of this indicator in the ANR project network.\n\nEigenvector Centrality\nThe centrality measures proposed so far identify the importance of a node based on a measure of its distance from other nodes. We can also consider that the importance of a node depends on the nodes to which it is connected. In other words, a node is important because it is connected to important nodes.\nImagine that a researcher with few connections, but who works with the most important researchers, could be more important than a researcher with many connections to lesser-known researchers. Eigenvector centrality (EC) is a measure that integrates this dimension. A node with a high EC is connected to important nodes in the network. The potential for this node to become important in the future is high. This measure ranges from 0 to 1. The higher the measure, the more important the node.\nThe calculation of this measure is more complex than the others. The indicator is obtained by spectral decomposition. Using the Perron-Frobenius theorem, we know that the largest eigenvector of this decomposition contains the centrality scores, hence the name: . To illustrate the indicator, the CE has been calculated for the various nodes in the network of ANR projects shown in figure \\(\\ref{Reseau_fil_rouge_EC}\\).\n\nFrom a mathematical perspective: If \\(A\\) is the adjacency matrix where \\(A_{ij}= 1\\) when there is a link between nodes \\(i\\) and \\(j\\) and \\(A_{ij} = 0\\) when there is no link. Then the eigenvectos centrality of node \\(i\\) is proportional to the sum of the centralities of its neighbors:\n\\[\\begin{equation}\nEC_i = \\frac{1}{\\lambda}\\sum_j A_{ij}EC_j\n\\end{equation}\\]\nThis shows that when we use compute the EC of a node, the value increases when the node is connected to nodes with a high EC.\nLe clustering\nBeyond the number of links and the centrality of the node, the direct neighborhood of the node also holds importance. A node positioned in a dense neighborhood (textit{i.e}, with a significant number of connections between its neighbors) does not have the same impact on the network as a node positioned with sparse connections.\nIn order to assess the level of interconnection in the direct neighborhood of the node, it is necessary to measure the propensity for interconnection among the neighbors of a node. At the maximum, all of a node’s neighbors are interconnected (the network is then complete, as in the network on the left in Figure \\(\\ref{nw_exemple_clust}\\)). Thus, for node \\(a\\), the entire set of neighbors is interconnected, unlike node \\(g\\), which has two neighbors that are not connected. The neighborhood of node \\(g\\) is therefore less dense than the neighborhood of node \\(b\\).\nMeasuring the level of interconnection relies on the identification of triangles, i.e., groups of three connected nodes ({\\(a\\) - \\(b\\) - \\(c\\)} in the network on the left). By counting how many of these triangles are present relative to the number of possible triangles, we obtain a measure known as clustering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{equation}\nCl_a = \\displaystyle\\frac{\\mbox{Number of observed triangles}}{\\mbox{Number of possible triangles}}\n\\end{equation}\\]\nThe clustering coefficient gives a value between 0 and 1; the higher the value, the greater the clustering. The number can be interpreted as the fraction of triangles formed between the node and its neighborhood.\nTo illustrate, let’s calculate the clustering coefficient for nodes \\(a\\) and \\(e\\).\nWe begin by identifying the number of triangles:\nFor \\(a\\): \\({a - b - d}\\), \\({a - c - d}\\), and \\({a - b - c}\\) For \\(e\\): No triangles, so the numerator for \\(Cl_e = 0\\)\nIl convient maintenant de trouver le nombre potentiel de triangles. Autrement dit le nombre de triangles qui pourraient exister en connectant les voisins directs qui ne sont pas déjà connectés.\\\nNow, we need to find the potential number of triangles. In other words, the number of triangles that could exist by connecting the direct neighbors that are not already connected.\nFor \\(a\\): All the neighbors are interconnected, so the potential number is equal to the observed number: 3. For \\(e\\): {\\(e\\) - \\(g\\) - \\(h\\)}, {\\(e\\) - \\(g\\) - \\(f\\)}, and {\\(e\\) - \\(h\\) - \\(f\\)}. \\(e\\) could potentially be part of 3 triangles. Finally, by taking the ratio of the two:\nFor \\(a\\): \\(Cl_a = \\frac{3}{3} = 1\\) For \\(e\\): \\(Cl_e = \\frac{0}{3} = 0\\)\nA node with a high clustering coefficient is therefore densely interconnected in its neighborhood and, consequently, within its community. A node with a low clustering coefficient is more isolated compared to other nodes.\nIn Figure \\(\\ref{Reseau_fil_rouge_clust}\\), clustering is calculated for the network of ANR projects. Since the neighborhood of nodes is entirely defined by the partners of a project, we know that the number of triangles is significant in this type of network (each actor is linked to every other actor in a project). The most central nodes have a lower clustering coefficient than some other nodes in the core (Parrot, CNES, L2S, TRCOM). At the same time, the central nodes also participate in more projects. If the clustering for these nodes is lower, it implies that the central actors collaborate more with different actors, who in turn do not collaborate with each other. These actors therefore play a role as intermediaries.",
    "crumbs": [
      "Home",
      "Network Analysis",
      "The basics of Network Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NetworkIsLife",
    "section": "",
    "text": "This website regroups elements for text mining, social network analysis and programming"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Programming",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Topic_modelling.html",
    "href": "Topic_modelling.html",
    "title": "Topic Modelling",
    "section": "",
    "text": "We will use two data types for this tutorial:\n\nLexisNexis from which we have extracted articles about climate anxiety\nLens.org from which we have extracted patents about water desalination\n\nThe aims are:\n\nLearn how to perform topic modelling using R\nLearn how to prepare the data for topic modelling\nLearn how to adjust text preparation to optimize your results for a given question\nDecide on the optimal number of topics in your data\nConnect the topics to other variables in your dataset (players, time, inventors,…)\n\nThe datasets can be found on blackboard. If, howerver, you prefer working on your own data/topic we will first start with a quick brief on how to export data from LexisNexis and import this data into R.\n\n\n\n\nEven though exporting data from LexisUni is quite straightforward, if we want to export in a format that can be directly read by R and used for textmining, we need to tick the right boxes. We will go through those steps now:\nStart with building a query in the advances query section. Make sure you check the AND and OR operators. \nBelow the text elements you can adapt the data range if required.\n\nThen click on search to launch the search process. Once the results are shown, make sure to click the switch to remove Group Duplicates (this should be on “On”). At this point adjust any other filter. Be particulary cautious with the language. Many language models are trained for one language so remove any document not in that language if you use a mono-lingual model.\n Once you’re happy with the dataset, go to the “download button”:  A window will appear, this is were you need to be sure to click the same boxes as in the following screenshots. Ensure that the format is Word (docx) and that the documents are grouped into one.\n\nThe second screen should look like this: (these should be the basic options):\n\nYou can then click on “download”.\nDo this as many times are required to download all the articles (100 per 100). Creating an account with your uu email will make this process slightly more streamlined and faster.\nPut all the files into one folder. We will then use the LexisNexisTools package to import and format this data to make it directly usable.\nFirst we need to search for all the documents, using the list.files() function we create a list of all the .docx documents in the “LN” folder. Note that the LN folder is created by me to store the files, replace LN with the name of the folder in which you store the files. If the files are stored directly at the root of the current WD, use ““.\n\nmy_files &lt;- list.files(pattern = \".docx\", path = \"LN\", full.names = TRUE, recursive = TRUE, ignore.case = TRUE)\n\nAt this stage the data is not yet imported, we merely created a list with paths to the files. We will now load those files into R with the lnt_read() function. This will load the data into a specific object with a specific format that we cannot use directly. We therefore transform this object with the lnt_convent() function. We specify “to =”data.frame” so that the result is a dataframe that we can use.\n\nlibrary(LexisNexisTools)\ndat &lt;- lnt_read(my_files) #Object of class 'LNT output'\nLN_dataframe = lnt_convert(dat, to = \"data.frame\")\n\nWe now have the data loaded in R and ready for use. We will perform two extra actions before we continue to ensure that we don’t run into trouble later on. We start with the removal of identical articles (even though the switch is on, there are still some left…) and we remove the articles that are too short for the analysis. We use the nchar() function to count the number of characters in each article and remove those with less than 200 characters.\n\nLN_dataframe = unique(LN_dataframe)\nLN_dataframe$length = nchar(LN_dataframe$Article)\nLN_dataframe = subset(LN_dataframe, LN_dataframe$length &gt;= 200)\n\nThe dataset is now ready for use.\n\n# loading our custom functions\nlibrary(tm)\nlibrary(udpipe)\nlibrary(tidyverse)\nlibrary(textstem)\nclean_text_lemma &lt;- function(text){\n  #text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with \"\".\n  text &lt;- gsub(\"\\\\.\", \"\", text)\n  text &lt;- gsub(\"\\\\;\", \"\", text)\n  text &lt;- gsub(\"\\\\-\", \"\", text)\n  text &lt;- gsub(\"\\\\+\", \"plus\", text)\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  # here we apply lemmatization instead of stemming:\n  lemma_dico &lt;-  make_lemma_dictionary(text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\n  text &lt;- lemmatize_strings(text, dictionary = lemma_dico)\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n  text &lt;- gsub(\"  \", \" \", text)\n}\nc_value = function(words){\n# we need to compute the number of higher order terms and the frequency of these terms\n# we initiate two empty columns in which we can store this information\nwords$terms_containing_term &lt;- 0\nwords$Sum_freq_higher_terms &lt;- 0\n# We make sure the data has a dataframe format\nwords = as.data.frame(words)\n# we now loop over all the words to check how often they are nested\nfor(i in 1:dim(words)[1]){\n  # first we check in which term the term is nested\n  # if the term is part of another string it will return TRUE, FALSE otherwise\n  # The str_detect() function searches for a pattern in a second string and returns\n  # True if the pattern is part of the string\n  words$tmp = stringr::str_detect(words$keyword, words[i,1])\n  \n  # We then only keep the words that contain the word we are searching for\n  tmp = subset(words, words$keyword != words[i,1] & words$tmp == TRUE)\n  # The number of strings in which the pattern is nested is then simply the \n  # dimension of the dataframe we just found\n  words[i,4] &lt;- dim(tmp)[1]\n  # the sum of the frequencies is simply the sum of the individual frequencies\n  words[i,5] &lt;- sum(tmp$freq)\n}\n# now compute the c-value\n# we first adda column that will contain this value\nwords$C_value = 0\n# then we check if there are nested terms or not and apply the formula accordingly\nwords$C_value = case_when(\n    words$terms_containing_term == 0 ~ log2(words$ngram) * words$freq,\n    #keyword | n_gram | freq dataset | terms count terms | sum_freq_high\n    words$terms_containing_term != 0 ~ (log2(words$ngram) * (words$freq - (1/words$terms_containing_term) * words$Sum_freq_higher_terms))\n)\n# to make this work with the other functions we remove the \"tmp\" column...\nwords = words[,-6]\n#... and we reorganise the columns so that we do not have to adjust the other functions\nwords = words[,c(1,3,2,4,5,6)]\nreturn(words)\n}\nterm_extraction = function(Text_data, max_gram, min_freq){\n  # we need three elements of importance for the function to run\n  # the data, the max ngram and the minimum frequency\n  Text_df = as.data.frame(Text_data[1,])\n  x &lt;- udpipe_annotate(ud_model, x = as.character(Text_df[1,2]), doc_id = Text_df[1,1])\n  x &lt;- as.data.frame(x)\n  \n  stats &lt;- keywords_rake(x = x, term = \"lemma\", group = \"doc_id\", \n                         relevant = x$upos %in% c(\"NOUN\", \"ADJ\"))\n  stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword))\n  \n  \n  x$phrase_tag &lt;- as_phrasemachine(x$upos, type = \"upos\")\n  stats &lt;- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \n                            pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                            is_regex = TRUE, detailed = FALSE)\n  \n  stats = subset(stats, stats$ngram &gt;= 1 & stats$ngram &lt;= max_gram)\n  stats = subset(stats, stats$freq &gt;= min_freq)\n  return(stats)\n}\n# specify the dictionary\nmy_dico = c(\"a method\", \"patent\")\n# load the language model\nlibrary(udpipe)\nud_model &lt;- udpipe::udpipe_download_model(language = \"english\")\n#save(ud_model, file = \"ud_model.rdata\")\nud_model &lt;- udpipe::udpipe_load_model(ud_model)\n\n\n\n\n\nWe start by prepping the text as we did in the first tutorial. We use a loop since it is likely that errors occur in this process that will push us to adjust parameters. The use of a loop makes it easier to pause and continue where we left of. Once the code is stabilised you can use the map() function from tidyverse to make this more efficient and run faster. Or even use multi-threading with the foreach package.\nColumn 11 contains the text, we start by lemmatizing the text.\n\nlibrary(textstem)\nLN_dataframe = LN_dataframe[-c(382,431, 464, 1087),]\nfor(i in 1:dim(LN_dataframe)[1]){\n  LN_dataframe[i,11] &lt;-clean_text_lemma(LN_dataframe[i,11])\n  # now er extract the terms\n  tmp &lt;- term_extraction(LN_dataframe[i,c(1,11)], max_gram = 4, min_freq =  1)\n  # and compute the c_value\n  tmp &lt;- c_value(tmp)\n  # we remove words with low values\n  tmp &lt;- subset(tmp, tmp$C_value &gt; 0)\n  # we combine the tokens\n  tmp$keyword &lt;- apply(as.matrix(tmp$keyword), 1, gsub, pattern = \" \", replacement = \"_\")\n  # and we put the text back into the document\n  LN_dataframe[i,11] &lt;- paste(tmp$keyword, collapse = \" \")\n}\n# we save the result in a .rdata file in case we make a mistake later\nsave(LN_dataframe, file = \"LN_dataframe_POS_lemmcleaning.rdata\")\n\nNow that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the DocumentTermMatrix() function from the tm package. This function has one argument which is a dataframe with the text of the corpus.\n\nlibrary(tm)\n# create the document-Term-matrix\ndtm &lt;- DocumentTermMatrix(LN_dataframe$Article)\n\nBased on this matrix we will now try to define how many topics we need to extract. For this we use the FindTopicsNumber and FindTopicsNumber_plot from the ldatuning package.\nThe FindTopicsNumber functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The seq() function creates a vector which starts at the from arguments, stops at the to argument. The size of the steps is set by the by argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, …) we would write seq(from = 2, to = 60, by = 5).\nWe then specify the metrics we want to compute, we have discussed these in the lecture.\n\n\n\n\n\n\nWarning\n\n\n\nFor an unknown reason, the “Griffiths2004” function does not work on mac OSX. It should work for windows users.\n\n\nThe mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algo is stuck, still running or finished.\n\nlibrary(ldatuning)\ntopic_num &lt;- FindTopicsNumber(\n  dtm,\n  topics = seq(from = 2, to = 60, by = 5),\n  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  mc.cores = 8L,\n  verbose = TRUE\n)\nFindTopicsNumber_plot(topic_num)\n\nNow that we have an idea of how many topics we need, so let’s extract the topics. We will use the LDA function from the topicmodels package.\n\nlibrary(topicmodels)\nlibrary(tidytext)\n# perform topic modelling\ntopics_model &lt;- LDA(dtm, k = 7)\n# get the betas\nbetas &lt;- tidytext::tidy(topics_model, matrix = \"beta\")\n# subset the betas for results\nbetas2&lt;- subset(betas, betas$topic %in% c(2,3,4,5,7))\n\nWe now have the topics and the betas for each topic-term couple. We only keep the highest values for each topic:\n\nap_top_terms &lt;- betas2 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nAnd now we visualise some of the results:\n\nlibrary(ggplot2)\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\nAdjust the parameters of the previous text cleaning functions. Adjust the dictionnary, maybe switch the noun phrases option ((A|N)*N(P+D*(A|N)*N)*) to an equivalent that includes actions.\n\n\n\nNow try the same logic on patent data. Check the results step by step. Adjust the dictionary etc. to suit patent data. What do you look for in the case of patent data, how is this different from news data?"
  },
  {
    "objectID": "Topic_modelling.html#exporting-and-importing-lexisuni-data",
    "href": "Topic_modelling.html#exporting-and-importing-lexisuni-data",
    "title": "Topic Modelling",
    "section": "",
    "text": "Even though exporting data from LexisUni is quite straightforward, if we want to export in a format that can be directly read by R and used for textmining, we need to tick the right boxes. We will go through those steps now:\nStart with building a query in the advances query section. Make sure you check the AND and OR operators. \nBelow the text elements you can adapt the data range if required.\n\nThen click on search to launch the search process. Once the results are shown, make sure to click the switch to remove Group Duplicates (this should be on “On”). At this point adjust any other filter. Be particulary cautious with the language. Many language models are trained for one language so remove any document not in that language if you use a mono-lingual model.\n Once you’re happy with the dataset, go to the “download button”:  A window will appear, this is were you need to be sure to click the same boxes as in the following screenshots. Ensure that the format is Word (docx) and that the documents are grouped into one.\n\nThe second screen should look like this: (these should be the basic options):\n\nYou can then click on “download”.\nDo this as many times are required to download all the articles (100 per 100). Creating an account with your uu email will make this process slightly more streamlined and faster.\nPut all the files into one folder. We will then use the LexisNexisTools package to import and format this data to make it directly usable.\nFirst we need to search for all the documents, using the list.files() function we create a list of all the .docx documents in the “LN” folder. Note that the LN folder is created by me to store the files, replace LN with the name of the folder in which you store the files. If the files are stored directly at the root of the current WD, use ““.\n\nmy_files &lt;- list.files(pattern = \".docx\", path = \"LN\", full.names = TRUE, recursive = TRUE, ignore.case = TRUE)\n\nAt this stage the data is not yet imported, we merely created a list with paths to the files. We will now load those files into R with the lnt_read() function. This will load the data into a specific object with a specific format that we cannot use directly. We therefore transform this object with the lnt_convent() function. We specify “to =”data.frame” so that the result is a dataframe that we can use.\n\nlibrary(LexisNexisTools)\ndat &lt;- lnt_read(my_files) #Object of class 'LNT output'\nLN_dataframe = lnt_convert(dat, to = \"data.frame\")\n\nWe now have the data loaded in R and ready for use. We will perform two extra actions before we continue to ensure that we don’t run into trouble later on. We start with the removal of identical articles (even though the switch is on, there are still some left…) and we remove the articles that are too short for the analysis. We use the nchar() function to count the number of characters in each article and remove those with less than 200 characters.\n\nLN_dataframe = unique(LN_dataframe)\nLN_dataframe$length = nchar(LN_dataframe$Article)\nLN_dataframe = subset(LN_dataframe, LN_dataframe$length &gt;= 200)\n\nThe dataset is now ready for use.\n\n# loading our custom functions\nlibrary(tm)\nlibrary(udpipe)\nlibrary(tidyverse)\nlibrary(textstem)\nclean_text_lemma &lt;- function(text){\n  #text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with \"\".\n  text &lt;- gsub(\"\\\\.\", \"\", text)\n  text &lt;- gsub(\"\\\\;\", \"\", text)\n  text &lt;- gsub(\"\\\\-\", \"\", text)\n  text &lt;- gsub(\"\\\\+\", \"plus\", text)\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  # here we apply lemmatization instead of stemming:\n  lemma_dico &lt;-  make_lemma_dictionary(text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\n  text &lt;- lemmatize_strings(text, dictionary = lemma_dico)\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n  text &lt;- gsub(\"  \", \" \", text)\n}\nc_value = function(words){\n# we need to compute the number of higher order terms and the frequency of these terms\n# we initiate two empty columns in which we can store this information\nwords$terms_containing_term &lt;- 0\nwords$Sum_freq_higher_terms &lt;- 0\n# We make sure the data has a dataframe format\nwords = as.data.frame(words)\n# we now loop over all the words to check how often they are nested\nfor(i in 1:dim(words)[1]){\n  # first we check in which term the term is nested\n  # if the term is part of another string it will return TRUE, FALSE otherwise\n  # The str_detect() function searches for a pattern in a second string and returns\n  # True if the pattern is part of the string\n  words$tmp = stringr::str_detect(words$keyword, words[i,1])\n  \n  # We then only keep the words that contain the word we are searching for\n  tmp = subset(words, words$keyword != words[i,1] & words$tmp == TRUE)\n  # The number of strings in which the pattern is nested is then simply the \n  # dimension of the dataframe we just found\n  words[i,4] &lt;- dim(tmp)[1]\n  # the sum of the frequencies is simply the sum of the individual frequencies\n  words[i,5] &lt;- sum(tmp$freq)\n}\n# now compute the c-value\n# we first adda column that will contain this value\nwords$C_value = 0\n# then we check if there are nested terms or not and apply the formula accordingly\nwords$C_value = case_when(\n    words$terms_containing_term == 0 ~ log2(words$ngram) * words$freq,\n    #keyword | n_gram | freq dataset | terms count terms | sum_freq_high\n    words$terms_containing_term != 0 ~ (log2(words$ngram) * (words$freq - (1/words$terms_containing_term) * words$Sum_freq_higher_terms))\n)\n# to make this work with the other functions we remove the \"tmp\" column...\nwords = words[,-6]\n#... and we reorganise the columns so that we do not have to adjust the other functions\nwords = words[,c(1,3,2,4,5,6)]\nreturn(words)\n}\nterm_extraction = function(Text_data, max_gram, min_freq){\n  # we need three elements of importance for the function to run\n  # the data, the max ngram and the minimum frequency\n  Text_df = as.data.frame(Text_data[1,])\n  x &lt;- udpipe_annotate(ud_model, x = as.character(Text_df[1,2]), doc_id = Text_df[1,1])\n  x &lt;- as.data.frame(x)\n  \n  stats &lt;- keywords_rake(x = x, term = \"lemma\", group = \"doc_id\", \n                         relevant = x$upos %in% c(\"NOUN\", \"ADJ\"))\n  stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword))\n  \n  \n  x$phrase_tag &lt;- as_phrasemachine(x$upos, type = \"upos\")\n  stats &lt;- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), \n                            pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                            is_regex = TRUE, detailed = FALSE)\n  \n  stats = subset(stats, stats$ngram &gt;= 1 & stats$ngram &lt;= max_gram)\n  stats = subset(stats, stats$freq &gt;= min_freq)\n  return(stats)\n}\n# specify the dictionary\nmy_dico = c(\"a method\", \"patent\")\n# load the language model\nlibrary(udpipe)\nud_model &lt;- udpipe::udpipe_download_model(language = \"english\")\n#save(ud_model, file = \"ud_model.rdata\")\nud_model &lt;- udpipe::udpipe_load_model(ud_model)"
  },
  {
    "objectID": "Topic_modelling.html#topic-modelling-on-news-data",
    "href": "Topic_modelling.html#topic-modelling-on-news-data",
    "title": "Topic Modelling",
    "section": "",
    "text": "We start by prepping the text as we did in the first tutorial. We use a loop since it is likely that errors occur in this process that will push us to adjust parameters. The use of a loop makes it easier to pause and continue where we left of. Once the code is stabilised you can use the map() function from tidyverse to make this more efficient and run faster. Or even use multi-threading with the foreach package.\nColumn 11 contains the text, we start by lemmatizing the text.\n\nlibrary(textstem)\nLN_dataframe = LN_dataframe[-c(382,431, 464, 1087),]\nfor(i in 1:dim(LN_dataframe)[1]){\n  LN_dataframe[i,11] &lt;-clean_text_lemma(LN_dataframe[i,11])\n  # now er extract the terms\n  tmp &lt;- term_extraction(LN_dataframe[i,c(1,11)], max_gram = 4, min_freq =  1)\n  # and compute the c_value\n  tmp &lt;- c_value(tmp)\n  # we remove words with low values\n  tmp &lt;- subset(tmp, tmp$C_value &gt; 0)\n  # we combine the tokens\n  tmp$keyword &lt;- apply(as.matrix(tmp$keyword), 1, gsub, pattern = \" \", replacement = \"_\")\n  # and we put the text back into the document\n  LN_dataframe[i,11] &lt;- paste(tmp$keyword, collapse = \" \")\n}\n# we save the result in a .rdata file in case we make a mistake later\nsave(LN_dataframe, file = \"LN_dataframe_POS_lemmcleaning.rdata\")\n\nNow that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the DocumentTermMatrix() function from the tm package. This function has one argument which is a dataframe with the text of the corpus.\n\nlibrary(tm)\n# create the document-Term-matrix\ndtm &lt;- DocumentTermMatrix(LN_dataframe$Article)\n\nBased on this matrix we will now try to define how many topics we need to extract. For this we use the FindTopicsNumber and FindTopicsNumber_plot from the ldatuning package.\nThe FindTopicsNumber functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The seq() function creates a vector which starts at the from arguments, stops at the to argument. The size of the steps is set by the by argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, …) we would write seq(from = 2, to = 60, by = 5).\nWe then specify the metrics we want to compute, we have discussed these in the lecture.\n\n\n\n\n\n\nWarning\n\n\n\nFor an unknown reason, the “Griffiths2004” function does not work on mac OSX. It should work for windows users.\n\n\nThe mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algo is stuck, still running or finished.\n\nlibrary(ldatuning)\ntopic_num &lt;- FindTopicsNumber(\n  dtm,\n  topics = seq(from = 2, to = 60, by = 5),\n  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  mc.cores = 8L,\n  verbose = TRUE\n)\nFindTopicsNumber_plot(topic_num)\n\nNow that we have an idea of how many topics we need, so let’s extract the topics. We will use the LDA function from the topicmodels package.\n\nlibrary(topicmodels)\nlibrary(tidytext)\n# perform topic modelling\ntopics_model &lt;- LDA(dtm, k = 7)\n# get the betas\nbetas &lt;- tidytext::tidy(topics_model, matrix = \"beta\")\n# subset the betas for results\nbetas2&lt;- subset(betas, betas$topic %in% c(2,3,4,5,7))\n\nWe now have the topics and the betas for each topic-term couple. We only keep the highest values for each topic:\n\nap_top_terms &lt;- betas2 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nAnd now we visualise some of the results:\n\nlibrary(ggplot2)\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\nAdjust the parameters of the previous text cleaning functions. Adjust the dictionnary, maybe switch the noun phrases option ((A|N)*N(P+D*(A|N)*N)*) to an equivalent that includes actions.\n\n\n\nNow try the same logic on patent data. Check the results step by step. Adjust the dictionary etc. to suit patent data. What do you look for in the case of patent data, how is this different from news data?"
  },
  {
    "objectID": "useful_scripts.html",
    "href": "useful_scripts.html",
    "title": "Useful scripts and where to find them",
    "section": "",
    "text": "Suppose I have a DB with players located by lon and lat and i want to find the NUTS2/3 code associated with this:\n\nlibrary(sf)\n# first loas a shapefile of the region in which we search\nnuts2 &lt;- st_read(\"somepatch/NUTS_RG_20M_2021_4326.shp\")\nnuts2 &lt;- st_transform(nuts2, crs = 4326)\n# Example data frame with longitude and latitude\ndf &lt;- trd102_app_detail_loc_nuts[,c(\"GEO_LONG\", \"GEO_LAT\")]\ndf = na.omit(df)\n# Convert the data frame to an sf object\ndf_sf &lt;- st_as_sf(df, coords = c(\"GEO_LONG\", \"GEO_LAT\"), crs = 4326, agr = \"constant\")\n# Perform spatial join to match points to NUTS2 polygons\npoints_with_nuts2 &lt;- st_join(df_sf, nuts2)\n\nFind the localisation and eventually KVK number of players identified in data:\n\nNDS_pats_clean_companies_geoloc$searchname &lt;- gsub(\" \", \"\\\\+\", NDS_pats_clean_companies_geoloc$applicant_cleaned)\nfor(i in 1:dim(NDS_pats_clean_companies_geoloc)[1]){\n  if(i %% 100 == 0){print(i)}\n  if(NDS_pats_clean_companies_geoloc[i,3] == TRUE & is.na(NDS_pats_clean_companies_geoloc[i, 4]) == TRUE){ # we only search if it's a company and we haven't searched before\n  url = paste0(\"https://www.bedrijvenregister.nl/zoekresultaten?q_source=cta&q=\", NDS_pats_clean_companies_geoloc[i,5])\n  #url = paste0(\"https://www.telefoonboek.nl/zoeken/\",fabia_not_found[i,1],\"/\")\n  response &lt;- GET(as.character(url)) # get the information\n  NDS_pats_clean_companies_geoloc[i,4] &lt;- content(response, as = \"text\") # extract the content\n  #Sys.sleep(3)\n  #if(i %% 1000 == 0){Sys.sleep(300)}\n  }\n}\n\nDifferent approach:\n\npostcodes$long &lt;-NA\npostcodes$lan &lt;- NA\nfor(i in 3193:dim(postcodes)[1]){\n  if(i %% 100 == 0){print(i)}\n  pc = gsub(\" \",\"\",postcodes[i,1])\n  url = paste0(\"https://postcodebijadres.nl/\",pc)\n  test = GET(url)\n  content &lt;- content(test, as = \"text\")\n  postcodes[i,2] &lt;- str_extract(content, \"(?&lt;=Breedtegraad \\\\(N\\\\):&lt;\\\\/td&gt;&lt;td&gt;)(.*)(?=&lt;\\\\/td&gt;)\")\n  postcodes[i,3] &lt;- str_extract(content, \"(?&lt;=Lengtegraad \\\\(E\\\\):&lt;\\\\/td&gt;&lt;td&gt;)(.*)(?=&lt;\\\\/td&gt;)\")\n}\n\n\n\n\n\n\n\n# create a df with the names of the assignees\ndf &lt;- as.data.frame(unique(player_geoloc$final_player))\ncolnames(df) = \"applicant_name\"\ndf = na.omit(df)\n\n# Create a list of common company keywords\ncompany_keywords &lt;- c(\"CERAMICS\",\"HOSP\",\"MEDIC\",\"FONDATION\",\"FUNDACION\",\"PRODUCT\",\" KK\",\" S A\",\"DU PONT\",\"VISION\",\"CHEMI\",\"DEUTSCHES\",\" INT\",\" CO\",\"ELEKT\",\" ＳＥ\",\"BUND\",\"INSTR\",\"LICENS\",\"B V\",\"ELEC\",\"MICRO\",\"&\",\"SOLAR\",\"OPER\",\" AB\",\" SRL\",\" SE\",\"GERMANY\",\"NETHERLANDS\",\"AIR\",\"FRANCE\",\"FRAUNHOFER\",\"IMEC\",\"ASML\",\"PHILIPS\",\"SIEMENS\",\"KONINK\",\"ROYAL\",\" NV\",\" BV\",\"SCHOOL\",\"INSTIT\",\"CENT\",\"UNIV\",\"CONDU\",\"NANO\",\"PLASTIC\",\" AS\",\"PLC\",\"ＡＧ\",\"SYSTEM\",\"WERK\",\"INC\", \"LTD\", \"CORP\", \"LLC\", \"CO.\", \"GROUP\", \"COMPANY\", \"AG\", \"GMBH\", \" S.A.\", \" S.P.A.\", \" OY\", \" SPA\", \" SA\", \"TECH\", \"IND\", \"SOLU\")\n\n# Function to determine if a name is likely a person or a company\nis_person_or_company &lt;- function(name) {\n  # Check if the name contains any company-related keywords\n  if (any(str_detect(name, company_keywords))) {\n    return(\"Company\")\n  }\n  # Check if the name contains common patterns for person names\n  # Example: Names with 2 or 3 words and lack company-like keywords\n  words &lt;- str_split(name, \" \")[[1]]\n  \n  if (length(words) &gt;= 2 && length(words) &lt;= 3 && !any(str_detect(name, \"[0-9]\"))) {\n    return(\"Person\")\n  }\n  # Fallback to \"Company\" if none of the person rules matched\n  return(\"Company\")\n}\n\n# Apply the function to the applicant_name column\ndf &lt;- df %&gt;%\n  mutate(type = sapply(applicant_name, is_person_or_company))\n\n# Print the resulting data frame\nprint(df)\n\n\n\n\n\n\n\nStart with the dataframe in which each row has the elements that need to be linked in seperated by something: player_1;player_2;player_3 etc. Use splitstackshape::cSplit() to split the dataframe. Then use the following script on this df. Replace the start of the loop according to how many columns are in front of the player info: ID|year|players -&gt; start at 3 and adjust if you want to include this info into the final dataframe.\n\nplayer_geoloc_collab = as.matrix(player_geoloc_collab)\nfor(i in 2:(dim(player_geoloc_collab)[2]-1)){\n  for(j in (i+1):dim(player_geoloc_collab)[2]){\n    tmp = player_geoloc_collab[,c(1, i, j)]\n    tmp = na.omit(tmp)\n    if(i == 2 & j == 3){res = tmp}else{res = rbind(res, tmp)}\n  }\n}\n\n\n\n\n\nnetwork_creation = function(data, dynamic, sep){\n  library(tidyverse)\n  res = c(NA,NA,NA)\n  # we start by splitting the data\n  nw = splitstackshape::cSplit(data, 1, sep = sep)\n  nw = as.matrix(nw)\n  # garde-fou : if the dimension of the dataframe after splitting is the same, then there is no network\n  if(dim(nw)[2] == dim(data)[2]){\n    print(\"You idiot, there is no network to be made here!\")\n  }else{\n    # there is data, we need to make the list of links\n    # we do this by looping over the columns and combining them\n    for(i in 2:(dim(nw)[2]-1)){\n      for(j in (i+1):dim(nw)[2]){\n        tmp = cbind(nw[,1], nw[,i], nw[,j])\n        tmp = na.omit(tmp)\n        res = rbind(res, tmp)\n      }\n    }\n    res = as.data.frame(na.omit(res))\n    colnames(res) = c(\"Year\", \"Source\", \"Target\")\n    if(missing(dynamic) == FALSE){\n      res[,c(2,3)] = alpha.order(res[,c(2,3)])\n      res$linkid = paste(res[,2], res[,3], sep = \";\")\n      res = res %&gt;% group_by(linkid) %&gt;% summarize(\"weight\" = n(), \"first_year\" = min(Year), \"last_year\" = max(Year))\n      res = splitstackshape::cSplit(res, 1, sep = \";\")\n      colnames(res) = c(\"Weight\", \"First_year\", \"Last_year\", \"Source\", \"Target\")\n    }\n    return(res)\n  }\n}\n\n\n\n\n\n\n\n\nreduce.digits.fast = function (data, digit, sep){\n  # verifier le digit demandé\n  if (digit == 3){\n    y = function(x) {\n      substr(x, 1, 3)\n    }\n    z = function(x) {\n      tmp = x\n      x = na.omit(x)\n      x = unique(x)\n      x = paste(t(x), collapse = sep)\n    }\n    data &lt;- as.matrix(data)\n    result = matrix(NA, nrow = dim(data)[1])\n    data = splitstackshape::cSplit(data, 1,  sep = sep)\n    data = as.matrix(data)\n    data[] &lt;- vapply(data, y, character(1))\n    result[] = apply(data, 1, z)\n    #print(class(result))\n    return(result)\n  }\n  if (digit == 4){\n    # creer une fonction qui extrait le 4 premiers caracteres\n    y = function(x) {\n      substr(x, 1, 4)\n    }\n    # creer une fonction qui enlève les na, les doublons, recoller \n    z = function(x) {\n      tmp = x\n      x = na.omit(x)\n      x = unique(x)\n      x = paste(t(x), collapse = sep)\n    }\n    # ici traitement des donnees\n    data &lt;- as.matrix(data) # assurer que format soit matrice\n    result = matrix(NA, nrow = dim(data)[1]) # créer une matrice qui contiendra les resultats\n    data = splitstackshape::cSplit(data, 1, sep =sep) # couper les données\n    data = as.matrix(data) # remettre en matrice\n    data[] &lt;- vapply(data, y, character(1)) # appliquer la fonction y (soustraire) à toutes les lignes (vapply)\n    result[] &lt;- apply(data, 1, z) # Appliquer la fonction y sur toutes les\n    return(result)\n  }\n  if (digit == 7){\n    y = function(x) {\n      sub(\"/.*\", \"\", x)\n    }\n    z = function(x) {\n      tmp = x\n      x = na.omit(x)\n      x = unique(x)\n      x = paste(t(x), collapse = sep)\n    }\n    data &lt;- as.matrix(data)\n    result = matrix(NA, nrow = dim(data)[1])\n    data = splitstackshape::cSplit(data, 1, sep = sep)\n    data = as.matrix(data)\n    data[] &lt;- vapply(data, y, character(1))\n    result[] = apply(data, 1, z)\n    return(result)\n  }\n  if (digit == 9){return(data)}\n}\n\n\n\n\n\nalpha.order&lt;-function(data){\n  data&lt;-as.matrix(data)\n  for (i in 1:dim(data)[1]){\n    data[i,1] = textclean::replace_non_ascii(data[i,1], remove.nonconverted = TRUE)\n    data[i,2] = textclean::replace_non_ascii(data[i,2], remove.nonconverted = TRUE)\n    if (data[i,1] &gt; data[i,2]){\n      s&lt;-data[i,1]\n      data[i,1]&lt;-data[i,2]\n      data[i,2]&lt;-s\n    }  \n  }\n  return(data)\n}\n\n\n\n\n\nLoad the following packages:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(ggflags)\n\n\n\n\nback_to_back_barplot = function(data, col1, col2, couleur1, couleur2){\n  # libaries\n  #library(gridExtra)\n  if(missing(couleur1)){couleur1 =\"#009DE0\"}\n  if(missing(couleur2)){couleur2 =\"#009DE0\"}\n  # Make sure sata is numeric\n  data$col1 = as.numeric(data$col1)\n  data$col2 = as.numeric(data$col2)\n  \n  # Organiser les données en fonction de la valeur \n  data = data %&gt;% arrange(-col1)\n  # Put left column as negative value\n  data$col1 = -data$col1\n  data = data[c(1:20),]\n  # Create frame for the plot\n  par(mfrow=c(1,2), mai = c(0, 0, 0, 0))\n  \n  # Create first plot\n  p1 = data  %&gt;%\n    ggplot(aes(x = reorder(Id, -col1) , y = col2)) +\n    geom_bar(stat = \"identity\", fill = couleur1) +\n    coord_flip() +\n    ylab(\"\") +\n    xlab(\"\") + theme(panel.background = element_blank(), \n                     plot.title = element_text(hjust = 0.5),\n                     axis.title.x=element_blank(),\n                     axis.ticks.x=element_blank(),\n                     axis.text.x=element_blank(),\n                     axis.text.y = element_text(hjust=0, size = 15),\n                     axis.ticks.y=element_blank(),\n                     \n    ) +\n    geom_text(aes(label=col2), position=position_dodge(width=0), vjust=0, hjust = -0.25)\n    \n  # Create second plot\n  p2 = data %&gt;%\n    ggplot(aes(x = reorder(Id, -col1) , y = col1)) +\n    geom_bar(stat = \"identity\", fill = couleur2) +\n    #scale_fill_discrete(name = \"\", labels = c(\"Collaborateurs Francais\", \"Collaborateurs total\"), type = c(\"#443A31\",\"#009DE0\")) + \n    #scale_fill_manual(values = c(\"#443A31\",\"#009DE0\")) +\n    coord_flip() +\n    ylab(\"\") +\n    xlab(\"\") + theme(panel.background = element_blank(),\n                     axis.title.x=element_blank(),\n                     axis.text.x=element_blank(),\n                     axis.ticks.x=element_blank(),\n                     axis.text.y=element_blank(),\n                     axis.ticks.y=element_blank()\n    ) +\n    geom_text(aes(label=-col1), position=position_dodge(width=0), vjust=0, hjust = -0.25)\n  \n    # put back to back\n    p = grid.arrange(p2, p1, ncol =2)\n    \n    # Export\n    ggsave(p, file  = \"back_to_back_jp.pdf\", height = 10, width = 25)\n}\n\n\n\n\n\nsimple_barplot_flags = function(data, pays, chiffres, Id, couleur){\n  library(ggflags)\n  library(ggplot2)\n  #colnames(JP2) = c(\"Id\", \"chiffres\", \"pays\")\n  if(missing(couleur)){couleur =\"#009DE0\"}\n  \n  data$chiffres = as.numeric(data$chiffres)\n  data = data %&gt;% arrange(-chiffres)\n  \n  p1 = data %&gt;% mutate(code = tolower(pays)) %&gt;%\n    ggplot(aes(x = reorder(Id, chiffres), y =chiffres)) +\n    geom_bar(stat = \"identity\", fill = couleur) +\n    geom_flag(y = 0, aes(country = code), size = 10) +\n    coord_flip() +\n    ylab(\"\") +\n    xlab(\"\") + theme(panel.background = element_blank(), \n                     plot.title = element_text(hjust = 0.5),\n                     axis.title.x=element_blank(),\n                     axis.ticks.x=element_blank(),\n                     axis.text.x=element_blank(),\n                     axis.text.y = element_text(hjust=0, size = 15),\n                     axis.ticks.y=element_blank(),\n    ) +\n    geom_text(aes(label=chiffres), position=position_dodge(width=0), vjust=0, hjust = -0.25)\n    # Export\n    ggsave(p1, file  = \"Barplot_avec_drapeaux.pdf\", height = 10, width = 10)\n}\n\n\n\n\n\ndynamique_tempo = function(data, Id, Annee, couleur){\n  # libaries\n  #library(gridExtra)\n  if(missing(couleur)){couleur =\"#009DE0\"}\n  # Make sure data is numeric\n  data = as.data.frame(data)\n  data$Annee = as.numeric(data$Annee)\n  # Organiser les données en fonction de l'annee -&gt; compter les observations par an \n  data = data %&gt;% group_by(Annee) %&gt;% summarise(freq = n())\n\n  # Create first plot\n  par(bg = \"transparent\")\n  ggplot(data, aes(x = Annee, y = freq)) + geom_bar(stat = \"identity\", fill = couleur) +\n    xlab(\"\") + ylab(\"\") + theme( \n      text = element_text(size=10),\n      plot.title = element_text(hjust = 0.5),\n      panel.background = element_rect(fill = \"transparent\"), # bg of the panel\n      plot.background = element_rect(fill = \"transparent\", color = NA), # bg of the plot\n      legend.background = element_rect(fill = \"transparent\"), # get rid of legend bg\n      legend.box.background = element_rect(fill = \"transparent\")\n      ) + scale_y_continuous(breaks = seq(0, max(data$freq), by = 1))\n  \n  # get rid of legend panel bg)\n  ggsave(file= \"Dynamique_tempo.pdf\", bg = \"transparent\", width = 6, height = 4)\n}\n\n\n\n\n\nsimple_barplot = function(data, Id, Chiffres, couleur){\n\n  if(missing(couleur)){couleur =\"#009DE0\"}\n  # Make sure sata is numeric\n  data$Chiffres = as.numeric(data$Chiffres)\n  # Organiser les données en fonction de la valeur \n  data = data %&gt;% arrange(-Chiffres)\n\n  # Create first plot\n  data  %&gt;%\n    ggplot(aes(x = reorder(Id, Chiffres) , y = Chiffres)) +\n    geom_bar(stat = \"identity\", fill = couleur1) +\n    coord_flip() +\n    ylab(\"\") +\n    xlab(\"\") + theme(panel.background = element_blank(), \n                     plot.title = element_text(hjust = 0.5),\n                     axis.title.x=element_blank(),\n                     axis.ticks.x=element_blank(),\n                     axis.text.x=element_blank(),\n                     axis.text.y = element_text(hjust=0, size = 15),\n                     axis.ticks.y=element_blank(),\n                     \n    ) +\n    geom_text(aes(label=Chiffres), position=position_dodge(width=0), vjust=0, hjust = -0.25)\n  \n}\n\n\n\n\n\n\n## Set up the selenium server\nrD &lt;- RSelenium::rsDriver(browser = \"firefox\")\n# Assign the client to an object\nremDr &lt;- rD[[\"client\"]]\n\n\nfor(i in 10:dim(numbers_for_google)[1]){\n  if(i%%100==0){print(i)}\n  if(is.na(numbers_for_google[i,2])==TRUE){\n    remDr$navigate(paste0(\"https://patents.google.com/patent/\",numbers_for_google[i,4],\"/en?oq=\",numbers_for_google[i,4]))\n    WebsiteHTML = remDr$getPageSource()[[1]]\n    page &lt;- read_html(WebsiteHTML)\n    \n    UGV_abst[i,2] &lt;- page %&gt;% \n      html_node('div.abstract') %&gt;% \n      html_text()\n    \n    UGV_descr[i,2] &lt;- page %&gt;% \n      html_node('div.description') %&gt;% \n      html_text()\n    \n    UGV_claims[i,2] &lt;- page %&gt;%\n      html_node(\"div.claims\") %&gt;%\n      html_text\n  }\n  secs = runif(1, min = 1, max = 3)\n  Sys.sleep(secs)\n  if(i%%100==0){\n    # we save the data here in case of catastophic problems\n    save(UGV_abst, file = \"UGV_abst_backup.rdata\")\n    save(UGV_descr, file = \"UGV_descr_backup.rdata\")\n    save(UGV_claims, file = \"UGV_claims_backup.rdata\")\n  }\n}\nrD[[\"server\"]]$stop()",
    "crumbs": [
      "Home",
      "Useful scripts",
      "Useful scripts and where to find them"
    ]
  },
  {
    "objectID": "useful_scripts.html#geolocalisation",
    "href": "useful_scripts.html#geolocalisation",
    "title": "Useful scripts and where to find them",
    "section": "",
    "text": "Suppose I have a DB with players located by lon and lat and i want to find the NUTS2/3 code associated with this:\n\nlibrary(sf)\n# first loas a shapefile of the region in which we search\nnuts2 &lt;- st_read(\"somepatch/NUTS_RG_20M_2021_4326.shp\")\nnuts2 &lt;- st_transform(nuts2, crs = 4326)\n# Example data frame with longitude and latitude\ndf &lt;- trd102_app_detail_loc_nuts[,c(\"GEO_LONG\", \"GEO_LAT\")]\ndf = na.omit(df)\n# Convert the data frame to an sf object\ndf_sf &lt;- st_as_sf(df, coords = c(\"GEO_LONG\", \"GEO_LAT\"), crs = 4326, agr = \"constant\")\n# Perform spatial join to match points to NUTS2 polygons\npoints_with_nuts2 &lt;- st_join(df_sf, nuts2)\n\nFind the localisation and eventually KVK number of players identified in data:\n\nNDS_pats_clean_companies_geoloc$searchname &lt;- gsub(\" \", \"\\\\+\", NDS_pats_clean_companies_geoloc$applicant_cleaned)\nfor(i in 1:dim(NDS_pats_clean_companies_geoloc)[1]){\n  if(i %% 100 == 0){print(i)}\n  if(NDS_pats_clean_companies_geoloc[i,3] == TRUE & is.na(NDS_pats_clean_companies_geoloc[i, 4]) == TRUE){ # we only search if it's a company and we haven't searched before\n  url = paste0(\"https://www.bedrijvenregister.nl/zoekresultaten?q_source=cta&q=\", NDS_pats_clean_companies_geoloc[i,5])\n  #url = paste0(\"https://www.telefoonboek.nl/zoeken/\",fabia_not_found[i,1],\"/\")\n  response &lt;- GET(as.character(url)) # get the information\n  NDS_pats_clean_companies_geoloc[i,4] &lt;- content(response, as = \"text\") # extract the content\n  #Sys.sleep(3)\n  #if(i %% 1000 == 0){Sys.sleep(300)}\n  }\n}\n\nDifferent approach:\n\npostcodes$long &lt;-NA\npostcodes$lan &lt;- NA\nfor(i in 3193:dim(postcodes)[1]){\n  if(i %% 100 == 0){print(i)}\n  pc = gsub(\" \",\"\",postcodes[i,1])\n  url = paste0(\"https://postcodebijadres.nl/\",pc)\n  test = GET(url)\n  content &lt;- content(test, as = \"text\")\n  postcodes[i,2] &lt;- str_extract(content, \"(?&lt;=Breedtegraad \\\\(N\\\\):&lt;\\\\/td&gt;&lt;td&gt;)(.*)(?=&lt;\\\\/td&gt;)\")\n  postcodes[i,3] &lt;- str_extract(content, \"(?&lt;=Lengtegraad \\\\(E\\\\):&lt;\\\\/td&gt;&lt;td&gt;)(.*)(?=&lt;\\\\/td&gt;)\")\n}",
    "crumbs": [
      "Home",
      "Useful scripts",
      "Useful scripts and where to find them"
    ]
  },
  {
    "objectID": "useful_scripts.html#companies",
    "href": "useful_scripts.html#companies",
    "title": "Useful scripts and where to find them",
    "section": "",
    "text": "# create a df with the names of the assignees\ndf &lt;- as.data.frame(unique(player_geoloc$final_player))\ncolnames(df) = \"applicant_name\"\ndf = na.omit(df)\n\n# Create a list of common company keywords\ncompany_keywords &lt;- c(\"CERAMICS\",\"HOSP\",\"MEDIC\",\"FONDATION\",\"FUNDACION\",\"PRODUCT\",\" KK\",\" S A\",\"DU PONT\",\"VISION\",\"CHEMI\",\"DEUTSCHES\",\" INT\",\" CO\",\"ELEKT\",\" ＳＥ\",\"BUND\",\"INSTR\",\"LICENS\",\"B V\",\"ELEC\",\"MICRO\",\"&\",\"SOLAR\",\"OPER\",\" AB\",\" SRL\",\" SE\",\"GERMANY\",\"NETHERLANDS\",\"AIR\",\"FRANCE\",\"FRAUNHOFER\",\"IMEC\",\"ASML\",\"PHILIPS\",\"SIEMENS\",\"KONINK\",\"ROYAL\",\" NV\",\" BV\",\"SCHOOL\",\"INSTIT\",\"CENT\",\"UNIV\",\"CONDU\",\"NANO\",\"PLASTIC\",\" AS\",\"PLC\",\"ＡＧ\",\"SYSTEM\",\"WERK\",\"INC\", \"LTD\", \"CORP\", \"LLC\", \"CO.\", \"GROUP\", \"COMPANY\", \"AG\", \"GMBH\", \" S.A.\", \" S.P.A.\", \" OY\", \" SPA\", \" SA\", \"TECH\", \"IND\", \"SOLU\")\n\n# Function to determine if a name is likely a person or a company\nis_person_or_company &lt;- function(name) {\n  # Check if the name contains any company-related keywords\n  if (any(str_detect(name, company_keywords))) {\n    return(\"Company\")\n  }\n  # Check if the name contains common patterns for person names\n  # Example: Names with 2 or 3 words and lack company-like keywords\n  words &lt;- str_split(name, \" \")[[1]]\n  \n  if (length(words) &gt;= 2 && length(words) &lt;= 3 && !any(str_detect(name, \"[0-9]\"))) {\n    return(\"Person\")\n  }\n  # Fallback to \"Company\" if none of the person rules matched\n  return(\"Company\")\n}\n\n# Apply the function to the applicant_name column\ndf &lt;- df %&gt;%\n  mutate(type = sapply(applicant_name, is_person_or_company))\n\n# Print the resulting data frame\nprint(df)",
    "crumbs": [
      "Home",
      "Useful scripts",
      "Useful scripts and where to find them"
    ]
  },
  {
    "objectID": "useful_scripts.html#networks",
    "href": "useful_scripts.html#networks",
    "title": "Useful scripts and where to find them",
    "section": "",
    "text": "Start with the dataframe in which each row has the elements that need to be linked in seperated by something: player_1;player_2;player_3 etc. Use splitstackshape::cSplit() to split the dataframe. Then use the following script on this df. Replace the start of the loop according to how many columns are in front of the player info: ID|year|players -&gt; start at 3 and adjust if you want to include this info into the final dataframe.\n\nplayer_geoloc_collab = as.matrix(player_geoloc_collab)\nfor(i in 2:(dim(player_geoloc_collab)[2]-1)){\n  for(j in (i+1):dim(player_geoloc_collab)[2]){\n    tmp = player_geoloc_collab[,c(1, i, j)]\n    tmp = na.omit(tmp)\n    if(i == 2 & j == 3){res = tmp}else{res = rbind(res, tmp)}\n  }\n}\n\n\n\n\n\nnetwork_creation = function(data, dynamic, sep){\n  library(tidyverse)\n  res = c(NA,NA,NA)\n  # we start by splitting the data\n  nw = splitstackshape::cSplit(data, 1, sep = sep)\n  nw = as.matrix(nw)\n  # garde-fou : if the dimension of the dataframe after splitting is the same, then there is no network\n  if(dim(nw)[2] == dim(data)[2]){\n    print(\"You idiot, there is no network to be made here!\")\n  }else{\n    # there is data, we need to make the list of links\n    # we do this by looping over the columns and combining them\n    for(i in 2:(dim(nw)[2]-1)){\n      for(j in (i+1):dim(nw)[2]){\n        tmp = cbind(nw[,1], nw[,i], nw[,j])\n        tmp = na.omit(tmp)\n        res = rbind(res, tmp)\n      }\n    }\n    res = as.data.frame(na.omit(res))\n    colnames(res) = c(\"Year\", \"Source\", \"Target\")\n    if(missing(dynamic) == FALSE){\n      res[,c(2,3)] = alpha.order(res[,c(2,3)])\n      res$linkid = paste(res[,2], res[,3], sep = \";\")\n      res = res %&gt;% group_by(linkid) %&gt;% summarize(\"weight\" = n(), \"first_year\" = min(Year), \"last_year\" = max(Year))\n      res = splitstackshape::cSplit(res, 1, sep = \";\")\n      colnames(res) = c(\"Weight\", \"First_year\", \"Last_year\", \"Source\", \"Target\")\n    }\n    return(res)\n  }\n}",
    "crumbs": [
      "Home",
      "Useful scripts",
      "Useful scripts and where to find them"
    ]
  },
  {
    "objectID": "useful_scripts.html#patents",
    "href": "useful_scripts.html#patents",
    "title": "Useful scripts and where to find them",
    "section": "",
    "text": "reduce.digits.fast = function (data, digit, sep){\n  # verifier le digit demandé\n  if (digit == 3){\n    y = function(x) {\n      substr(x, 1, 3)\n    }\n    z = function(x) {\n      tmp = x\n      x = na.omit(x)\n      x = unique(x)\n      x = paste(t(x), collapse = sep)\n    }\n    data &lt;- as.matrix(data)\n    result = matrix(NA, nrow = dim(data)[1])\n    data = splitstackshape::cSplit(data, 1,  sep = sep)\n    data = as.matrix(data)\n    data[] &lt;- vapply(data, y, character(1))\n    result[] = apply(data, 1, z)\n    #print(class(result))\n    return(result)\n  }\n  if (digit == 4){\n    # creer une fonction qui extrait le 4 premiers caracteres\n    y = function(x) {\n      substr(x, 1, 4)\n    }\n    # creer une fonction qui enlève les na, les doublons, recoller \n    z = function(x) {\n      tmp = x\n      x = na.omit(x)\n      x = unique(x)\n      x = paste(t(x), collapse = sep)\n    }\n    # ici traitement des donnees\n    data &lt;- as.matrix(data) # assurer que format soit matrice\n    result = matrix(NA, nrow = dim(data)[1]) # créer une matrice qui contiendra les resultats\n    data = splitstackshape::cSplit(data, 1, sep =sep) # couper les données\n    data = as.matrix(data) # remettre en matrice\n    data[] &lt;- vapply(data, y, character(1)) # appliquer la fonction y (soustraire) à toutes les lignes (vapply)\n    result[] &lt;- apply(data, 1, z) # Appliquer la fonction y sur toutes les\n    return(result)\n  }\n  if (digit == 7){\n    y = function(x) {\n      sub(\"/.*\", \"\", x)\n    }\n    z = function(x) {\n      tmp = x\n      x = na.omit(x)\n      x = unique(x)\n      x = paste(t(x), collapse = sep)\n    }\n    data &lt;- as.matrix(data)\n    result = matrix(NA, nrow = dim(data)[1])\n    data = splitstackshape::cSplit(data, 1, sep = sep)\n    data = as.matrix(data)\n    data[] &lt;- vapply(data, y, character(1))\n    result[] = apply(data, 1, z)\n    return(result)\n  }\n  if (digit == 9){return(data)}\n}\n\n\n\n\n\nalpha.order&lt;-function(data){\n  data&lt;-as.matrix(data)\n  for (i in 1:dim(data)[1]){\n    data[i,1] = textclean::replace_non_ascii(data[i,1], remove.nonconverted = TRUE)\n    data[i,2] = textclean::replace_non_ascii(data[i,2], remove.nonconverted = TRUE)\n    if (data[i,1] &gt; data[i,2]){\n      s&lt;-data[i,1]\n      data[i,1]&lt;-data[i,2]\n      data[i,2]&lt;-s\n    }  \n  }\n  return(data)\n}",
    "crumbs": [
      "Home",
      "Useful scripts",
      "Useful scripts and where to find them"
    ]
  },
  {
    "objectID": "useful_scripts.html#visualisations",
    "href": "useful_scripts.html#visualisations",
    "title": "Useful scripts and where to find them",
    "section": "",
    "text": "Load the following packages:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(ggflags)\n\n\n\n\nback_to_back_barplot = function(data, col1, col2, couleur1, couleur2){\n  # libaries\n  #library(gridExtra)\n  if(missing(couleur1)){couleur1 =\"#009DE0\"}\n  if(missing(couleur2)){couleur2 =\"#009DE0\"}\n  # Make sure sata is numeric\n  data$col1 = as.numeric(data$col1)\n  data$col2 = as.numeric(data$col2)\n  \n  # Organiser les données en fonction de la valeur \n  data = data %&gt;% arrange(-col1)\n  # Put left column as negative value\n  data$col1 = -data$col1\n  data = data[c(1:20),]\n  # Create frame for the plot\n  par(mfrow=c(1,2), mai = c(0, 0, 0, 0))\n  \n  # Create first plot\n  p1 = data  %&gt;%\n    ggplot(aes(x = reorder(Id, -col1) , y = col2)) +\n    geom_bar(stat = \"identity\", fill = couleur1) +\n    coord_flip() +\n    ylab(\"\") +\n    xlab(\"\") + theme(panel.background = element_blank(), \n                     plot.title = element_text(hjust = 0.5),\n                     axis.title.x=element_blank(),\n                     axis.ticks.x=element_blank(),\n                     axis.text.x=element_blank(),\n                     axis.text.y = element_text(hjust=0, size = 15),\n                     axis.ticks.y=element_blank(),\n                     \n    ) +\n    geom_text(aes(label=col2), position=position_dodge(width=0), vjust=0, hjust = -0.25)\n    \n  # Create second plot\n  p2 = data %&gt;%\n    ggplot(aes(x = reorder(Id, -col1) , y = col1)) +\n    geom_bar(stat = \"identity\", fill = couleur2) +\n    #scale_fill_discrete(name = \"\", labels = c(\"Collaborateurs Francais\", \"Collaborateurs total\"), type = c(\"#443A31\",\"#009DE0\")) + \n    #scale_fill_manual(values = c(\"#443A31\",\"#009DE0\")) +\n    coord_flip() +\n    ylab(\"\") +\n    xlab(\"\") + theme(panel.background = element_blank(),\n                     axis.title.x=element_blank(),\n                     axis.text.x=element_blank(),\n                     axis.ticks.x=element_blank(),\n                     axis.text.y=element_blank(),\n                     axis.ticks.y=element_blank()\n    ) +\n    geom_text(aes(label=-col1), position=position_dodge(width=0), vjust=0, hjust = -0.25)\n  \n    # put back to back\n    p = grid.arrange(p2, p1, ncol =2)\n    \n    # Export\n    ggsave(p, file  = \"back_to_back_jp.pdf\", height = 10, width = 25)\n}\n\n\n\n\n\nsimple_barplot_flags = function(data, pays, chiffres, Id, couleur){\n  library(ggflags)\n  library(ggplot2)\n  #colnames(JP2) = c(\"Id\", \"chiffres\", \"pays\")\n  if(missing(couleur)){couleur =\"#009DE0\"}\n  \n  data$chiffres = as.numeric(data$chiffres)\n  data = data %&gt;% arrange(-chiffres)\n  \n  p1 = data %&gt;% mutate(code = tolower(pays)) %&gt;%\n    ggplot(aes(x = reorder(Id, chiffres), y =chiffres)) +\n    geom_bar(stat = \"identity\", fill = couleur) +\n    geom_flag(y = 0, aes(country = code), size = 10) +\n    coord_flip() +\n    ylab(\"\") +\n    xlab(\"\") + theme(panel.background = element_blank(), \n                     plot.title = element_text(hjust = 0.5),\n                     axis.title.x=element_blank(),\n                     axis.ticks.x=element_blank(),\n                     axis.text.x=element_blank(),\n                     axis.text.y = element_text(hjust=0, size = 15),\n                     axis.ticks.y=element_blank(),\n    ) +\n    geom_text(aes(label=chiffres), position=position_dodge(width=0), vjust=0, hjust = -0.25)\n    # Export\n    ggsave(p1, file  = \"Barplot_avec_drapeaux.pdf\", height = 10, width = 10)\n}\n\n\n\n\n\ndynamique_tempo = function(data, Id, Annee, couleur){\n  # libaries\n  #library(gridExtra)\n  if(missing(couleur)){couleur =\"#009DE0\"}\n  # Make sure data is numeric\n  data = as.data.frame(data)\n  data$Annee = as.numeric(data$Annee)\n  # Organiser les données en fonction de l'annee -&gt; compter les observations par an \n  data = data %&gt;% group_by(Annee) %&gt;% summarise(freq = n())\n\n  # Create first plot\n  par(bg = \"transparent\")\n  ggplot(data, aes(x = Annee, y = freq)) + geom_bar(stat = \"identity\", fill = couleur) +\n    xlab(\"\") + ylab(\"\") + theme( \n      text = element_text(size=10),\n      plot.title = element_text(hjust = 0.5),\n      panel.background = element_rect(fill = \"transparent\"), # bg of the panel\n      plot.background = element_rect(fill = \"transparent\", color = NA), # bg of the plot\n      legend.background = element_rect(fill = \"transparent\"), # get rid of legend bg\n      legend.box.background = element_rect(fill = \"transparent\")\n      ) + scale_y_continuous(breaks = seq(0, max(data$freq), by = 1))\n  \n  # get rid of legend panel bg)\n  ggsave(file= \"Dynamique_tempo.pdf\", bg = \"transparent\", width = 6, height = 4)\n}\n\n\n\n\n\nsimple_barplot = function(data, Id, Chiffres, couleur){\n\n  if(missing(couleur)){couleur =\"#009DE0\"}\n  # Make sure sata is numeric\n  data$Chiffres = as.numeric(data$Chiffres)\n  # Organiser les données en fonction de la valeur \n  data = data %&gt;% arrange(-Chiffres)\n\n  # Create first plot\n  data  %&gt;%\n    ggplot(aes(x = reorder(Id, Chiffres) , y = Chiffres)) +\n    geom_bar(stat = \"identity\", fill = couleur1) +\n    coord_flip() +\n    ylab(\"\") +\n    xlab(\"\") + theme(panel.background = element_blank(), \n                     plot.title = element_text(hjust = 0.5),\n                     axis.title.x=element_blank(),\n                     axis.ticks.x=element_blank(),\n                     axis.text.x=element_blank(),\n                     axis.text.y = element_text(hjust=0, size = 15),\n                     axis.ticks.y=element_blank(),\n                     \n    ) +\n    geom_text(aes(label=Chiffres), position=position_dodge(width=0), vjust=0, hjust = -0.25)\n  \n}",
    "crumbs": [
      "Home",
      "Useful scripts",
      "Useful scripts and where to find them"
    ]
  },
  {
    "objectID": "useful_scripts.html#get-patent-information",
    "href": "useful_scripts.html#get-patent-information",
    "title": "Useful scripts and where to find them",
    "section": "",
    "text": "## Set up the selenium server\nrD &lt;- RSelenium::rsDriver(browser = \"firefox\")\n# Assign the client to an object\nremDr &lt;- rD[[\"client\"]]\n\n\nfor(i in 10:dim(numbers_for_google)[1]){\n  if(i%%100==0){print(i)}\n  if(is.na(numbers_for_google[i,2])==TRUE){\n    remDr$navigate(paste0(\"https://patents.google.com/patent/\",numbers_for_google[i,4],\"/en?oq=\",numbers_for_google[i,4]))\n    WebsiteHTML = remDr$getPageSource()[[1]]\n    page &lt;- read_html(WebsiteHTML)\n    \n    UGV_abst[i,2] &lt;- page %&gt;% \n      html_node('div.abstract') %&gt;% \n      html_text()\n    \n    UGV_descr[i,2] &lt;- page %&gt;% \n      html_node('div.description') %&gt;% \n      html_text()\n    \n    UGV_claims[i,2] &lt;- page %&gt;%\n      html_node(\"div.claims\") %&gt;%\n      html_text\n  }\n  secs = runif(1, min = 1, max = 3)\n  Sys.sleep(secs)\n  if(i%%100==0){\n    # we save the data here in case of catastophic problems\n    save(UGV_abst, file = \"UGV_abst_backup.rdata\")\n    save(UGV_descr, file = \"UGV_descr_backup.rdata\")\n    save(UGV_claims, file = \"UGV_claims_backup.rdata\")\n  }\n}\nrD[[\"server\"]]$stop()",
    "crumbs": [
      "Home",
      "Useful scripts",
      "Useful scripts and where to find them"
    ]
  },
  {
    "objectID": "NER.html",
    "href": "NER.html",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "We will used pre-trained models to perform named entity recognition. These models are quite heavy and require python/spark/torch/hadoop infrastructures. In case these do not work we will have a basic version (tagged but not trained) that will always work.\n\n\nTo access and run the different models we want to use, we need to install a couple of things.\n\n\nWe are going to install a light version of python to run from R. We will not use the python language but an R interface. It will however be possible to program in Python from Rstudio if you want to do this for another reason. The reticulate package allows this R-Python interface.\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n# for windows users, install GIT\n\nWe now install python from R in addition to a light version of anaconda. The combination of these installs allow you to create an environment from which to work. R will be able to find all that is required since it standardises all the information it needs in this environment. You only have to run the virtualenv_create(“DAFS”) once to create the environment. Later on you only have to use the use_virtualenv(“DAFS”) to load the environment when you want to use it. Note that DAFS is the name of the environment, you can change this to whatever you want.\n\ninstall_python()\n# Restart R manually\ninstall_miniconda()\n# Restart R manually\n\n# We now create a virtual environment for the course\nvirtualenv_create(\"DAFS\")\nuse_virtualenv(\"DAFS\")\n\n# note that when you start this script the virtualenv_create function is no longer required. You only run it once to create the environment.\n\n\n\n\nWe now install the interface to the huggingface website. We then need to install some python dependencies. It’s best to restart R when this is done.\n\ninstall.packages(\"devtools\")\ninstall.packages(\"usethis\")\ninstall.packages(\"cli\")\nlibrary(devtools)\ndevtools::install_github(\"farach/huggingfaceR\")\ninstall.packages(\"hfhub\")\nhuggingfaceR::hf_python_depends()\n# Restart R\n\nWe now install a couple of important Python packages that will allow us to load and use the models we want to use from huggingface:\n\npy_install(\"transformers\") # so we can use the models that are trained on tensorflow\npy_install(\"torch\") # so we can use the models that are trained on pytorch\n# Restart R\n\n\n\n\n\nNow we can start downloading the models. Let’s start by loading a bert model trained for NER. We add the argument aggregation_strategy= “simple to get an output that inclused readable tokens and not a list of syllable level tokens (cf. slides from the lecture). For NER, we will use the following model: bert-base-NER. If you have more patience and computation power bert-large-NER is also an option. The hf_load_pipeline will download the models directly and put them to use.\n\nNER_extract &lt;- huggingfaceR::hf_load_pipeline(model = \"dslim/bert-large-NER\", task = \"ner\", aggregation_strategy = \"simple\")\n\nLet’s get a sample text and try it out.\n\ntext &lt;- c(\"The 2024 edition of The European 5G Conference will take place on 30-31 January at the Hotel nhow Brussels Bloom. Now, in its 8th year, the conference has an established reputation as Brussels’ leading meeting place for discussion on 5G policy. Registration is now available – secure your place today. The event will, once again, provide the opportunity to hear from high-level policymakers and industry stakeholders on key themes such as investment, security, sustainability, emerging business models, and connectivity. It will provide an update on progress that has been made towards the 2030 ‘Path to the Digital Decade’ targets, as well as offering a first opportunity to examine the outcomes from WRC-23 and at what this may mean for the future connectivity environment around 5G and future technologies. By looking back at the lessons learnt to date and forward to the path towards 5G Advanced and 6G, the event will provide a comprehensive insight into all the key policy aspects that are shaping the 5G ecosystem in Europe.\")\nextracted_NE &lt;- NER_extract(text)\n#transform output into something readable:\nextracted_NE &lt;- plyr::ldply(extracted_NE, data.frame)\nextracted_NE\n\nWe can do the same with a different model that is capable of treating multiple languages. The basic structure is the same. We change the links to the models, and adjust what we download (tensor or pytorch):\n\nmultilanguage_NER = huggingfaceR::hf_load_pipeline(model = \"Babelscape/wikineural-multilingual-ner\", tokenizer = \"Babelscape/wikineural-multilingual-ner\", task = \"ner\", aggregation_strategy=\"simple\")\ntest_multi &lt;- multilanguage_NER(text)\ntest_multi &lt;- plyr::ldply(test_multi, data.frame)\ntest_multi\n\n\n\n\nThe workflow is the same, we just need to find the correct models:\n\nsentiment_classifier &lt;- huggingfaceR::hf_load_pipeline(model = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", tokenizer = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", top_k = 4)\nsent  &lt;- sentiment_classifier(text)\nsent &lt;- plyr::ldply(sent, data.frame)\nsent\n\nother type of model:\n\n# other type\nsubj_classifier = huggingfaceR::hf_load_pipeline(model = \"cffl/bert-base-styleclassification-subjective-neutral\", tokenizer = \"cffl/bert-base-styleclassification-subjective-neutral\", task=\"text-classification\")\nsubj &lt;- subj_classifier(text)\nsubj &lt;- plyr::ldply(subj, data.frame)\nsubj\n\n\n\n\nThe previous sections have shown how to use NER and sentiment analysis on a text. We now need to run this over a corpus. This means building a function that will allow us to automate the execution of the tasks.\nStart by importing the lexis uni data used in the previous tutorial. For this tutorial, this data is stored in the “LN_dataframe” object. If this takes too long to run, you can always change the NER system. For a faster system use the “dslim/bert-base-NER” model at the beginning.\n\nload(\"LN_dataframe.rdata\")\n# the text we want to analyse is in the \"Article\" column\nNER_function = function(data, score_thresh){\n  data = as.data.frame(data)\n  # perform NER on the chosen column\n  output &lt;- multilanguage_NER(as.character(data))\n  # organise the result in a df\n  output &lt;- plyr::ldply(output, data.frame)\n  # subset according to threshold\n  output &lt;- subset(output, output$score &gt;= score_thresh)\n  # this gives us a dataframe will all identified objects\n  # we need to regroup them by type (ORG, PER, MISC, LOC)\n  output_df &lt;- data.frame(\"LOC\" = paste(subset(output, output$entity_group == \"LOC\")$word, collapse = \";\"),\n                          \"ORG\" = paste(subset(output, output$entity_group == \"ORG\")$word, collapse = \";\"),\n                          \"PER\" = paste(subset(output, output$entity_group == \"PER\")$word, collapse = \";\"),\n                          \"MISC\" = paste(subset(output, output$entity_group == \"MISC\")$word, collapse = \";\")\n                        )\n  # return the output dataframe (technically not needed here)\n  return(output_df)\n} # closes function\n\nNER_results = apply(LN_dataframe, 1, NER_function, score_thresh = 0)\nNER_results &lt;- plyr::ldply(NER_results, data.frame)\n# this can take a while so we save the results\nsave(NER_results, file = \"NER_results.rdata\")\n\nIf your computer is a bit slower (or you want to run this a little bit at a time), you can also use a loop:\n\n# we start by adding the columns we want to add\nLN_dataframe_NER &lt;- LN_dataframe %&gt;% mutate(\"LOC\" = NA, \"ORG\" = NA, \"PER\" = NA, \"MISC\"= NA)\nfor(i in 1:dim(LN_dataframe)[1]){\n  if(i%%50 == 0){print(i)}\n  output &lt;- multilanguage_NER(as.character(LN_dataframe_NER[i,11]))\n  # organise the result in a df\n  output &lt;- plyr::ldply(output, data.frame)\n  # subset according to threshold\n  output &lt;- subset(output, output$score &gt;= 0)\n  # this gives us a dataframe will all identified objects\n  # we need to regroup them by type (ORG, PER, MISC, LOC)\n  output_df &lt;- data.frame(\"LOC\" = paste(subset(output, output$entity_group == \"LOC\")$word, collapse = \";\"),\n                          \"ORG\" = paste(subset(output, output$entity_group == \"ORG\")$word, collapse = \";\"),\n                          \"PER\" = paste(subset(output, output$entity_group == \"PER\")$word, collapse = \";\"),\n                          \"MISC\" = paste(subset(output, output$entity_group == \"MISC\")$word, collapse = \";\")\n                        )\n  LN_dataframe_NER[i,12:15] &lt;- output_df\n}\nsave(LN_dataframe_NER, file = \"LN_dataframe_NER.rdata\")\n\nOnce you have all the results in the dataframe, you can start the analysis with the tools you learned in previous tutorials.\n\n\n\nIn the previous tutorial we extracted topics, now let’s connect the topics to the dataframe so that we know which document connects to which topic.\n\nmost_likely_topics &lt;- topics(topics_model)# if you want THE most likely topic\nmost_likely_topics &lt;- topics(topics_model, k = 3) # if you want the 3 most likely\n\n# organise this data a bit\n# when requesting more than one topic, you need to transpose the df:\nmost_likely_topics &lt;- t(most_likely_topics)",
    "crumbs": [
      "Home",
      "Text Mining",
      "Named Entity Recognition"
    ]
  },
  {
    "objectID": "NER.html#system-related-installation",
    "href": "NER.html#system-related-installation",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "To access and run the different models we want to use, we need to install a couple of things.\n\n\nWe are going to install a light version of python to run from R. We will not use the python language but an R interface. It will however be possible to program in Python from Rstudio if you want to do this for another reason. The reticulate package allows this R-Python interface.\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n# for windows users, install GIT\n\nWe now install python from R in addition to a light version of anaconda. The combination of these installs allow you to create an environment from which to work. R will be able to find all that is required since it standardises all the information it needs in this environment. You only have to run the virtualenv_create(“DAFS”) once to create the environment. Later on you only have to use the use_virtualenv(“DAFS”) to load the environment when you want to use it. Note that DAFS is the name of the environment, you can change this to whatever you want.\n\ninstall_python()\n# Restart R manually\ninstall_miniconda()\n# Restart R manually\n\n# We now create a virtual environment for the course\nvirtualenv_create(\"DAFS\")\nuse_virtualenv(\"DAFS\")\n\n# note that when you start this script the virtualenv_create function is no longer required. You only run it once to create the environment.\n\n\n\n\nWe now install the interface to the huggingface website. We then need to install some python dependencies. It’s best to restart R when this is done.\n\ninstall.packages(\"devtools\")\ninstall.packages(\"usethis\")\ninstall.packages(\"cli\")\nlibrary(devtools)\ndevtools::install_github(\"farach/huggingfaceR\")\ninstall.packages(\"hfhub\")\nhuggingfaceR::hf_python_depends()\n# Restart R\n\nWe now install a couple of important Python packages that will allow us to load and use the models we want to use from huggingface:\n\npy_install(\"transformers\") # so we can use the models that are trained on tensorflow\npy_install(\"torch\") # so we can use the models that are trained on pytorch\n# Restart R",
    "crumbs": [
      "Home",
      "Text Mining",
      "Named Entity Recognition"
    ]
  },
  {
    "objectID": "NER.html#now-we-install-the-models-and-perform-ner",
    "href": "NER.html#now-we-install-the-models-and-perform-ner",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "Now we can start downloading the models. Let’s start by loading a bert model trained for NER. We add the argument aggregation_strategy= “simple to get an output that inclused readable tokens and not a list of syllable level tokens (cf. slides from the lecture). For NER, we will use the following model: bert-base-NER. If you have more patience and computation power bert-large-NER is also an option. The hf_load_pipeline will download the models directly and put them to use.\n\nNER_extract &lt;- huggingfaceR::hf_load_pipeline(model = \"dslim/bert-large-NER\", task = \"ner\", aggregation_strategy = \"simple\")\n\nLet’s get a sample text and try it out.\n\ntext &lt;- c(\"The 2024 edition of The European 5G Conference will take place on 30-31 January at the Hotel nhow Brussels Bloom. Now, in its 8th year, the conference has an established reputation as Brussels’ leading meeting place for discussion on 5G policy. Registration is now available – secure your place today. The event will, once again, provide the opportunity to hear from high-level policymakers and industry stakeholders on key themes such as investment, security, sustainability, emerging business models, and connectivity. It will provide an update on progress that has been made towards the 2030 ‘Path to the Digital Decade’ targets, as well as offering a first opportunity to examine the outcomes from WRC-23 and at what this may mean for the future connectivity environment around 5G and future technologies. By looking back at the lessons learnt to date and forward to the path towards 5G Advanced and 6G, the event will provide a comprehensive insight into all the key policy aspects that are shaping the 5G ecosystem in Europe.\")\nextracted_NE &lt;- NER_extract(text)\n#transform output into something readable:\nextracted_NE &lt;- plyr::ldply(extracted_NE, data.frame)\nextracted_NE\n\nWe can do the same with a different model that is capable of treating multiple languages. The basic structure is the same. We change the links to the models, and adjust what we download (tensor or pytorch):\n\nmultilanguage_NER = huggingfaceR::hf_load_pipeline(model = \"Babelscape/wikineural-multilingual-ner\", tokenizer = \"Babelscape/wikineural-multilingual-ner\", task = \"ner\", aggregation_strategy=\"simple\")\ntest_multi &lt;- multilanguage_NER(text)\ntest_multi &lt;- plyr::ldply(test_multi, data.frame)\ntest_multi",
    "crumbs": [
      "Home",
      "Text Mining",
      "Named Entity Recognition"
    ]
  },
  {
    "objectID": "NER.html#and-now-sentiment-analysis",
    "href": "NER.html#and-now-sentiment-analysis",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "The workflow is the same, we just need to find the correct models:\n\nsentiment_classifier &lt;- huggingfaceR::hf_load_pipeline(model = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", tokenizer = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", top_k = 4)\nsent  &lt;- sentiment_classifier(text)\nsent &lt;- plyr::ldply(sent, data.frame)\nsent\n\nother type of model:\n\n# other type\nsubj_classifier = huggingfaceR::hf_load_pipeline(model = \"cffl/bert-base-styleclassification-subjective-neutral\", tokenizer = \"cffl/bert-base-styleclassification-subjective-neutral\", task=\"text-classification\")\nsubj &lt;- subj_classifier(text)\nsubj &lt;- plyr::ldply(subj, data.frame)\nsubj",
    "crumbs": [
      "Home",
      "Text Mining",
      "Named Entity Recognition"
    ]
  },
  {
    "objectID": "NER.html#apply-this-to-our-data",
    "href": "NER.html#apply-this-to-our-data",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "The previous sections have shown how to use NER and sentiment analysis on a text. We now need to run this over a corpus. This means building a function that will allow us to automate the execution of the tasks.\nStart by importing the lexis uni data used in the previous tutorial. For this tutorial, this data is stored in the “LN_dataframe” object. If this takes too long to run, you can always change the NER system. For a faster system use the “dslim/bert-base-NER” model at the beginning.\n\nload(\"LN_dataframe.rdata\")\n# the text we want to analyse is in the \"Article\" column\nNER_function = function(data, score_thresh){\n  data = as.data.frame(data)\n  # perform NER on the chosen column\n  output &lt;- multilanguage_NER(as.character(data))\n  # organise the result in a df\n  output &lt;- plyr::ldply(output, data.frame)\n  # subset according to threshold\n  output &lt;- subset(output, output$score &gt;= score_thresh)\n  # this gives us a dataframe will all identified objects\n  # we need to regroup them by type (ORG, PER, MISC, LOC)\n  output_df &lt;- data.frame(\"LOC\" = paste(subset(output, output$entity_group == \"LOC\")$word, collapse = \";\"),\n                          \"ORG\" = paste(subset(output, output$entity_group == \"ORG\")$word, collapse = \";\"),\n                          \"PER\" = paste(subset(output, output$entity_group == \"PER\")$word, collapse = \";\"),\n                          \"MISC\" = paste(subset(output, output$entity_group == \"MISC\")$word, collapse = \";\")\n                        )\n  # return the output dataframe (technically not needed here)\n  return(output_df)\n} # closes function\n\nNER_results = apply(LN_dataframe, 1, NER_function, score_thresh = 0)\nNER_results &lt;- plyr::ldply(NER_results, data.frame)\n# this can take a while so we save the results\nsave(NER_results, file = \"NER_results.rdata\")\n\nIf your computer is a bit slower (or you want to run this a little bit at a time), you can also use a loop:\n\n# we start by adding the columns we want to add\nLN_dataframe_NER &lt;- LN_dataframe %&gt;% mutate(\"LOC\" = NA, \"ORG\" = NA, \"PER\" = NA, \"MISC\"= NA)\nfor(i in 1:dim(LN_dataframe)[1]){\n  if(i%%50 == 0){print(i)}\n  output &lt;- multilanguage_NER(as.character(LN_dataframe_NER[i,11]))\n  # organise the result in a df\n  output &lt;- plyr::ldply(output, data.frame)\n  # subset according to threshold\n  output &lt;- subset(output, output$score &gt;= 0)\n  # this gives us a dataframe will all identified objects\n  # we need to regroup them by type (ORG, PER, MISC, LOC)\n  output_df &lt;- data.frame(\"LOC\" = paste(subset(output, output$entity_group == \"LOC\")$word, collapse = \";\"),\n                          \"ORG\" = paste(subset(output, output$entity_group == \"ORG\")$word, collapse = \";\"),\n                          \"PER\" = paste(subset(output, output$entity_group == \"PER\")$word, collapse = \";\"),\n                          \"MISC\" = paste(subset(output, output$entity_group == \"MISC\")$word, collapse = \";\")\n                        )\n  LN_dataframe_NER[i,12:15] &lt;- output_df\n}\nsave(LN_dataframe_NER, file = \"LN_dataframe_NER.rdata\")\n\nOnce you have all the results in the dataframe, you can start the analysis with the tools you learned in previous tutorials.",
    "crumbs": [
      "Home",
      "Text Mining",
      "Named Entity Recognition"
    ]
  },
  {
    "objectID": "NER.html#connecting-topics-to-the-initial-dataframe",
    "href": "NER.html#connecting-topics-to-the-initial-dataframe",
    "title": "Named Entity Recognition",
    "section": "",
    "text": "In the previous tutorial we extracted topics, now let’s connect the topics to the dataframe so that we know which document connects to which topic.\n\nmost_likely_topics &lt;- topics(topics_model)# if you want THE most likely topic\nmost_likely_topics &lt;- topics(topics_model, k = 3) # if you want the 3 most likely\n\n# organise this data a bit\n# when requesting more than one topic, you need to transpose the df:\nmost_likely_topics &lt;- t(most_likely_topics)",
    "crumbs": [
      "Home",
      "Text Mining",
      "Named Entity Recognition"
    ]
  },
  {
    "objectID": "Topic_modelling_run_bertopic.html",
    "href": "Topic_modelling_run_bertopic.html",
    "title": "Run Bertopic",
    "section": "",
    "text": "topics, probs = topic_model.fit_transform(data)\n\n\n\nCreate a plot with the documents projected in a two dimensional space:\n\ntopic_model.visualize_documents(data)\n\n\nIn this space, each dot is a document. The color of the document represents its cluster/topic. The distance between the documents should be interpreted in terms of a semantic proximity between the documents.\nBeyond this visualisation we usually want to know which words constitute these topics. We can do this by creating a barplot of the words:\n\ntopic_model.visualize_barchart(top_n_topics = 10, n_words = 5)\n\n\nThe numbers under the terms, represent the contribution of the term to the topic (c-TF-IDF)\nWe can run the model again with bi-grams for example:\n\nWe can export the information per topic using the following function:\n\ntopic_info = topic_model.get_topic_info()\ntopic_info\n\n\nBertopic creates a -1 topic that will contain terms considered very generic. Using the probabilities option in the mode we can assign topic to the publications classified in topic -1.\nBased on the results of these topics we can go back to the parameters and adjust the model to improve the output. Once we are happy with the results, we can export the topics and connect them with other variables. Bertopic exports the results in the same order as the input data, topics can therefore be combined directly with the intial dataframe that we have been working with.\n\n\n\nWe can check the evolution of topics over time by adding information on the year of publication of the documents. For this we first need a dataframe with the years:\n\ntimestamps = data_full['Year']\ntimestamps\n\nWe then run a topic model, supplying this time information:\n\ntopics_dynamic = topic_model.topics_over_time(data, timestamps, nr_bins = 10)\n\n\n\n\n\n\nJust as for an LDA, we can get the probability for each to document to be linked to a topic.\n\nTopic_distribution = pd.DataFrame(probs, columns = [f\"Topic_{i}]\" for i in range(probs.shape[1])])\nTopic_distribution[\"Document\"] = data\nprint(Topic_distribution)\n\n\n\n\n\nBERTopic allows for a specific type of topic model which takes a hierachy of topics into account. This allows us to visually check what level of aggregation suits us best.\n\nmy_hier_topic_model = topic_model.hierarchical_topics(data)\nmy_hier_topic_model.visualize_hierarchical_documents(data, my_hier_topic_model)\n\n\n\n\n\ntopic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n\n\n\n\n\n\nWe need some additional help from other packages to use lemmatization:\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n \n# Download required resources\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')\n\nThen we need a function to assign the part-of-speech tags to the text\n\n# Function to map NLTK position tags to WordNet position tags\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\n        \"J\": wordnet.ADJ,\n        \"N\": wordnet.NOUN,\n        \"V\": wordnet.VERB,\n        \"R\": wordnet.ADV\n    }\n    return tag_dict.get(tag, wordnet.NOUN)\nlemmatizer = WordNetLemmatizer()\n\nAnd we need a function that goes over the words to lemmatize them.\n\ndef tokenize_text_bert(tmp):\n    # Tokenize the text into sentences\n    sentences = nltk.sent_tokenize(tmp)\n \n    # Lemmatize each word in each sentence with its correct pos tag\n    lemmatized_sentences = []\n    for sentence in sentences:\n        words = nltk.word_tokenize(sentence)\n        lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n        lemmatized_sentence = ' '.join(lemmatized_words)\n        lemmatized_sentences.append(lemmatized_sentence)\n    # Join the lemmatized sentences\n    reconstructed_text = ' '.join(lemmatized_sentences)\n    return reconstructed_text\n\nW now apply the function to the text, we can then us this data as an input for the embeddings.\n\ndata = list(map(tokenize_text_bert, data['Abstract']))",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Run Bertopic"
    ]
  },
  {
    "objectID": "Topic_modelling_run_bertopic.html#run-a-classic-bertopic-model",
    "href": "Topic_modelling_run_bertopic.html#run-a-classic-bertopic-model",
    "title": "Run Bertopic",
    "section": "",
    "text": "topics, probs = topic_model.fit_transform(data)\n\n\n\nCreate a plot with the documents projected in a two dimensional space:\n\ntopic_model.visualize_documents(data)\n\n\nIn this space, each dot is a document. The color of the document represents its cluster/topic. The distance between the documents should be interpreted in terms of a semantic proximity between the documents.\nBeyond this visualisation we usually want to know which words constitute these topics. We can do this by creating a barplot of the words:\n\ntopic_model.visualize_barchart(top_n_topics = 10, n_words = 5)\n\n\nThe numbers under the terms, represent the contribution of the term to the topic (c-TF-IDF)\nWe can run the model again with bi-grams for example:\n\nWe can export the information per topic using the following function:\n\ntopic_info = topic_model.get_topic_info()\ntopic_info\n\n\nBertopic creates a -1 topic that will contain terms considered very generic. Using the probabilities option in the mode we can assign topic to the publications classified in topic -1.\nBased on the results of these topics we can go back to the parameters and adjust the model to improve the output. Once we are happy with the results, we can export the topics and connect them with other variables. Bertopic exports the results in the same order as the input data, topics can therefore be combined directly with the intial dataframe that we have been working with.\n\n\n\nWe can check the evolution of topics over time by adding information on the year of publication of the documents. For this we first need a dataframe with the years:\n\ntimestamps = data_full['Year']\ntimestamps\n\nWe then run a topic model, supplying this time information:\n\ntopics_dynamic = topic_model.topics_over_time(data, timestamps, nr_bins = 10)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Run Bertopic"
    ]
  },
  {
    "objectID": "Topic_modelling_run_bertopic.html#proba-per-document",
    "href": "Topic_modelling_run_bertopic.html#proba-per-document",
    "title": "Run Bertopic",
    "section": "",
    "text": "Just as for an LDA, we can get the probability for each to document to be linked to a topic.\n\nTopic_distribution = pd.DataFrame(probs, columns = [f\"Topic_{i}]\" for i in range(probs.shape[1])])\nTopic_distribution[\"Document\"] = data\nprint(Topic_distribution)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Run Bertopic"
    ]
  },
  {
    "objectID": "Topic_modelling_run_bertopic.html#hierchical-topic-models",
    "href": "Topic_modelling_run_bertopic.html#hierchical-topic-models",
    "title": "Run Bertopic",
    "section": "",
    "text": "BERTopic allows for a specific type of topic model which takes a hierachy of topics into account. This allows us to visually check what level of aggregation suits us best.\n\nmy_hier_topic_model = topic_model.hierarchical_topics(data)\nmy_hier_topic_model.visualize_hierarchical_documents(data, my_hier_topic_model)\n\n\n\n\n\ntopic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Run Bertopic"
    ]
  },
  {
    "objectID": "Topic_modelling_run_bertopic.html#additional-data-prep-lemmatization",
    "href": "Topic_modelling_run_bertopic.html#additional-data-prep-lemmatization",
    "title": "Run Bertopic",
    "section": "",
    "text": "We need some additional help from other packages to use lemmatization:\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n \n# Download required resources\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')\n\nThen we need a function to assign the part-of-speech tags to the text\n\n# Function to map NLTK position tags to WordNet position tags\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\n        \"J\": wordnet.ADJ,\n        \"N\": wordnet.NOUN,\n        \"V\": wordnet.VERB,\n        \"R\": wordnet.ADV\n    }\n    return tag_dict.get(tag, wordnet.NOUN)\nlemmatizer = WordNetLemmatizer()\n\nAnd we need a function that goes over the words to lemmatize them.\n\ndef tokenize_text_bert(tmp):\n    # Tokenize the text into sentences\n    sentences = nltk.sent_tokenize(tmp)\n \n    # Lemmatize each word in each sentence with its correct pos tag\n    lemmatized_sentences = []\n    for sentence in sentences:\n        words = nltk.word_tokenize(sentence)\n        lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n        lemmatized_sentence = ' '.join(lemmatized_words)\n        lemmatized_sentences.append(lemmatized_sentence)\n    # Join the lemmatized sentences\n    reconstructed_text = ' '.join(lemmatized_sentences)\n    return reconstructed_text\n\nW now apply the function to the text, we can then us this data as an input for the embeddings.\n\ndata = list(map(tokenize_text_bert, data['Abstract']))",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Run Bertopic"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html",
    "href": "Topic_modelling_lda2.html",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "We start by loading the data from a csv file, we then remove the publications without an abstract\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(ldatuning)\nlibrary(topicmodels)\nlibrary(tidytext)\nlibrary(LDAvis)\nlibrary(ggplot2)\nentr_eco_publications &lt;- read_csv(\"~/Desktop/Teachings/Topic_mod_workshop/entr_eco_publications.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 1603 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (30): Authors, Author full names, Author(s) ID, Title, Source title, Iss...\ndbl  (4): Year, Volume, Page count, Cited by\nlgl (12): Molecular Sequence Numbers, Chemicals/CAS, Tradenames, Manufacture...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nentr_eco_publications = subset(entr_eco_publications, entr_eco_publications$Abstract != \"[No abstract available]\")\nentr_eco_publications = entr_eco_publications$Abstract\n\n\n\n\nBefore creating the model we need to prepare the text. This means removing punctuation, weird characters or even words that we don’t want to find in our topics simply because they don’t have any informational value.\n\nmy_dico = c(\"publication\", \"research\", \"focus\", \"results\", \"result\")\n\nWe build a function that prepares the text. It removed punctuation, removes stopwords and the words we don’t want, stems the text etc. This function can be adapted and extended to your need by adding lemmatization for example.\n\nlibrary(tm)\nclean_text &lt;- function(text){\n  text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with \"\".\n  text &lt;- gsub(\"\\\\.\", \"\", text)\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  text &lt;- stemDocument(text) # stem the terms\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n}\n\nWe apply the function to our text\n\nentr_eco_publications = apply(as.matrix(entr_eco_publications), 1, clean_text)\n\n\n\n\nNow that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the DocumentTermMatrix() function from the tm package. This function has one argument which is a dataframe with the text of the corpus.\n\n# create the document-Term-matrix\ndtm &lt;- DocumentTermMatrix(entr_eco_publications)\n\n\n\n\nBased on this matrix we will now try to define how many topics we need to extract. For this we use the FindTopicsNumber and FindTopicsNumber_plot from the ldatuning package.\nThe FindTopicsNumber functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The seq() function creates a vector which starts at the from arguments, stops at the to argument. The size of the steps is set by the by argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, …) we would write seq(from = 2, to = 60, by = 5).\nWe then specify the metrics we want to compute, we have discussed these in the lecture.\n\n\n\n\n\n\nWarning\n\n\n\nFor an unknown reason, the “Griffiths2004” function does not work on mac OSX. It should work for windows users.\n\n\nThe mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algorithm is stuck, still running or finished.\n\ntopic_num &lt;- FindTopicsNumber(\n  dtm,\n  topics = seq(from = 2, to = 60, by = 5),\n  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  mc.cores = 8L,\n  verbose = TRUE\n)\n\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Arun2010... done.\n  Deveaud2014... done.\n\nFindTopicsNumber_plot(topic_num)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the ldatuning package.\n  Please report the issue at &lt;https://github.com/nikita-moor/ldatuning/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we have an idea of how many topics we need, let’s extract the topics. We will use the LDA function from the topicmodels package, and specify K as the number of topics. Here we can pick 12~17 based on the previous plot.\n\n# perform topic modelling\nk = 17\ntopic_model_results  &lt;- LDA(dtm, k = k, method=\"Gibbs\", control = list(seed = 42, alpha=50/k, nstart = 1, keep = 1, burnin = 1000, iter = 1000, verbose = FALSE)) \n\n\n\n\ntopic_model_results contains the results of the topic modelling process, in other words it contains the probabilities that a word is part of a topic and a topic is part of a document. We can visualise this information by plotting the betas (probability that a word is part of a topic).\n\n# get the betas\nbetas &lt;- tidy(topic_model_results, matrix = \"beta\")\n# subset the betas for results\nap_top_terms &lt;- betas %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nlibrary(ggplot2)\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\nThis plot shows the top 10 words for each topic. This means that it shows the 10 words with the highest probability to be part of the topic. The values under each barplot represent the betas.\n\n\n\n\n# Extract the necessary components for LDAvis\nlda_topics &lt;- posterior(topic_model_results)$topics\nlda_terms &lt;- posterior(topic_model_results)$terms\nterm_frequency &lt;- colSums(as.matrix(dtm))\ndoc_length &lt;- rowSums(as.matrix(dtm))\n\n# Create the JSON object for visualization\njson_lda &lt;- createJSON(phi = lda_terms,\n                       theta = lda_topics,\n                       doc.length = doc_length,\n                       vocab = colnames(dtm),\n                       term.frequency = term_frequency)\n\n\nserVis(json_lda)\n\n\nIn this visualisation, lambda controls the relevance of the shown terms:\n\nlambda = 1: The relevance of terms is based entirely on their probability within the topic (p(term | topic)). This means that the most probable terms for each topic will be shown.\nlambda = 0: The relevance of terms is based entirely on their lift (p(term | topic) / p(term)). This means that terms that are particularly unique to a topic (compared to their overall frequency) will be shown.\n0 &lt; lambda &lt; 1: The relevance is a combination of probability and lift, allowing a balance between common terms within the topic and terms that are distinctive for the topic.\n\n\n\n\nOften, we want to use the results of topic modelling to connect with other data (funding, authors, dates etc.). We can directly export the probability matrix. In this matrix each row is a document, and each column contains the probability that the document relates to this topic. The documents are in the same order as the initial data:\n\nposterior_probs &lt;- posterior(topic_model_results)$topics\nprint(head(posterior_probs))\n\n           1          2          3          4          5          6          7\n1 0.03713235 0.10588235 0.05588235 0.01838235 0.05588235 0.08088235 0.03713235\n2 0.04428288 0.05551884 0.10046266 0.04428288 0.04428288 0.03304693 0.04428288\n3 0.02492522 0.05882353 0.02492522 0.03339980 0.05882353 0.07577268 0.03339980\n4 0.03385940 0.02410330 0.03385940 0.07288379 0.08751793 0.02898135 0.02410330\n5 0.01867856 0.03763591 0.02341790 0.02341790 0.08028994 0.06133259 0.02341790\n6 0.04507257 0.03208556 0.05805959 0.29182582 0.04507257 0.02559206 0.04507257\n           8          9         10         11         12         13         14\n1 0.04338235 0.07463235 0.11213235 0.11213235 0.03088235 0.06213235 0.03713235\n2 0.05551884 0.03304693 0.05551884 0.08922670 0.03304693 0.05551884 0.03304693\n3 0.26221336 0.04187438 0.03339980 0.04187438 0.05882353 0.02492522 0.03339980\n4 0.05337159 0.15581062 0.06800574 0.03385940 0.04361549 0.04361549 0.02898135\n5 0.03289657 0.21773069 0.15611932 0.01393922 0.03289657 0.02815723 0.11820463\n6 0.02559206 0.07104660 0.02559206 0.01909855 0.01909855 0.04507257 0.03208556\n          15         16         17\n1 0.09338235 0.02463235 0.01838235\n2 0.14540648 0.04428288 0.08922670\n3 0.05034895 0.03339980 0.10967099\n4 0.16068867 0.06312769 0.04361549\n5 0.03289657 0.06133259 0.03763591\n6 0.06455309 0.11650115 0.03857907",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#load-the-data",
    "href": "Topic_modelling_lda2.html#load-the-data",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "We start by loading the data from a csv file, we then remove the publications without an abstract\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(ldatuning)\nlibrary(topicmodels)\nlibrary(tidytext)\nlibrary(LDAvis)\nlibrary(ggplot2)\nentr_eco_publications &lt;- read_csv(\"~/Desktop/Teachings/Topic_mod_workshop/entr_eco_publications.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 1603 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (30): Authors, Author full names, Author(s) ID, Title, Source title, Iss...\ndbl  (4): Year, Volume, Page count, Cited by\nlgl (12): Molecular Sequence Numbers, Chemicals/CAS, Tradenames, Manufacture...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nentr_eco_publications = subset(entr_eco_publications, entr_eco_publications$Abstract != \"[No abstract available]\")\nentr_eco_publications = entr_eco_publications$Abstract",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#text-preprocessing",
    "href": "Topic_modelling_lda2.html#text-preprocessing",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "Before creating the model we need to prepare the text. This means removing punctuation, weird characters or even words that we don’t want to find in our topics simply because they don’t have any informational value.\n\nmy_dico = c(\"publication\", \"research\", \"focus\", \"results\", \"result\")\n\nWe build a function that prepares the text. It removed punctuation, removes stopwords and the words we don’t want, stems the text etc. This function can be adapted and extended to your need by adding lemmatization for example.\n\nlibrary(tm)\nclean_text &lt;- function(text){\n  text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  # we can use the gsub funciton to substite specific patterns in the text with something else. Or remove them by replacing them with \"\".\n  text &lt;- gsub(\"\\\\.\", \"\", text)\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  text &lt;- stemDocument(text) # stem the terms\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n}\n\nWe apply the function to our text\n\nentr_eco_publications = apply(as.matrix(entr_eco_publications), 1, clean_text)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#prepare-the-data-format",
    "href": "Topic_modelling_lda2.html#prepare-the-data-format",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "Now that we have the text prepared, we need to create a document-term matrix for the topic modelling function. The document-term matrix specifies for each document which terms are contained within it. We use the DocumentTermMatrix() function from the tm package. This function has one argument which is a dataframe with the text of the corpus.\n\n# create the document-Term-matrix\ndtm &lt;- DocumentTermMatrix(entr_eco_publications)",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#find-the-number-of-topics",
    "href": "Topic_modelling_lda2.html#find-the-number-of-topics",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "Based on this matrix we will now try to define how many topics we need to extract. For this we use the FindTopicsNumber and FindTopicsNumber_plot from the ldatuning package.\nThe FindTopicsNumber functions takes several arguments. The first is directly the document term matrix. We also need to specify which number of topics we want to try. The seq() function creates a vector which starts at the from arguments, stops at the to argument. The size of the steps is set by the by argument. If we want to check for a number of topics between 2 and 60 with steps of 5 (2, 7, 13, 18, …) we would write seq(from = 2, to = 60, by = 5).\nWe then specify the metrics we want to compute, we have discussed these in the lecture.\n\n\n\n\n\n\nWarning\n\n\n\nFor an unknown reason, the “Griffiths2004” function does not work on mac OSX. It should work for windows users.\n\n\nThe mc.cores option specifies on how many cores you want the algorithm to run, this depends on your laptop, adjust to suit your needs. The verbose argument defines whether or not you want the algorithm to provide some information on which stage it is working. This will reduce the anxiety of not knowing whether the algorithm is stuck, still running or finished.\n\ntopic_num &lt;- FindTopicsNumber(\n  dtm,\n  topics = seq(from = 2, to = 60, by = 5),\n  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  mc.cores = 8L,\n  verbose = TRUE\n)\n\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Arun2010... done.\n  Deveaud2014... done.\n\nFindTopicsNumber_plot(topic_num)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the ldatuning package.\n  Please report the issue at &lt;https://github.com/nikita-moor/ldatuning/issues&gt;.",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#run-the-model",
    "href": "Topic_modelling_lda2.html#run-the-model",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "Now that we have an idea of how many topics we need, let’s extract the topics. We will use the LDA function from the topicmodels package, and specify K as the number of topics. Here we can pick 12~17 based on the previous plot.\n\n# perform topic modelling\nk = 17\ntopic_model_results  &lt;- LDA(dtm, k = k, method=\"Gibbs\", control = list(seed = 42, alpha=50/k, nstart = 1, keep = 1, burnin = 1000, iter = 1000, verbose = FALSE))",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#visualize-the-results",
    "href": "Topic_modelling_lda2.html#visualize-the-results",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "topic_model_results contains the results of the topic modelling process, in other words it contains the probabilities that a word is part of a topic and a topic is part of a document. We can visualise this information by plotting the betas (probability that a word is part of a topic).\n\n# get the betas\nbetas &lt;- tidy(topic_model_results, matrix = \"beta\")\n# subset the betas for results\nap_top_terms &lt;- betas %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nlibrary(ggplot2)\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\nThis plot shows the top 10 words for each topic. This means that it shows the 10 words with the highest probability to be part of the topic. The values under each barplot represent the betas.",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#create-an-interactive-visualisation",
    "href": "Topic_modelling_lda2.html#create-an-interactive-visualisation",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "# Extract the necessary components for LDAvis\nlda_topics &lt;- posterior(topic_model_results)$topics\nlda_terms &lt;- posterior(topic_model_results)$terms\nterm_frequency &lt;- colSums(as.matrix(dtm))\ndoc_length &lt;- rowSums(as.matrix(dtm))\n\n# Create the JSON object for visualization\njson_lda &lt;- createJSON(phi = lda_terms,\n                       theta = lda_topics,\n                       doc.length = doc_length,\n                       vocab = colnames(dtm),\n                       term.frequency = term_frequency)\n\n\nserVis(json_lda)\n\n\nIn this visualisation, lambda controls the relevance of the shown terms:\n\nlambda = 1: The relevance of terms is based entirely on their probability within the topic (p(term | topic)). This means that the most probable terms for each topic will be shown.\nlambda = 0: The relevance of terms is based entirely on their lift (p(term | topic) / p(term)). This means that terms that are particularly unique to a topic (compared to their overall frequency) will be shown.\n0 &lt; lambda &lt; 1: The relevance is a combination of probability and lift, allowing a balance between common terms within the topic and terms that are distinctive for the topic.",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#export-the-information",
    "href": "Topic_modelling_lda2.html#export-the-information",
    "title": "Topic Modelling with LDA",
    "section": "",
    "text": "Often, we want to use the results of topic modelling to connect with other data (funding, authors, dates etc.). We can directly export the probability matrix. In this matrix each row is a document, and each column contains the probability that the document relates to this topic. The documents are in the same order as the initial data:\n\nposterior_probs &lt;- posterior(topic_model_results)$topics\nprint(head(posterior_probs))\n\n           1          2          3          4          5          6          7\n1 0.03713235 0.10588235 0.05588235 0.01838235 0.05588235 0.08088235 0.03713235\n2 0.04428288 0.05551884 0.10046266 0.04428288 0.04428288 0.03304693 0.04428288\n3 0.02492522 0.05882353 0.02492522 0.03339980 0.05882353 0.07577268 0.03339980\n4 0.03385940 0.02410330 0.03385940 0.07288379 0.08751793 0.02898135 0.02410330\n5 0.01867856 0.03763591 0.02341790 0.02341790 0.08028994 0.06133259 0.02341790\n6 0.04507257 0.03208556 0.05805959 0.29182582 0.04507257 0.02559206 0.04507257\n           8          9         10         11         12         13         14\n1 0.04338235 0.07463235 0.11213235 0.11213235 0.03088235 0.06213235 0.03713235\n2 0.05551884 0.03304693 0.05551884 0.08922670 0.03304693 0.05551884 0.03304693\n3 0.26221336 0.04187438 0.03339980 0.04187438 0.05882353 0.02492522 0.03339980\n4 0.05337159 0.15581062 0.06800574 0.03385940 0.04361549 0.04361549 0.02898135\n5 0.03289657 0.21773069 0.15611932 0.01393922 0.03289657 0.02815723 0.11820463\n6 0.02559206 0.07104660 0.02559206 0.01909855 0.01909855 0.04507257 0.03208556\n          15         16         17\n1 0.09338235 0.02463235 0.01838235\n2 0.14540648 0.04428288 0.08922670\n3 0.05034895 0.03339980 0.10967099\n4 0.16068867 0.06312769 0.04361549\n5 0.03289657 0.06133259 0.03763591\n6 0.06455309 0.11650115 0.03857907",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  },
  {
    "objectID": "Topic_modelling_lda2.html#lemmatization",
    "href": "Topic_modelling_lda2.html#lemmatization",
    "title": "Topic Modelling with LDA",
    "section": "Lemmatization",
    "text": "Lemmatization\nIt is common, even natural, to find inflections of the same term in a corpus of texts. The presence of a term and its plural (desalinator, desalinators), abbreviations (pneu, pneumatics), conjugations (run, ran, running) or terms with a close semantic meaning (desalinator, desalination) are common occurrences. These inflections, however, pose a problem in term frequency counts. In general, we consider that the terms desalinator, desalination and desalinators have the same informational value and are therefore synonymous. Retaining multiple inflections in the text results in a frequency calculation for each individual term resulting in a lower overall importance of each term. We would like to have only one count, for a term that we consider to be the reference term. There are two approaches to doing this, stemming and lemmatization. Stemming approaches this issue by reducing each word to its stem. The stem that results from this process is not always a word and can be difficult to understand out of context. Lemmatization has a different approach and used dictionary definitions to replace terms by a main form. Figure 1 gives an example for different inflections which are reduced to a main form, in this case: desalinate).\n\n\n\n\n\n\n\n\nflowchart LR\n  C(Desalinate)\n  D[Desalinates] --&gt; C\n  E[Desalinating] --&gt; C\n  F[Desalinated] --&gt; C\n  G[Desalinator] --&gt; C\n  H[Desalination] --&gt; C\n\n\n\n\nFigure 1: Example of lemmatisation, where variations are replaced by “desalinate”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  O(Desalin)\n  J[Desalinates] --&gt; O\n  K[Desalinating] --&gt; O\n  L[Desalinated] --&gt; O\n  M[Desalinator] --&gt; O\n  N[Desalination] --&gt; O\n\n\n\n\nFigure 2: Example of Stemming, where variations are reduced to their stem “desalin”\n\n\n\n\n\n\n\nThere are practical advantages to using lemmatization since the main form remains readable, while with stemming this is more complicated. In fine, it’s up to the analyst to decide which approach is best for both the question at hand and the data chosen. In the following table some advantages and disadvantages are shown:\n\n\n\n\n\n\n\n\nAspect\nLemmatization\nStemming\n\n\n\n\nAccuracy\nBetter accuracy, considers context\nFaster, computationally less expensive\n\n\nReadability\nImproved, real words\nSimpler, heuristic rules\n\n\nContext Preservation\nConsiders word meaning, preserves context\nMay lead to over-stemming, loss of specificity\n\n\nComputational Complexity\nMore computationally expensive\nLess computationally expensive\n\n\nResource Intensive\nRequires linguistic resources\nMinimal resources required\n\n\n\n\nStemming and Lemmatization in R\nFor the implementation of lemmatization we will use the textstem package. Lemmatization is done in two steps, first a dictionnary is created based on the text. Basically this means that all terms in the provided text are identified and for these terms lemmas are identified. In a second step this dictionary is then applied to the text. The main reason for this two-step approach is to reduce computation time since we don’t have to search through words that are not in the text.\nFor example, when we apply lemmatisation on the variation of the word desalinate:\n\nlibrary(textstem)\n\nLoading required package: koRpus.lang.en\n\n\nLoading required package: koRpus\n\n\nLoading required package: sylly\n\n\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nThe following object is masked from 'package:readr':\n\n    tokenize\n\n# Some variations on a word as an example:\nExample_text &lt;-  c(\"Desalinates\", \"Desalinating\", \"Desalinated\", \"Desalinator\", \"Desalination\")\nExample_text &lt;- tolower(Example_text) # remove the capital letters (required)\n# we make a dictionary from the text\nMy_dico &lt;-  make_lemma_dictionary(Example_text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\nlemmatized_text &lt;- lemmatize_strings(Example_text, dictionary = My_dico)\nlemmatized_text\n\n[1] \"desalinate\"  \"desalinate\"  \"desalinate\"  \"desalinator\" \"desalinate\" \n\n\nWe can include lemmatization in the cleaning function and then rerun the script from the beginning with this function.\n\nclean_text_lemma &lt;- function(text){\n  #text = removePunctuation(text) # optional\n  text &lt;- tolower(text) # remove caps\n  text &lt;- removeWords(text, my_dico) # Remove the terms from our own dictionary\n  # here we apply lemmatization instead of stemming:\n  lemma_dico &lt;-  make_lemma_dictionary(text, engine = 'hunspell')\n# now we apply the dictionnary to clean the text\n  text &lt;- lemmatize_strings(text, dictionary = lemma_dico)\n  text &lt;- removeWords(text, stopwords(kind &lt;- \"en\")) # remove stopwords (in english)\n  text &lt;- trimws(text) # remove any weird spaces in the text\n  text &lt;- gsub(\"  \", \" \", text)\n}",
    "crumbs": [
      "Home",
      "Topic Modelling Workshop",
      "Topic Modelling with LDA"
    ]
  }
]